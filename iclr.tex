
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% Packages added by me
% \usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,mathtools,amsthm}
\usepackage{mathrsfs}
\usepackage{dirtytalk}
\usepackage{tikz}
\usepackage{placeins}
\usepackage{csvsimple}
\usepackage{siunitx}
% \usepackage{xfp}
% \usepackage{etoolbox}
% \usepackage{ifthen}
% \usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
% \usepackage{array}


%Macros
% \newcommand{\R}{\mathbb{R}}
% \newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathfrak{I}}
\newcommand{\HS}{\mathcal{L}^{2}}
\newcommand{\Op}{\mathcal{L}^{\infty}}
\newcommand{\Lp}{\mathcal{L}^{p}}
\newcommand{\Hil}{\mathcal{H}}
\newcommand{\range}{\text{Ran}}
\newcommand{\rep}{\varphi}
\newcommand{\repone}{\phi}
\newcommand{\reptwo}{\psi}
\newcommand{\repthree}{\varphi}
\newcommand{\Hone}{\mathcal{H}_{\phi}}
\newcommand{\Htwo}{\mathcal{H}_{\psi}}
\newcommand{\Hf}{\mathcal{H}_{f}}
\newcommand{\Hg}{\mathcal{H}_{g}}
\newcommand{\Hb}{\mathcal{H}_{\vartheta}}
\newcommand{\Hthree}{\mathcal{H}_{\varphi}}
\newcommand{\Hrep}{\mathcal{H}_{\varphi}}
\newcommand{\Srep}{\Sigma_{\rep}}
\newcommand{\Sone}{\Sigma_{\repone}}
\newcommand{\Stwo}{\Sigma_{\reptwo}}
\newcommand{\Sthree}{\Sigma_{\repthree}}
\newcommand{\Sonetwo}{\Sigma_{\repone \reptwo}}
\newcommand{\Stwoone}
{\Sigma_{\reptwo \repone}}
\newcommand{\Sab}{\Sigma_{\varphi \vartheta}}
\newcommand{\Sba}{\Sigma_{\vartheta \varphi}}
\newcommand{\Sreplambda}{\Srep+\lambda I}
\newcommand{\Sonelambda}{\Sone+\lambda I}
\newcommand{\Stwolambda}{\Stwo+\lambda I}
\newcommand{\Sthreelambda}{\Sthree+\lambda I}
\newcommand{\Sonetwolambda}{\Sonetwo+\lambda I}
\newcommand{\Stwoonelambda}{\Stwoone+\lambda I}
\newcommand{\Sablambda}{\Sab+\lambda I}
\newcommand{\Sbalambda}{\Sba+\lambda I}
\newcommand{\Sreplambdainv}{(\Srep+\lambda I)^{-1}}
\newcommand{\Sonelambdainv}{(\Sone+\lambda I)^{-1}}
\newcommand{\Stwolambdainv}{(\Stwo+\lambda I)^{-1}}
\newcommand{\Sthreelambdainv}{(\Sthree+\lambda I)^{-1}}
\newcommand{\Sonetwolambdainv}{(\Sonewtwo+\lambda I)^{-1}}
\newcommand{\Stwoonelambdainv}{(\Stwoone+\lambda I)^{-1}}
\newcommand{\Sablambdainv}{(\Sab+\lambda I)^{-1}}
\newcommand{\Sbalambdainv}{(\Sba+\lambda I)^{-1}}
\newcommand{\Srephat}{\hat{\Sigma}_{\rep}}
\newcommand{\Sonehat}{\hat{\Sigma}_{\repone}}
\newcommand{\Stwohat}{\hat{\Sigma}_{\reptwo}}
\newcommand{\Sthreehat}{\hat{\Sigma}_{\repthree}}
\newcommand{\Sonetwohat}{\hat{\Sigma}_{\repone \reptwo}}
\newcommand{\Stwoonehat}
{\hat{\Sigma}_{\reptwo \repone}}
\newcommand{\Sabhat}{\hat{\Sigma}_{\varphi \vartheta}}
\newcommand{\Sbahat}{\hat{\Sigma}_{\vartheta \varphi}}
\newcommand{\Sreplambdahat}{\Srephat+\lambda I}
\newcommand{\Sonelambdahat}{\Sonehat+\lambda I}
\newcommand{\Stwolambdahat}{\Stwohat+\lambda I}
\newcommand{\Sthreelambdahat}{\Sthreehat+\lambda I}
\newcommand{\Sonetwolambdahat}{\Sonetwohat+\lambda I}
\newcommand{\Stwoonelambdahat}{\Stwoonehat+\lambda I}
\newcommand{\Sablambdahat}{\Sabhat+\lambda I}
\newcommand{\Sbalambdahat}{\Sbahat+\lambda I}
\newcommand{\Sreplambdainvhat}{(\Srephat+\lambda I)^{-1}}
\newcommand{\Sonelambdainvhat}{(\Sonehat+\lambda I)^{-1}}
\newcommand{\Stwolambdainvhat}{(\Stwohat+\lambda I)^{-1}}
\newcommand{\Sthreelambdainvhat}{(\Sthreehat+\lambda I)^{-1}}
\newcommand{\Sonetwolambdainvhat}{(\Sonetwohat+\lambda I)^{-1}}
\newcommand{\Stwoonelambdainvhat}{(\Stwoonehat+\lambda I)^{-1}}
\newcommand{\Sablambdainvhat}{(\Sabhat+\lambda I)^{-1}}
\newcommand{\Sbalambdainvhat}{(\Sbahat+\lambda I)^{-1}}
\newcommand{\Trep}{\mathcal{T}_{\rep}}
\newcommand{\Tone}{\mathcal{T}_{\repone}}
\newcommand{\Ttwo}{\mathcal{T}_{\reptwo}}
\newcommand{\Tthree}{\mathcal{T}_{\repthree}}
\newcommand{\Treplambda}{\Trep+\lambda I}
\newcommand{\Tonelambda}{\Tone+\lambda I}
\newcommand{\Ttwolambda}{\Ttwo+\lambda I}
\newcommand{\Tthreelambda}{\Tthree+\lambda I}
\newcommand{\Treplambdainv}{(\Trep+\lambda I)^{-1}}
\newcommand{\Tonelambdainv}{(\Tone+\lambda I)^{-1}}
\newcommand{\Ttwolambdainv}{(\Ttwo+\lambda I)^{-1}}
\newcommand{\Tthreelambdainv}{(\Tthree+\lambda I)^{-1}}
\newcommand{\Irep}{\I_{\rep}}
\newcommand{\Irepone}{\I_{\repone}}
\newcommand{\Ireptwo}{\I_{\reptwo}}
\newcommand{\Irepthree}{\I_{\repthree}}
\newcommand{\Irepad}{\Irep^{*}}
\newcommand{\Ireponead}{\Irepone^{*}}
\newcommand{\Ireptwoad}{\Ireptwo^{*}}
\newcommand{\Irepthreead}{\Irepthree^{*}}
\newcommand{\Samprep}{\mathcal{S}_{\rep}}
\newcommand{\Sampb}{\mathcal{S}_{\vartheta}}
\newcommand{\Sampone}{\mathcal{S}_{\repone}}
\newcommand{\Samptwo}{\mathcal{S}_{\reptwo}}
\newcommand{\Sampthree}{\mathcal{S}_{\repthree}}
\newcommand{\Samprepad}{\Samprep^{*}}
\newcommand{\Sampbad}{\Sampb^{*}}
\newcommand{\Samponead}{\Sampone^{*}}
\newcommand{\Samptwoad}{\Samptwo^{*}}
\newcommand{\Sampthreead}{\Sampthree^{*}}
\newcommand{\Knrep}{K_{n,\rep}}
\newcommand{\Knb}{K_{n,\vartheta}}
\newcommand{\Knone}{K_{n,\repone}}
\newcommand{\Kntwo}{K_{n,\reptwo}}
\newcommand{\Knthree}{K_{n,\repthree}}
\newcommand{\Knreplambda}{\frac{1}{n}\Knrep + \lambda I}
\newcommand{\Knblambda}{\frac{1}{n}\Knb + \lambda I}
\newcommand{\Knonelambda}{\frac{1}{n}\Knone + \lambda I}
\newcommand{\Kntwolambda}{\frac{1}{n}\Kntwo + \lambda I}
\newcommand{\Knthreelambda}{\frac{1}{n}\Knthree + \lambda I}
\newcommand{\Knreplambdainv}{(\frac{1}{n}\Knrep+\lambda I)^{-1}}
\newcommand{\Knblambdainv}{(\frac{1}{n}\Knb+\lambda I)^{-1}}
\newcommand{\Knonelambdainv}{(\frac{1}{n}\Knone+\lambda I)^{-1}}
\newcommand{\Kntwolambdainv}{(\frac{1}{n}\Kntwo+\lambda I)^{-1}}
\newcommand{\Knthreelambdainv}{(\frac{1}{n}\Knthree+\lambda I)^{-1}}
\newcommand{\Knreplambdaalt}{\Knrep + n\lambda I}
\newcommand{\Knblambdaalt}{\Knb + n\lambda I}
\newcommand{\Knonelambdaalt}{\Knone + n\lambda I}
\newcommand{\Kntwolambdaalt}{\Kntwo + n\lambda I}
\newcommand{\Knthreelambdaalt}{\Knthree + n\lambda I}
\newcommand{\Knreplambdainvalt}{(\Knrep+n\lambda I)^{-1}}
\newcommand{\Knblambdainvalt}{(\Knb+n\lambda I)^{-1}}
\newcommand{\Knonelambdainvalt}{(\Knone+n\lambda I)^{-1}}
\newcommand{\Kntwolambdainvalt}{(\Kntwo+n\lambda I)^{-1}}
\newcommand{\Knthreelambdainvalt}{(\Knthree+n\lambda I)^{-1}}
\newcommand{\gl}{g_{\lambda}}
\newcommand{\glTik}{\gl^{\operatorname{Tik}}}
\newcommand{\glSh}{\gl^{\operatorname{Sho}}}
\newcommand{\glCut}{\gl^{\operatorname{Cut}}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\inprod}[1]{\left \langle #1 \right\rangle}
\newcommand{\Rancl}[1]{\overline{\range(#1)}}
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\Tr}{\text{Tr}}
\newcommand{\LPtwo}{L^{2}(P_{X})}
\newcommand{\optwo}{\mathcal{L}^{\infty}(\LPtwo)}
\newcommand{\HStwo}{\mathcal{L}^{2}(\LPtwo)}
\newcommand{\Lptwo}{\mathcal{L}^{p}(\LPtwo)}
\newcommand{\metricstname}{UKP }
\newcommand{\metricfullname}{Uniform Kernel Prober}
\newcommand{\dop}{d_{\lambda,K,\mathcal{L}^{\infty}}^{\emph{\metricstname}}}
\newcommand{\dtwo}{d_{\lambda,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\dtwohat}{\hat{d}_{\lambda,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\done}{d_{\lambda,K,\mathcal{L}^{1}}^{\emph{\metricstname}}}
\newcommand{\dLp}{d_{\lambda,K,\mathcal{L}^{p}}^{\emph{\metricstname}}}
\newcommand{\dopgl}{d_{\gl,K,\mathcal{L}^{\infty}}^{\emph{\metricstname}}}
\newcommand{\dtwogl}{d_{\gl,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\dtwoglhat}{\hat{d}_{\gl,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\donegl}{d_{\gl,K,\mathcal{L}^{1}}^{\emph{\metricstname}}}
\newcommand{\dLpgl}{d_{\gl,K,\mathcal{L}^{p}}^{\emph{\metricstname}}}
\newcommand{\dopglTik}{d_{\glTik,K,\mathcal{L}^{\infty}}^{\emph{\metricstname}}}
\newcommand{\dtwoglTik}{d_{\glTik,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\dtwoglTikhat}{\hat{d}_{\glTik,K,\mathcal{L}^{2}}^{\emph{\metricstname}}}
\newcommand{\doneglTik}{d_{\glTik,K,\mathcal{L}^{1}}^{\emph{\metricstname}}}
\newcommand{\dLpglTik}{d_{\glTik,K,\mathcal{L}^{p}}^{\emph{\metricstname}}}
\newcommand{\SpectralAssumptionone}{A_{1}}
\newcommand{\SpectralAssumptiontwo}{A_{2}}
\newcommand{\SpectralAssumptionthree}{A_{3}}
\newcommand{\SpectralAssumptionfour}{A_{4}}
\newcommand{\SpectralAssumptionfive}{A_{5}}
\newcommand{\trt}[1]{\textcolor{red}{#1}}
% split \left and \right across two lines
% use only between \[ and \] (modify as needed for \begin{equation}\end{equation}, etc.)
\newcommand\MatchBrackets[2]{\left[#1\vphantom{#2}\right.\]\[\left.\vphantom{#1}#2\right]}
\newcommand\MatchParentheses[2]{\left(#1\vphantom{#2}\right.\]\[\left.\vphantom{#1}#2\right)}
\newcommand{\Riskrepone}{\mathcal{R}_{\repone}}
\newcommand{\Riskreptwo}{\mathcal{R}_{\reptwo}}
\newcommand{\Riskrep}{\mathcal{R}_{\rep}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}

%Counters
\newcounter{lemmano}
\newcounter{theoremno}
\newcounter{propositionno}
\newcounter{claimno}
\newcounter{corollarynno}
\newcounter{definitionno}
\newcounter{remarkno}
\newcounter{assumptionno}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}[theoremno]{Theorem}
\newtheorem{lemma}[lemmano]{Lemma}
\newtheorem{proposition}[propositionno]{Proposition}
\newtheorem{claim}[claimno]{Claim}
\newtheorem{corollary}[corollarynno]{Corollary}
\newtheorem{definition}[definitionno]{Definition}
\newtheorem{remark}[remarkno]{Remark}
\newtheorem{assumption}[assumptionno]{Assumption}
\newenvironment{theorem*}{{\bf Lemma:}}

\title{Uniform Kernel Prober}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Soumya Mukherjee \& Bharath Sriperumbudur \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Statistics\\
Pennsylvania State University\\
University Park, PA 16802, USA \\
\texttt{\{szm6510,bks18\}@psu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The ability to identify useful features or representations of the input data based on training data that achieves low prediction error on test data across multiple prediction tasks is considered the key to multitask learning success. In practice, however, one faces the issue of the choice of prediction tasks and the availability of test data from the chosen tasks while comparing the relative performance of different features. In this work, we develop a class of pseudometrics called Uniform Kernel Prober (UKP) for comparing features or representations learned by different statistical models such as neural networks when the downstream prediction tasks involve kernel ridge regression. The proposed pseudometric, UKP, between any two representations, provides a uniform measure of prediction error on test data corresponding to a general class of kernel ridge regression tasks for a given choice of a kernel without access to test data. Additionally, desired invariances in representations can be successfully captured by UKP only through the choice of the kernel function and the pseudometric can be efficiently estimated from $n$ input data samples with $O(\frac{1}{\sqrt{n}})$ estimation error. We also experimentally demonstrate the ability of UKP to discriminate between different types of features or representations based on their generalization performance on downstream kernel ridge regression tasks.
\end{abstract}

\section{Introduction}

Model comparison is a classical problem in Statistics and Machine Learning \citep{burnham1998practical,pfahringer2000meta,spiegelhalter2002bayesian,caruana2006empirical,fernandez2014we}. This question has received tremendous attention from the scientific community, especially after the widespread adoption and implementation of modern general-purpose large-scale models such as deep neural networks (DNNs). Developing broadly applicable criteria for model comparison remains challenging due to variation in mathematical representations, no. of trainable parameters, and transparency (open vs. black-box access). In supervised learning, however, models can naturally be compared by differences in predictive performance, since this directly aligns with the goal of maximizing accuracy on the prediction task. It is now well understood that the key to success for training models with good generalization ability over multiple tasks (i.e. achieves low prediction error on test data across multiple prediction tasks) is directly correlated to the ability of models to identify useful features or representations of the input data based on training data \citep{bengio2013representation,lecun2015deep,maurer2016benefit}. Therefore, one can attempt to resolve the question of model comparison by considering metrics (more precisely, pseudometrics) on the space of features or representations, and there is extensive literature in this area \citep{laakso2000content,li2015convergent,morcos2018insights,wang2018towards,kornblith2019similarity,GULP}.

  An ideal pseudometric must be interpretable and efficiently computable based on a reasonably small amount of data samples. It must also be sensitive only to differences in features that will lead to differences in predictive performance, but be fairly insensitive to any other differences in features that do not affect predictive performance. Finally, it must be flexible enough to accommodate available prior knowledge about the class of prediction tasks that is of interest to the model users. However, most pseudometrics fall short of fulfilling this extensive set of desiderata. In this work, we develop a class of pseudometrics on the space of representations called Uniform Kernel Prober (UKP) that can be used to compare features or representations learned by any class of statistical models. 

 The proposed \metricstname pseudometric is motivated by the need for a distance measure over representations of differing 
 dimensionalities that captures the ability of a model to generalize over a general and flexible class of prediction tasks, specifically, the class of kernel ridge regression-based tasks. Depending on the choice of the kernel, one can probe which models share ``similar" features, with similarity being understood in the following sense: If the features or representations for a pair of models are similar, then, if they are both trained to perform kernel ridge regression tasks, their predictive performances will be close to each other.
 The \metricstname pseudometric is a unique distance measure over features or representations and is a useful contribution to the existing literature since it has the following desirable characteristics:
\begin{enumerate}
    \item The proposed pseudometric offers a uniform guarantee of performance similarity for a wide range of regression functions, irrespective of whether the tasks are kernel ridge-regression or not. This is particularly beneficial when the prediction tasks align with models whose representations share similar characteristics with the kernel used to compute the \metricstname distance.

     \item The pseudometric is adaptable to incorporate inductive biases that help identify models suited for specific tasks. A simple choice of the kernel parameter of the \metricstname distance can help us encode these inductive biases. For example, suppose we are interested in image classification tasks where the rotation and/or translation of the images should not affect the model prediction. In that case, we can encode this inductive bias into the pseudometric by choosing a rotationally and translationally invariant kernel, such as a Gaussian RBF kernel, as the kernel parameter for UKP. This results in the creation of two clusters: one for models with rotationally and translationally invariant features and another for models without such features.
    
    To the best of our knowledge, ours is the first pseudometric on the space of representations in the ML literature that can flexibly encode a wide range of inductive biases and treat them within a single framework.

    \item \metricstname distance has a practical prediction-based interpretation in addition to usual mathematical interpretations of similarity or dissimilarity in terms of inner product or pseudometric.
    
    \item Computation of the estimate of \metricstname distance only requires unlabelled data, i.e., data samples from the input domain, and therefore preserves labeled data for model training/fitting. Moreover, the computation of the estimate of \metricstname distance only requires black-box access to model representations, i.e., pairs of inputs and outputs to the model. 
    \item It is possible to design a statistically efficient estimator for the \metricstname distance based on a finite number ($n$) of samples from the input domain, that enjoys an estimation error rate of $n^{-1/2}$.
    
    \item The \metricstname distance enables us to even compare representations that differ in their dimensionalities. 
    
\end{enumerate}

The paper is organized as follows. In Section \ref{Problem Setup}, we formally define the \metricstname distance. In Section \ref{Properties}, we provide elegant and tractable characterizations of the \metricstname distance and prove that it satisfies all criteria of being a pseudometric. Then using Theorem \ref{Invariance of UKP distance}, we also find the type of transformations under which the \metricstname distance remains invariant. We propose a statistical estimator of the \metricstname distance in Section \ref{statistical estimation of UKP}. In Sections \ref{Relation to other comparison measures} and \ref{Finite sample convergence rate}, we mathematically demonstrate its relationship to other pseudometrics used for model comparison and show that our proposed estimator converges to the true \metricstname distance as the sample size goes to infinity. Finally, in Section \ref{experiments}, we provide numerical experiments that validate our theory. Proofs of all lemmas, propositions and theorems are provided in Section \ref{Proofs} of the Appendix. 
\section{Problem setup}\label{Problem Setup}
Consider $\repone: \mathbb{R}^d \rightarrow \mathbb{R}^{\ell}$ and $\reptwo: \mathbb{R}^d \rightarrow \mathbb{R}^k$ to be two representation maps (with output dimensions $l, k$, respectively) that transform an input to its corresponding feature representation, typically obtained from a pair of trained/fitted models.
Let $Y$ be the random real-valued response corresponding to the input $X$ generated from the nonparametric regression model $Y=\eta(X) + \epsilon$, where $\epsilon$ is mean-zero noise and $\eta(x) = \E(Y \mid X = x)$ is the population regression function of $Y$ on $X$.

Let $K(\cdot,\cdot)$ be a positive definite, symmetric, bounded, and continuous ``base" kernel function, mapping pairs of vectors in Euclidean spaces of different dimensions to real numbers. Common choices of radial kernels include the Gaussian RBF kernel $K_{RBF,h}(x,y) = \exp(-\frac{1}{2h}\norm{x-y}_{2}^{2})$ and the Laplace kernel $K_{Lap,h}(x,y) = \exp(-\frac{1}{2h}\norm{x-y}_{1})$, where $x,y\in \R^{d}$ for any $d\in \N$. By the Moore-Aronszajn Theorem \citep{aronszajn1950theory} and Lemma 4.33 of \citet{steinwart2008support}, there exists a unique separable Reproducing Kernel Hilbert Space (RKHS) $\Hil$ of functions such that $K(\cdot,\cdot)$ is its unique reproducing kernel. Now, fix a single representation  $\rep: \R^{d}\rightarrow \R^{\textrm{out}}$. Define the corresponding ``pullback kernel", $K_{\rep}(\cdot,\cdot) \coloneq K(\rep(\cdot),\rep(\cdot))$. By Theorem 5.7 of \citet{paulsen2016introduction}, $K_{\rep}$ is positive‑definite on $\R^{d}$ and is the (unique) reproducing kernel of the ``pullback'' RKHS $\Hrep \coloneq \Hil\left(K \circ \left(\rep \times \rep\right)\right)$. If we let $\Hil^{\textrm{out}}$ denote the RKHS associated with $K$ when the domain is restricted to $\R^{\textrm{out}} \times \R^{\textrm{out}}$, then the $\Hrep$-norm (pullback RKHS norm) of any $f_{\rep}\in \Hrep$ satisfies the minimal-norm characterization 
$\norm{f_{\rep}}_{\Hrep}=\underset{f \in \Hil^{\textrm{out}}: f \circ \rep = f_{\rep}}{\min}\norm{f}_{\Hil^{\textrm{out}}}$. 


For any two representations $\repone,\reptwo$ and for any $\lambda>0$, let $\alpha_{\lambda}^{\repone}$ and $\alpha_{\lambda}^{\reptwo}$ be the population kernel ridge regression estimators of the regression function $\eta$ using their respective pullback kernels $K_{\repone}(\cdot,\cdot)$ and $K_{\reptwo}(\cdot,\cdot)$, defined as,

\begin{equation}\label{Defintion of alpha}
\begin{aligned}
    \alpha_{\lambda}^{\repone} = \underset{f \in \Hone}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \lambda \norm{f}_{\Hone}^{2} &\textrm{ and }\alpha_{\lambda}^{\reptwo} = \underset{f \in \Htwo}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \lambda \norm{f}_{\Htwo}^{2},
\end{aligned}
\end{equation}


respectively. Here, since the prediction loss is the squared error loss, $\alpha_{\lambda}^{\repone}$ and $\alpha_{\lambda}^{\reptwo}$ depend on the distribution of $Y$ only through the population regression function $\eta$. We suppress this dependence on $\eta$ in the notation for convenience and clarity.

We now define the kernel ridge regression-based pseudometric between the two representations of the input $\repone$ and $\reptwo$, based on the difference between predictions for $Y$ uniformly over all regression functions $\eta \in \LPtwo$ such that its $\LPtwo$ norm is bounded above by 1.
\begin{definition}\label{Definition of pseudometric}
    For any $\lambda>0$ and choice of kernel $K(\cdot,\cdot)$, the \metricstname (\metricfullname) distance between representations $\repone(X)$ and $\reptwo(X)$ is defined as, 
    \[
    \dop(\repone,\reptwo) \coloneq \underset{\norm{\eta}_{\LPtwo} \leq 1}{\sup} \left(\E\left[\alpha_{\lambda}^{\repone}(X)-\alpha_{\lambda}^{\reptwo}(X)\right]^{2}\right)^{\frac{1}{2}},
    \]
    where $\alpha_{\lambda}^{\repone}$ and $\alpha_{\lambda}^{\reptwo}$ are defined in Equation\ref{Defintion of alpha}.
\end{definition}
The reasoning behind the $\mathcal{L}^{\infty}$ in the notation for the \metricstname distance $\dop$ will be clear going forward, as we will demonstrate that it is actually an operator norm in an appropriate formal sense.

\begin{remark}
    In the main paper, we consider the  \metricstname pseudometric w.r.t kernel ridge regression tasks only. Other types of regularization is also possible based on the manner in which the spectrum of the integral operators are regularized (Refer to Appendix \ref{subsec: An introduction to spectral regularizers} for examples). We refer the readers to Definition \ref{Definition of pseudometric operator norm general case} for the generalized definition of the \metricstname distance, denoted by $\dopgl$. All theoretical results in the main paper are proved for a general class of spectral regularizers in the Appendix. 
\end{remark}
% \section{Problem setup}\label{Problem Setup} \trt{Remove duplications of definitions}

% Let the input/predictor of the model be $X \in \R^d$ and $P_{X}$ be the distribution of the input. Let $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ be two instances of a representation map that transforms an input to a feature representation used in a trained/fitted model. Let $Y$ be the random real-valued response corresponding to the input $X$ generated from the nonparametric regression model $Y=\eta(X) + \epsilon$, where $\epsilon$ is mean-zero noise and $\eta(x) = \E(Y \mid X = x)$ is the population regression function of $Y$ on $X$.

% Let $K(\cdot,\cdot)$ be a positive definite, symmetric, bounded, and continuous kernel function, mapping pairs of vectors in Euclidean spaces of different dimensions to real numbers. Examples of radial kernels include the Gaussian RBF kernel $K_{RBF,h}(x,y) = \exp(-\frac{1}{2h}\norm{x-y}_{2}^{2})$ and the Laplace kernel $K_{Lap,h}(x,y) = \exp(-\frac{1}{2h}\norm{x-y}_{1})$, where $x,y\in \R^{d}$ for any $d\in \N$. By the Moore-Aronszajn Theorem \citep{aronszajn1950theory} and Lemma 4.33 of \citet{steinwart2008support}, there exists a unique separable Reproducing Kernel Hilbert Space (RKHS) $\Hil$ of functions such that $K(\cdot,\cdot)$ is its unique reproducing kernel. Theorem 5.7 of \citet{paulsen2016introduction}
% ensures that $K_{\repone}(\cdot,\cdot) \coloneq K(\repone(\cdot),\repone(\cdot))$ and $K_{\reptwo}(\cdot,\cdot) \coloneq K(\reptwo(\cdot),\reptwo(\cdot))$ are the unique reproducing kernels corresponding to the ``pullback'' RKHS's $\Hone \coloneq \Hil\left(K \circ \left(\repone \times \repone\right)\right)$ and $\Htwo \coloneq \Hil\left(K \circ \left(\reptwo \times \reptwo\right)\right)$. Further, let $\Hil^{k}$ and $\Hil^{l}$ be the RKHS's associated with the kernel $K$ when the domain is restricted to $\R^{k} \times \R^{k}$ and $\R^{l} \times \R^{l}$, respectively. Then, for any $f_{\repone} \in \Hone$, we have $\norm{f_{\repone}}_{\Hone}=\underset{f \in \Hil^{k}: f \circ \repone = f_{\repone}}{\min}\norm{f}_{\Hil^{k}}$ and for any $f_{\reptwo} \in \Htwo$, we have $\norm{f_{\reptwo}}_{\Htwo}=\underset{f \in \Hil^{l}: f \circ \reptwo = f_{\reptwo}}{\min}\norm{f}_{\Hil^{l}}$. 

% For any $\lambda>0$, let $\alpha_{\lambda}$ and $\beta_{\lambda}$ be the population kernel ridge regression estimators of the regression function $\eta$, given by
% \begin{equation}\label{Defintion of alpha}
%     \alpha_{\lambda} = \underset{\alpha \in \Hone}{\argmin} \hspace{2pt} \E\left[Y-\alpha(X)\right]^{2} + \lambda \norm{\alpha}_{\Hone}^{2}
% \end{equation}
% and
% \begin{equation}\label{Defintion of beta}
% \beta_{\lambda} = \underset{\beta \in \Htwo}{\argmin} \hspace{2pt} \E\left[Y-\beta(X)\right]^{2} + \lambda \norm{\beta}_{\Htwo}^{2},
% \end{equation}
% respectively. The prediction loss being the squared error loss, $\alpha_{\lambda}$ and $\beta_{\lambda}$ depend on the distribution of $Y$ only through the population regression function $\eta$.

% We now define the kernel ridge regression-based pseudometric between the two representations of the input $\repone$ and $\reptwo$, based on the difference between predictions for $Y$ uniformly over all regression functions $\eta \in \LPtwo$ such that its $\LPtwo$ norm is bounded above by 1.
% \begin{definition}\label{Definition of pseudometric}
%     For any $\lambda>0$ and choice of kernel $K(\cdot,\cdot)$, the \metricstname (\metricfullname) distance between representations $\repone(X)$ and $\reptwo(X)$ is defined as, 
%     \[
%     d_{\lambda,K}^{\metricstname}(\repone,\reptwo) \coloneq \underset{\norm{\eta}_{\LPtwo} \leq 1}{\sup} \left(\E\left[\alpha_{\lambda}(X)-\beta_{\lambda}(X)\right]^{2}\right)^{\frac{1}{2}},
%     \]
%     where $\alpha_{\lambda}$ and $\beta_{\lambda}$ are defined in Equations \eqref{Defintion of alpha} and \eqref{Defintion of beta}, respectively.
% \end{definition}

\section{Properties of the \metricstname distance} \label{Properties}

Let $\I_{\repone}: \Hone \to \LPtwo, f \to f$ be the inclusion operator, which maps any $f \in \Hone$ to its representation $f \in \LPtwo$. Then the adjoint of the inclusion operator is given by $\I_{\repone}^{*}:\LPtwo \to \Hone, f \to \int K_{\repone}(\cdot,x)f(x)dP_{X}(x)$. The inclusion operator $\I_{\reptwo}$ and the corresponding adjoint operator $\I_{\reptwo}^{*}$ can be analogously defined. 

Let us define the covariance operators corresponding to the RKHS's $\Hone$ and $\Htwo$ as
\[
\begin{aligned}
    \Sigma_{\repone} \coloneq \int K_{\repone}(\cdot,x) \otimes_{\Hone}  K_{\repone}(\cdot,x) dP_{X}(x)= \int K(\repone(\cdot),\repone(x)) \otimes_{\Hone} K(\repone(\cdot),\repone(x)) dP_{X}(x)
\end{aligned}
\]
and 
\[
\begin{aligned}
    \Sigma_{\reptwo} \coloneq \int K_{\reptwo}(\cdot,x) \otimes_{\Htwo}  K_{\reptwo}(\cdot,x) dP_{X}(x)= \int K(\reptwo(\cdot),\reptwo(x)) \otimes_{\Htwo} K(\reptwo(\cdot),\reptwo(x)) dP_{X}(x).
\end{aligned}
\]

$\Sigma_{\repone}: \Hil_{\repone} \to \Hil_{\repone}$ and $\Sigma_{\reptwo}: \Hil_{\reptwo} \to \Hil_{\reptwo}$ are the unique operators that satisfy 
\[
    \inprod{\Sigma_{\repone}f_{1},g_{1}}_{\Hone} = \E\left[f_{1}(X)g_{1}(X)\right], \quad
% \]
% and
% \[
    \inprod{\Sigma_{\reptwo}f_{2},g_{2}}_{\Htwo} = \E\left[f_{2}(X)g_{2}(X)\right],
\]
where $f_{1},g_{1} \in \Hone$ and $f_{2},g_{2} \in \Htwo$, respectively. In terms of inclusion operators, it can be easily shown that $\Sigma_{\repone} = \I_{\repone}^{*}\I_{\repone}$ and $\Sigma_{\reptwo} = \I_{\reptwo}^{*}\I_{\reptwo}$.

Let us define the integral operators corresponding to the RKHS's $\Hone$ and $\Htwo$ as follows:


\[
\begin{aligned}
    \mathcal{T}_{\repone} f &\coloneq \int K_{\repone}(\cdot,x) f(x) dP_{X}(x), \quad
% \end{aligned}
% \]
% and 
% \[
% \begin{aligned}
    \mathcal{T}_{\reptwo} f \coloneq \int K_{\reptwo}(\cdot,x) f(x) dP_{X}(x),
\end{aligned}
\]

for any $f \in \LPtwo$.
It is also easy to show that $\mathcal{T}_{\repone} = \mathfrak{I}_{\repone}\mathfrak{I}_{\repone}^{*}$ and $\mathcal{T}_{\reptwo} = \mathfrak{I}_{\reptwo}\mathfrak{I}_{\reptwo} ^{*}$. The boundedness and continuity of the kernel $K$ ensures that $\Sigma_{\repone}$, $\Sigma_{\reptwo}$, $\mathcal{T}_{\repone}$ and $\mathcal{T}_{\reptwo}$ are all compact trace-class operators, which consequently ensures that they are also Hilbert-Schmidt operators. Further, each of $\Sigma_{\repone}$, $\Sigma_{\reptwo}$, $\mathcal{T}_{\repone}$ and $\mathcal{T}_{\reptwo}$ are self-adjoint positive operators and therefore have a spectral representation \citep[Theorems~VI.16,VI.17]{reed1980methods}. For any $\lambda>0$, the regularized inverse covariance operators are defined as $\Sigma_{\repone}^{-\lambda} \coloneq \left(\Sigma_{\repone} + \lambda I\right)^{-1}$ and $\Sigma_{\reptwo}^{-\lambda} \coloneq \left(\Sigma_{\reptwo} + \lambda I\right)^{-1}$ , while the corresponding square roots are defined as  $\Sigma_{\repone}^{-\frac{\lambda}{2}} \coloneq \left(\Sigma_{\repone} + \lambda I\right)^{-\frac{1}{2}}$ and $\Sigma_{\reptwo}^{-\frac{\lambda}{2}} \coloneq \left(\Sigma_{\reptwo} + \lambda I\right)^{-\frac{1}{2}}$. Further, let us  define $\widetilde{K}_{\repone}(x,y) \coloneq \Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(x,y)$ and $\widetilde{K}_{\reptwo}(x,y)\coloneq \Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(x,y)$. For any $p \geq 1$, we will use $\norm{\cdot}_{\Lp(\mathcal{S})}$ to denote the $p$-Schatten norm of any operator mapping from its domain $\mathcal{S}$ into itself. In particular, for $p=1,2$ and $\infty$, the $p$-Schatten norm corresponds to the trace norm, Hilbert-Schmidt norm and the operator norm, respectively.

The \metricstname distance has the following characterization in terms of the integral operators, covariance operators and inclusion operators corresponding to the pullback kernels $K_{\repone}$ and $K_{\reptwo}$:
\begin{theorem}\label{Characterization of pseudometric in terms of operator norm short version}
Assume that the base kernel $K$ is defined on any Euclidean space and is positive definite, symmetric, bounded and continuous. Then, for any $\lambda>0$, the the UKP distance $ \dop (\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as 
\[
\begin{aligned}
     \dop(\repone,\reptwo) =& \norm{\Tonelambdainv \Tone - \Ttwolambdainv \Ttwo}_{\optwo}\\
     =& \norm{ \Irepone \Sonelambdainv \Ireponead -  \Ireptwo \Stwolambdainv \Ireptwoad }_{\optwo}.
\end{aligned}
\]
% where $X$ and $X^{\prime}$ are i.i.d observations drawn from $P_{X}$.
\end{theorem}

The proof is provided in Section \ref{Expressing the UKP distance in terms of RKHS operators} of the Appendix as part of the proof of a generalized version of this theorem (Theorem \ref{Characterization of pseudometric in terms of operator norm}). The above characterization shows that the \metricstname distance is the operator norm of the difference between a regularized/smoothed version of the integral operators corresponding to the pair of pullback RKHS's associated with the representations $\repone$ and $\reptwo$ via the base kernel $K$. 
Using the monotonicity properties of $p$-Schatten norms (See Proposition 2.1 of \citet{pfeiffer2021stability}) we can develop a hierarchy of distances (pseudometrics), which we call generalized \metricstname distances, corresponding to the choice of the Schatten norm $\norm{\cdot}_{\Lptwo}$ for any $ p \geq 1$, defined as follows:
\begin{definition}\label{Definition of pseudometric Lp norm}
    For any $\lambda>0$, choice of kernel $K(\cdot,\cdot)$ and $p \geq 1$, the $(\lambda,K,p)$-\metricstname (\metricfullname) distance between representations $\repone(X)$ and $\reptwo(X)$ is defined as, 
    \[
    \dLp(\repone,\reptwo) \coloneq \norm{\Tonelambdainv \Tone - \Ttwolambdainv \Ttwo}_{\Lptwo},
    \]
    where $\Tone$ and $\Ttwo$ are the integral operators corresponding to the pullback RKHS's $\Hone$ and $\Htwo$, respectively.
\end{definition}

Next, we show that the $(\lambda,K,p)$-\metricstname distance indeed satisfies the axioms of a pseudometric for any valid choice of $\norm{\cdot}_{\Lptwo}$ corresponding to $\lambda>0$ and $p \geq 1$. Of particular importance is the choice $p=2$, which corresponds to the Hilbert-Schmidt norm, since it leads to a pseudometric which can be efficiently estimated using i.i.d samples from $P_X$. The question regarding whether it is possible to develop an estimator of $\dop(\repone,\reptwo)$ is still open because of the challenges involving operator norm estimation and the lack of inner product structure in such a scenario.

\begin{theorem}\label{Theorem: Monotonicity and Pseudometricity short version}
    Assume that the setting of Theorem \ref{Characterization of pseudometric in terms of operator norm short version} holds true. Consider any three representations $\repone: \R^d \to \R^k$, $\reptwo: \R^d \to \R^l$ and $\repthree: \R^d \to \R^m$ for some $k,l,m \in \N$.  
    Then, for any $1 \leq p \leq \infty$, we have that
    \begin{equation}
    \begin{aligned}\label{monotonicity of family of UKP distances for p choices short version}
        \dop(\repone,\reptwo) \leq \dLp(\repone,\reptwo) \leq \done(\repone, \reptwo)
    \end{aligned}
    \end{equation}

    Further, the $\dLp(\repone,\reptwo)$ distance satisfies the following properties:
    \begin{enumerate}
        \item (Positivity) $\dLp(\repone,\repone) = 0$,
        \item (Non-negativity)  $\dLp(\repone,\reptwo) \geq 0$,
        \item (Symmetricity) $\dLp(\repone,\reptwo) = \dLp(\reptwo,\repone)$,
        \item (Triangle inequality)  $\dLp(\repone,\reptwo) \leq \dLp(\repone,\repthree) + \dLp(\repthree,\reptwo)$.
    \end{enumerate}
    Hence, $\dLp(\repone,\reptwo)$ is a pseudometric over the space of all functions that maps $\R^d$ to some Euclidean space $\R^t$ for any $t \in \N$.
    
    % Further, the $\dLp(\repone,\reptwo)$ distance satisfies the following properties:
    % \begin{enumerate}
    %     \item For any function $\repone: \R^d \to \R^k$ for some $k \in \N$, $\dLp(\repone,\repone) = 0$,
    %     \item (Non-negativity) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $\dLp(\repone,\reptwo) \geq 0$,
    %     \item (Symmetric) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $\dLp(\repone,\reptwo) = \dLp(\reptwo,\repone)$,
    %     \item (Triangle inequality) For any three functions $\repone: \R^d \to \R^k$, $\reptwo: \R^d \to \R^l$ and $\repthree: \R^d \to \R^m$ for some $k,l,m \in \N$, $\dLp(\repone,\reptwo) \leq \dLp(\repone,\repthree) + \dLp(\repthree,\reptwo)$.
    % \end{enumerate}
    % Hence, $\dLp(\repone,\reptwo)$ is a pseudometric over the space of all functions that maps $\R^d$ to some Euclidean space $\R^t$ for any $t \in \N$.
\end{theorem}

The proof is provided in Section \ref{Properties of the UKP distance} of the Appendix as part of the proof of a more general result (Theorem \ref{Theorem: Monotonicity and Pseudometricity}). We now analyze the invariance properties of the pseudometric $\dLp$ and identify the transformations of the representations $\repone$ and $\reptwo$ that leave its value unchanged. Based on the following theorem, we can identify representations that \metricstname treats as equivalent in terms of prediction-based performance for a general collection of kernel ridge regression tasks corresponding to a particular kernel $K$. We achieve this goal by deriving an exact characterization of the representations that lead to $\dLp=0$.

\begin{theorem}\label{Invariance of UKP distance}
     Assume that the setting of Theorem \ref{Characterization of pseudometric in terms of operator norm short version} holds true. Then, for any $p \geq 1$ and $\lambda>0$, given any two representations $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ we have that 
     \begin{equation}\label{Necessity and sufficiency of UKP distance being zero if and only if the integral operators coincide short version}
         \dLp(\repone,\reptwo) = 0 \,\ \textrm{ if and only if }\,\ \Tone = \Ttwo .
     \end{equation}

     Further, let $\mathcal{H}$ be the class of transformations under which the kernel $K$ is invariant, i.e., $\mathcal{H} = \left\{h : K(\cdot,\cdot) = K(h(\cdot),h(\cdot))\textrm{ a.e. } P_{X}\right\}$. Then, the \emph{\metricstname} distance $\dLp(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ is invariant under the same class of transformations that the kernel $K$ is invariant for, i.e., for any $h_{1},h_{2} \in \mathcal{H}$, 
\[
\dLp(h_{1} \circ \repone,h_{2} \circ \reptwo) = \dLpgl(\repone,\reptwo)
\]
and if either $h_{1}$ or $h_{2}$ does not belong to $\mathcal{H}$, \[
\dLp(h_{1} \circ \repone,h_{2} \circ \reptwo) \neq \dLp(\repone,\reptwo).
\]

Consequently, a necessary and sufficient condition for the \emph{\metricstname} distance $\dLpgl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ to be zero is that $K_{\repone}(\cdot,\cdot) = K_{\reptwo}(\cdot,\cdot)$ a.e. $P_{X}$.
\end{theorem}

Additionally we can also provide a bound on the sensitivity of risk functional, uniformly over a class of kernel ridge regression tasks (Refer to Theorem \ref{Risk bound} for more details). 

% induces an isometric embedding $\repone \mapsto \inprod{\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X),\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X^{\prime})}_{\Hone}$ of $\repone$ into $L^{2}(P_{X}^{\otimes 2})$. This characterization allows us to prove Theorem \ref{Proposition: Squared pseudometric using trace operators}, which will be useful throughout the rest of the paper.

Next, we show that the \metricstname distance $\dtwo(\repone,\reptwo)$ (corresponding to a Hilbert-Schmidt norm) can be expressed in terms of the trace operator, which will be essential for developing a statistical estimator of the pseudometric based on random samples from the input distribution $P_{X}$. 

To do so, we define the cross-covariance operators $\Sigma_{\repone\reptwo}: \Htwo \to \Hone$ and $\Sigma_{\reptwo\repone}: \Hone \to \Htwo$ as follows:
\[
\begin{aligned}
    \Sigma_{\repone\reptwo} &\coloneq\int K_{\repone}(\cdot,x) \otimes_{\HS(\Htwo,\Hone)}  K_{\reptwo}(\cdot,x) dP_{X}(x) \\
    =& \int K(\repone(\cdot),\repone(x)) \otimes_{\HS(\Htwo,\Hone)}  K(\reptwo(\cdot),\reptwo(x)) dP_{X}(x)
\end{aligned}
\]and
\[
\begin{aligned}
    \Sigma_{\reptwo\repone} &\coloneq \int K_{\reptwo}(\cdot,x) \otimes_{\HS(\Hone,\Htwo)} K_{\repone}(\cdot,x) dP_{X}(x)\\
    =& \int  K(\reptwo(\cdot),\reptwo(x))\otimes_{\HS(\Hone,\Htwo)}  K(\repone(\cdot),\repone(x))dP_{X}(x)
    = \Sigma_{\repone\reptwo}^{*}.
\end{aligned}
\]

\begin{theorem}\label{Squared pseudometric using trace operators}
    For any $\lambda>0$, the squared \emph{\metricstname} distance $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as
    \[
\begin{aligned}
        \left[d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo)\right]^{2}
        = \emph{Tr}\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\right) + \emph{Tr}\left(\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\right)-2\emph{Tr}\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo\repone}\right).
    \end{aligned}
    \]
\end{theorem}

The proof is provided in Section \ref{Properties of dtwo} of the Appendix, as part of the proof of a more general result (Theorem \ref{Squared dtwo pseudometric using trace operators general case}). 

% The following theorem serves to show that the \metricstname distance does satisfy the axioms of a pseudometric.

% \begin{theorem}\label{Theorem: Pseudometric}
%     For any $\lambda>0$, the $d_{\lambda,K}^{\emph{\metricstname}}$ distance satisfies the following properties:
%     \begin{enumerate}
%         \item For any function $\repone: \R^d \to \R^k$ for some $k \in \N$, $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\repone) = 0$,
%         \item (Non-negativity) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo) \geq 0$,
%         \item (Symmetric) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo) = d_{\lambda,K}^{\metricstname}(\reptwo,\repone)$,
%         \item (Triangle inequality) For any three functions $\repone: \R^d \to \R^k$, $\reptwo: \R^d \to \R^l$ and $\repthree: \R^d \to \R^m$ for some $k,l,m \in \N$, $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo) \leq d_{\lambda,K}^{\emph{\metricstname}}(\repone,\repthree) + d_{\lambda,K}^{\emph{\metricstname}}(\repthree,\reptwo)$.
%     \end{enumerate}
%     Hence, $d_{\lambda,K}^{\emph{\metricstname}}$ is a pseudometric over the space of all functions that maps $\R^d$ to some Euclidean space $\R^t$ for any $t \in \N$.
% \end{theorem}

% The proof is provided in Section \ref{Proof of Theorem 1} of the Appendix. We now analyze the invariance properties of the pseudometric $d_{\lambda,K}^{\text{\metricstname}}$ and identify the transformations of the representations $\repone$ and $\reptwo$ that leave its value unchanged. To this end, the following lemma will be useful, whose proof is provided in Section \ref{Proof Lemma 2} of the Appendix.

% \begin{lemma}\label{Invariance of regularized kernel inner product}

% Let $f: \R^d \to \R^k$ and $g: \R^d \to \R^l$ be any two functions. Consider a positive definite, symmetric, bounded and continuous kernel function $K(\cdot,\cdot)$ defined on the domain $\cup_{d} \left\{\mathcal{X}_{d} \times \mathcal{X}_{d}\right\}$, where $\mathcal{X}_{d} \subset \mathbb{R}^{d}$ is a separable space for $d \in \mathbb{N}$. Let $K_{f}(\cdot,\cdot) \coloneq K(f\cdot),f(\cdot))$ and $K_{g}(\cdot,\cdot) \coloneq K(g\cdot),g(\cdot))$ be the unique reproducing kernels corresponding to the ``pullback" RKHS's $\Hf \coloneq \Hil\left(K \circ \left(f \times f\right)\right)$ and $\Hg \coloneq \Hil\left(K \circ \left(g \times g\right)\right)$. For any $\lambda>0$, let $\Sigma_{f}^{-\frac{\lambda}{2}}$ and $\Sigma_{g}^{-\frac{\lambda}{2}}$ denote the square roots of the $\lambda$-regularized covariance operators corresponding to the kernels $K_{f}$ and $K_{g}$, respectively. For any $x,x^{\prime} \in \mathbb{R}^{d}$ and $\lambda>0$, define the operator $\mathcal{I}$ as follows: 
% \[
% \begin{aligned}
% \mathcal{I}(f)(x,x^{\prime}) =& \inprod{\Sigma_{f}^{-\frac{\lambda}{2}}K_{f}(\cdot,x),\Sigma_{f}^{-\frac{\lambda}{2}}K_{f}(\cdot,x^{\prime})}_{\Hf}  = \inprod{K_{f}(\cdot,x),\Sigma_{f}^{-\lambda}K_{f}(\cdot,x^{\prime})}_{\Hf}. 
% \end{aligned}
% \]
% Then, a necessary and sufficient condition for $f$ and $g$ to satisfy $ \mathcal{I}(f) = \mathcal{I}(g)$ is that $ K_{f}(\cdot, \cdot) = K_{g}(\cdot, \cdot)$. 
% \end{lemma}

% As an easy corollary of Lemma \ref{Invariance of regularized kernel inner product}, we can identify representations that \metricstname treats as equivalent in terms of prediction-based performance for a general collection of kernel ridge regression tasks corresponding to a particular kernel $K$.

% \begin{corollary}
% \label{corollary 1}
% Let $\mathcal{H}$ be the class of transformations under which the kernel $K$ is invariant, i.e., $\mathcal{H} = \left\{h : K(\cdot,\cdot) = K(h(\cdot),h(\cdot))\textrm{ a.e. } P_{X}\right\}$. Then, the \emph{\metricstname} distance $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ is invariant under the same class of transformations that the kernel $K$ is invariant for, i.e., for any $h_{1},h_{2} \in \mathcal{H}$, 
% \[
% d_{\lambda,K}^{\emph{\metricstname}}(h_{1} \circ \repone,h_{2} \circ \reptwo) = d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo)
% \]
% and if either $h_{1}$ or $h_{2}$ does not belong to $\mathcal{H}$, \[
% d_{\lambda,K}^{\emph{\metricstname}}(h_{1} \circ \repone,h_{2} \circ \reptwo) \neq d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo).
% \]
% \end{corollary}

% The proof of Corollary \ref{corollary 1} is provided in Section  \ref{Proof of Corollary 1} of the Appendix. Based on these results, the following corollary of Lemma \ref{Invariance of regularized kernel inner product} then provides an exact characterization of the representations that lead to $d_{\lambda,K}^{\text{\metricstname}}=0$.

% \begin{corollary}\label{corollary 2}
%     A necessary and sufficient condition for the \emph{\metricstname} distance $d_{\lambda,K}^{\emph{\metricstname}}(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ to be zero is that $K_{\repone}(\cdot,\cdot) = K_{\reptwo}(\cdot,\cdot)$ a.e. $P_{X}$.
% \end{corollary}

% The proof is straightforward, similar to that of Corollary \ref{corollary 1}, and is therefore omitted.



\section{Statistical estimation of $\dtwo$} \label{statistical estimation of UKP}

In practice, when comparing the prediction-based utility of different representations, we consider the realistic scenario where one only has access to a random sample $X_{1},\dots, X_{n} \overset{i.i.d}{\sim}P_{X}$ and a statistical estimator of the proposed distance measure is required. In supervised learning settings, the goal is to allocate most of the data for training and model fitting while minimizing the amount of data used for diagnostics and exploratory analysis. Using the empirical covariance and cross-covariance operators $\hat{\Sigma}_{\repone}$, $\hat{\Sigma}_{\reptwo}$, $\hat{\Sigma}_{\repone\reptwo}$ and $\hat{\Sigma}_{\reptwo\repone} = \hat{\Sigma}_{\repone\reptwo}^{*}$ as plug-in estimators of $\Sigma_{\repone}$, $\Sigma_{\reptwo}$, $\Sigma_{\repone\reptwo}$ and $\Sigma_{\reptwo\repone}$ in the trace operator based expression of $d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)$ as derived in Theorem \ref{Squared pseudometric using trace operators}, we arrive at the following V-statistic type estimator of $d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)$:
\begin{equation}\label{Pseudometric V-statistic estimator}
    \begin{aligned}
        \hat{d}_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)
        = \left[\Tr\left(\hat{\Sigma}_{\repone}^{-\lambda}\hat{\Sigma}_{\repone}\hat{\Sigma}_{\repone}^{-\lambda}\hat{\Sigma}_{\repone}\right) + \Tr\left(\hat{\Sigma}_{\reptwo}^{-\lambda}\hat{\Sigma}_{\reptwo}\hat{\Sigma}_{\reptwo}^{-\lambda}\hat{\Sigma}_{\reptwo}\right)-2\Tr\left(\hat{\Sigma}_{\repone}^{-\lambda}\hat{\Sigma}_{\repone\reptwo}\hat{\Sigma}_{\reptwo}^{-\lambda}\hat{\Sigma}_{\reptwo\repone}\right)\right]^{\frac{1}{2}},
    \end{aligned}
\end{equation}
where
\[
\begin{aligned}
    \hat{\Sigma}_{\repone}=\frac{1}{n}\sum_{i=1}^{n}K_{\repone}(\cdot,X_{i}) \otimes_{\Hone}  K_{\repone}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\repone(\cdot),\repone(X_{i})) \otimes_{\Hone} K(\repone(\cdot),\repone(X_{i})),
\end{aligned}
\]
\[
\begin{aligned}
    \hat{\Sigma}_{\reptwo}=\frac{1}{n}\sum_{i=1}^{n}K_{\reptwo}(\cdot,X_{i}) \otimes_{\Htwo}  K_{\reptwo}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\reptwo(\cdot),\reptwo(X_{i})) \otimes_{\Htwo} K(\reptwo(\cdot),\reptwo(X_{i})),
\end{aligned}\]
\[
\begin{aligned}
    \hat{\Sigma}_{\repone\reptwo}=\frac{1}{n}\sum_{i=1}^{n}K_{\repone}(\cdot,X_{i}) \otimes_{\HS(\Htwo,\Hone)} K_{\reptwo}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\repone(\cdot),\repone(X_{i})) \otimes_{\HS(\Htwo,\Hone)} K(\reptwo(\cdot),\reptwo(X_{i})),
\end{aligned}\] and 
\[\begin{aligned}
    \hat{\Sigma}_{\reptwo\repone} =&\frac{1}{n}\sum_{i=1}^{n} K_{\reptwo}(\cdot,X_{i}) \otimes_{\HS(\Hone,\Htwo)}  K_{\repone}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\reptwo(\cdot),\reptwo(X_{i}))\otimes_{\HS(\Hone,\Htwo)}  K(\repone(\cdot),\repone(X_{i}))\\
    =& \hat{\Sigma}_{\repone\reptwo}^{*}.
\end{aligned}\]

It is an easy exercise to show that the V-statistic type estimator $\hat{d}_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)$ can be expressed in terms of the number of input data points $n$, the chosen regularization parameter $\lambda$ and the empirical Gram matrices $K_{n,\repone}$ and $K_{n,\reptwo}$ whose $(i,j)$-th elements are the kernel evaluations for the $(i,j)$-th input data pair $(X_{i},X_{j})$, i.e., $\left(K_{n,\repone}\right)_{ij} = K(\repone(X_{i}),\repone(X_{j}))$ and $\left(K_{n,\reptwo}\right)_{ij} = K(\reptwo(X_{i}),\reptwo(X_{j}))$. If $\lambda=0$, one is required to ensure the invertibility of $K_{n,\repone}$ and $K_{n,\reptwo}$.

\begin{theorem}\label{Proposition: Estimator in terms of Gram matrices}
    For any $\lambda>0$, the V-statistic type estimator $\dtwohat(\repone,\reptwo)$ of $\dtwo(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as
    \[
    \begin{aligned}
        &\dtwohat(\repone,\reptwo)\\
        =& \left[\Tr\left(K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}\right) + \Tr\left(K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}\right)\right.\\
        &-2\left.\Tr\left(K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}\right)\right]^{\frac{1}{2}}.
    \end{aligned}
    \]
\end{theorem}

\subsection{Relation to other comparison measures} 
\label{Relation to other comparison measures}

In this subsection, we discuss the relationship between the \metricstname distance and some popular distances between representations that are popularly used in Machine Learning.
The \metricstname distance for the choice $p=2$ (i.e. the Hilbert-Schmidt norm based $\dtwo$) is a generalization of the GULP distance, as proposed in \citet{GULP}, in the sense that, if we choose the kernel for the \metricstname to be the linear kernel $K_{lin}(x,y) = x^{T}y$, we exactly recover the GULP distance. Our proposed pseudometric $\dLp$ provides the additional flexibility of choosing other kernel functions (such as the Gaussian RBF kernel $K_{RBF,h}$ and the Laplace $K_{Lap,h}$) additional norms as well as additional regularization choices for understanding the relative difference between the generalization performance on different classes of kernel ridge regression-based prediction tasks. 

Let $K_{n,\repone} = U_{\repone} \Lambda_{n,\repone}U_{\repone}^{T}$ and $K_{n,\reptwo} = U_{\reptwo} \Lambda_{n,\reptwo}U_{\reptwo}^{T}$ be the eigenvalue decompositions of $K_{n,\repone}$ and 
$K_{n,\reptwo}$, respectively. Here $\Lambda_{n,\repone} = \operatorname{diag}\left\{\mu_{\repone}^{(1)},\dots,\mu_{\repone}^{(n)}\right\}$ and $\Lambda_{n,\reptwo} = \operatorname{diag}\left\{\mu_{\reptwo}^{(1)},\dots,\mu_{\reptwo}^{(n)}\right\}$. Define $c_{\repone,\reptwo}^{(i),(j)} = \left(u_{\repone}^{(i)}\right)^{T}u_{\reptwo}^{(j)}$, as the inner product between the $i$-th eigenvector $u_{\repone}^{(i)}$ corresponding to the $i$-th eigenvalue $\mu_{\repone}^{(i)}$ of $K_{n,\repone}$ and $j$-th eigenvector $u_{\reptwo}^{(j)}$ corresponding to the $j$-th eigenvalue $\mu_{\reptwo}^{(i)}$  of $K_{n,\reptwo}$. In the following proposition, we express the V-statistic type estimator $\dtwohat(\repone,\reptwo)$ exclusively in terms of the inner products $c_{\repone,\reptwo}^{(i),(j)}$'s, the regularization parameter $\lambda$ and the eigenvalues $\mu_{\repone}^{(i)}$'s and $\mu_{\reptwo}^{(j)}$'s, which is useful for understanding the effect of changing the regularization parameter $\lambda$ on the estimate and its relation to other popular pseudometrics on the space of representations.

\begin{proposition}\label{Proposition: Estimator in terms of eigenvalues and eigenvectors of Gram matrices}
    For any $\lambda>0$, the V-statistic type estimator $\dtwohat(\repone,\reptwo)$ of $\dtwo(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as
    \[
    \begin{aligned}
        &\dtwohat(\repone,\reptwo)\\
        =& \left[\sum_{i=1}^{n} \left(\frac{\mu_{\repone}^{(i)}}{\mu_{\repone}^{(i)}+ n\lambda}\right)^{2} + \sum_{j=1}^{n} \left(\frac{\mu_{\reptwo}^{(j)}}{\mu_{\reptwo}^{(i)}+ n\lambda}\right)^{2}\right.-2\left.\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\mu_{\repone}^{(i)}\mu_{\reptwo}^{(j)}}{\left(\mu_{\repone}^{(i)} + n\lambda\right)\left(\mu_{\reptwo}^{(j)} + n\lambda\right)}\left(c_{\repone,\reptwo}^{(i),(j)} \right)^{2}\right]^{\frac{1}{2}}.
    \end{aligned}
    \]
\end{proposition}

The proof is straightforward, relying on the spectral decomposition of $K_{n,\repone}$ and $K_{n,\reptwo}$ and the properties of the trace operator, and is thus omitted.

The general kernelized version of the Ridge-CCA (Canonical Correlation Analysis) distance, introduced by \citet{vinod1976canonical} and later discussed in \citet{kuss2003geometry}, is defined as
\[
\begin{aligned}
 \hat{d}^{\text{RCCA}}_{\lambda,K}(\repone,\reptwo) =& \Tr\left(\hat{\Sigma}_{\repone}^{-\lambda}\hat{\Sigma}_{\repone\reptwo}\hat{\Sigma}_{\reptwo}^{-\lambda}\hat{\Sigma}_{\reptwo\repone}\right) 
    = \sum_{i=1}^{n}\sum_{j=1}^{n}\frac{\mu_{\repone}^{(i)}\mu_{\reptwo}^{(j)}}{\left(\mu_{\repone}^{(i)} + n\lambda\right)\left(\mu_{\reptwo}^{(j)} + n\lambda\right)}\left(c_{\repone,\reptwo}^{(i),(j)} \right)^{2}.
\end{aligned}
\]
However, the machine learning literature has largely focused on the original Ridge-CCA formulation with a linear kernel, as discussed in \citet{kornblith2019similarity}. The classical CCA distance $\hat{d}^{\text{CCA}}$ can be derived from the kernelized Ridge-CCA distance $\hat{d}^{\text{RCCA}}_{\lambda,K}$ by selecting a linear kernel and setting $\lambda=0$. From these definitions, it is clear that \metricstname is a distance measure on the Hilbert space of representations, while the kernelized Ridge-CCA serves as the corresponding inner product on the Hilbert space when the kernel and regularization parameter $\lambda$ are the same for both.

Another related notion of distance, as proposed in \citet{cristianini2001kernel} and popularized  by \citet{kornblith2019similarity}, is known as CKA (Centered Kernel Alignment) and is defined as 
\[
\begin{aligned}
    \hat{d}_{K}^{\text{CKA}}(\repone,\reptwo) = \frac{\Tr\left(K_{n,\repone}H_{n}K_{n,\reptwo}H_{n}\right)}{\sqrt{\Tr\left(K_{n,\repone}H_{n}K_{n,\repone}H_{n}\right) \Tr\left(K_{n,\reptwo}H_{n}K_{n,\reptwo}H_{n}\right)}}
\end{aligned}
\]
where $H_{n} = I_{n} - \frac{1}{n}1_{n}1_{n}^{T}$. We can equivalently express $\hat{d}_{K}^{\text{CKA}}(\repone,\reptwo) $ as 
\[
\begin{aligned}
    \hat{d}^{\text{CKA}}(\repone,\reptwo) = \frac{\sum_{i=1}^{n}\sum_{j=1}^{n}\mu_{\repone}^{(i)}\mu_{\reptwo}^{(j)}\left(c_{\repone,\reptwo}^{(i),(j)} \right)^{2}}{\sqrt{\sum_{i=1}^{n} \left(\mu_{\repone}^{(i)}\right)^{2}}\sqrt{\sum_{j=1}^{n} \left(\mu_{\reptwo}^{(j)}\right)^{2}}}.
\end{aligned}
\]

If the kernelized Ridge-CCA distance is normalized by dividing it by the product of the norms of the pair of representations, taking the regularization parameter $\lambda$ to $+\infty$ recovers the CKA measure $\hat{d}_{K}^{\text{CKA}}(\repone,\reptwo)$ in the limit. This can be shown by expressing $\dtwohat(\repone,\reptwo)$ and $\hat{d}_{K}^{\text{CKA}}(\repone,\reptwo)$ in terms of the eigenvalues and eigenvectors of the empirical Gram matrices $K_{n,\repone}$ and $K_{n,\reptwo}$ and then taking the limit as $\lambda \to +\infty$. The kernelized Ridge-CCA distance thus serves as a bridge between the CKA measure, interpreted as a normalized inner product, and the \metricstname distance, understood as an unnormalized pseudometric in the space of representations. This connection implies a linear correlation between the two measures for sufficiently high value of the regularization parameter. While the CKA and kernelized Ridge-CCA measures naturally reflect similarity between representations via inner products, the \metricstname distance offers a broader perspective. Beyond functioning as a distance on the space of representations, it provides a relative measure of generalization performance uniformly across a wide range of prediction tasks involving kernel ridge regression—something other comparison measures fail to deliver. The \metricstname pseudometric efficiently compares learned representations by quantifying generalization similarity without task-specific training, leveraging pseudometric properties for meaningful and efficient assessment.

% It is desirable for discrepancy measures to satisfy pseudometric properties, particularly when comparing representations or features learned by DNN models. The UKP metric enables the assessment of similarity in generalization performance between two representations, even if they were not directly compared during experiments. Most importantly, the \metricstname distance can differentiate between the generalization ability of models based on their associated representations/features without requiring any ``training'' on particular prediction-based tasks, which makes it efficient in terms of data and computational requirements.

% This is especially useful when a sequence of proposed models is compared to a baseline but not to each other. For instance, suppose $\phi_{1}$ represents a baseline model's representation. If one experimenter uses the UKP metric to compare $\phi_{1}$ with a second representation $\phi_{2}$, while another experimenter compares 
% $\phi_{1}$ with a third representation $\phi_{3}$, the triangle inequality provides an upper bound for the UKP distance between $\phi_{2}$ and $\phi_{3}$, even without directly comparing them. This eliminates the need for additional experiments, a valuable feature in the context of deep learning and large-scale data. In contrast, CKA cannot reuse such pairwise comparisons to approximate the similarity between $\phi_{2}$ and $\phi_{3}$. 



\subsection{Finite sample convergence rate of $\dtwohat$} \label{Finite sample convergence rate}

From a statistical estimation viewpoint, it is possible that the estimator $\hat{d}_{\lambda,K}^{\text{\metricstname}}$ converges to $d_{\lambda,K}^{\text{\metricstname}}$ as the number of data samples $X_{1},\dots,X_{n}$ from the input domain grows to infinity. In addition, we also provide a rate of convergence of the order of $O(\frac{1}{\sqrt{n}})$, which is a parametric rate of convergence. The following theorem, proved in Section \ref{Proof of Theorem 2} of the Appendix, combines these two results and consequently illustrates the finite sample concentration of the  estimator proposed in Equation \eqref{Pseudometric V-statistic estimator} around the population $\dtwo$.

\begin{theorem} \label{Theorem: Finite sample convergence}
    Let $\kappa$ be an upper bound on the kernel function $K(\cdot,\cdot)$. Then, for any $\lambda>0$ and $\delta>0$, with probability atleast $1-\delta$, the V-statistic estimator $\hat{d}_{\lambda}^{\emph{\metricstname}}(\repone,\reptwo)$ satisfies
    \[
    \begin{aligned}
    \left|\left(\dtwo(\repone,\reptwo)\right)^{2}-\left(\dtwohat(\repone,\reptwo)\right)^{2}\right|\leq \frac{8\kappa^{3}}{\lambda^{3}}\left[\frac{2\log(\frac{6}{\delta})}{n} + \sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right]+ \frac{4\kappa^{2}}{\lambda^{2}}\left[\frac{2}{n} + \sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right] .
    \end{aligned}
    \]
\end{theorem}
For details regarding computational complexity, refer to Appendix \ref{Computational Complexity}.
% \subsection{Computational complexity of $\dtwohat$}

% From the expression of the estimator $\dtwohat$ in Proposition \ref{Proposition: Estimator in terms of Gram matrices}, it can be shown that its computational complexity is $O(n^3)$, where $n$ is the sample size. Notably, the GULP distance proposed in \citet{GULP} shares the same complexity. The primary computational cost arises from inverting the Gram matrix, which can be reduced using kernel approximation techniques like Random Fourier Features (RFF) or Nystr\"{o}m approximation. For example, by using $D$ RFF samples from the spectral distribution of the kernel $K$ or $D$ subsamples from the $n$ data samples in the Nystr\"{o}m method, the complexity of the UKP distance estimator $\dtwohat(\phi,\psi)$ can be reduced from $O(n^3)$ to $O(nD^{2} + D^{3})$, which is significantly lower than $O(n^3)$ when $D \ll n$. Exploring the tradeoff between the statistical accuracy of \metricstname distance estimation and the computational efficiency of kernel approximation methods is a promising direction for future research.

\section{Experiments} \label{experiments}

 In this section, we present experimental results that showcase the efficacy of the \metricstname distance in identifying similarities and differences between representations relevant to generalization performance on prediction tasks. For simplicity, we have considered $\dtwo$ as the \metricstname pseudometric and $\gl$ as the Tikhonov regularizer. Additional experiments, including model architecture details and training, are provided in the Appendix. All computations were performed on a single A100 GPU using Google Colab.

\subsection{Ability of \metricstname to predict generalization performance by kernel ridge regression-based predictors} \label{MNIST experiments}
The \metricstname pseudometric gives a uniform bound on the difference in predictions generated by a pair of models, based on kernel ridge regression-based estimators that utilize the respective representations of the two models. It is a natural question to ask if this uniform or worst-case guarantee on the difference in prediction performance between representations is useful on a per-instance basis, i.e., given a specific kernel ridge regression task, whether the \metricstname distance is positively correlated with the generalization performance of different models.
We consider 50 fully-connected neural networks with ReLU activation, each having uniform widths of 200, 400, 700, 800, or 900 and depths ranging from 1 to 10. These networks are trained on 60,000 $28 \times 28$-pixel training images from the MNIST handwritten digits dataset \citep{deng2012mnist} for 50 epochs. Representations are then extracted from the penultimate (final hidden) layer of each network, and the CCA, linear CKA (CKA with a linear kernel), GULP, and UKP distances are estimated for each pair of representations using 5,000 test images from the same dataset.
\begin{figure}[h]
\begin{center}
\includegraphics[height=7cm]{Figures/krrgen_mnist/generalization_lambda0.01_sigma0.1.png}
\caption{Generalization of kernel ridge regression-based predictors is strongly positively correlated with \metricstname distance values. We report the average correlation across 10 random synthetic kernel ridge regression tasks. Error bars are negligibly small and hence not visible.}
\label{Generalization plot}
\end{center}
\vspace{-4mm}
\end{figure}
We create synthetic kernel ridge regression tasks where we randomly sample 5000 images and randomly assign a standard Gaussian label to each image to create the synthetic label/target vector. We obtain the kernel ridge regression estimator for each representation with ridge penalty $\lambda \in \{10^{-2},1\}$ and Gaussian RBF kernel with bandwidth $\sigma \in \{10^{-1}, 1\}$. The empirical mean of the squared difference between predictions based on a pair of representations (say $\repone$ and $\reptwo$) is then computed using 5000 test images to estimate $err_{\repone,\reptwo} = \E_{X \sim P_{X}}\left[\alpha_{\lambda}^{\repone}(X)-\alpha_{\lambda}^{\reptwo}(X)\right]^{2}$, where $\alpha_{\lambda}^{\repone}$ and $\alpha_{\lambda}^{\reptwo}$ are the kernel ridge regression based predictors. 
In Fig. \ref{Generalization plot}, we plot the Spearman's $\rho$ rank correlation coefficient between the $err_{\repone,\reptwo}$'s and the pairwise distances between the representations using CCA, linear CKA, GULP and UKP distances. For this particular regression task, we chose the synthetic ridge penalty to be $\lambda=10^{-2}$ and used a Gaussian RBF kernel with $\sigma=10^{-1}$. For the \metricstname distance, we use the Gaussian RBF kernel as the choice of kernel. We observe that the pairwise \metricstname distance is highly positively correlated with the collection of $err_{\repone,\reptwo}$'s, as evident from the large positive values of the blue bars, with the largest correlation being observed when the ridge penalty used in the \metricstname distance matches with the synthetic ridge penalty we chose, i.e., $\lambda=10^{-2}$. In contrast, GULP distances exhibit inconsistent behavior across varying levels of regularization, while CCA and linear CKA distances show a significantly weaker positive correlation with generalization performance. As expected, due to the relationship between CKA and \metricstname discussed in Section \ref{Relation to other comparison measures}, the CKA distance with a Gaussian RBF kernel performs comparably to UKP. 
Experiments with the remaining combinations of tuning parameters $\lambda$ and $\sigma$ are presented in Fig. \ref{MNIST generalization plots} in Section \ref{MNIST Experiments additional} of the Appendix, yielding qualitatively similar conclusions.
We also discuss the ability of \metricstname to identify differences in architectures and inductive biases in Appendix \ref{ImageNet experiments experiments}.
% \begin{figure}[t]
% \begin{center}
% \includegraphics[width=10cm]{Figures/DendogramtSNE_imagenet/DendogramandTSNE_for_UKP_dist_RBF_1.000000e+00_1.000000e+01.png}
% \caption{Clustering based on \metricstname distance is sensitive to differences in architectures of neural network models.}\label{DendrogramandtSNE}
% \end{center}
% \vspace{-4mm}
% \end{figure}\subsection{Ability of \metricstname to identify differences in architectures and inductive biases} \label{ImageNet experiments experiments}
%  A key source of inductive biases in neural network models is their architecture, with features such as residual connections and variations in convolutional filter complexity shaping the representations learned during training. As a pseudometric over feature space, the \metricstname distance is expected to capture intrinsic differences in these inductive biases, which are known to impact generalization performance across tasks. To explore this, we analyze representations from 35 pre-trained neural network architectures used for image classification, described in detail in Section \ref{Additional ImageNet experiments} of the Appendix.

%  We estimate pairwise \metricstname distances between model representations using 3,000 images from the validation set of the ImageNet dataset \citep{krizhevsky2012imagenet}, a regularization parameter $\lambda=1$ and a Gaussian kernel with bandwidth $\sigma=10$. The tSNE embedding method is then used to embed these representations into 2-D space utilizing the distance measures given by the \metricstname pseudometric. Concurrently, we perform an agglomerative (bottom-up) hierarchical clustering of the representations based on the pairwise \metricstname distances and obtain the corresponding dendrogram. We observe in Fig. \ref{DendrogramandtSNE} that similar architectures which share important properties, such as the Regnets and Resnets are clustered together, while they are well separated from smaller efficient architectures such as MobileNets and ConvNexts. This demonstrates that the \metricstname distance effectively captures notions of similarity and dissimilarity aligned with interpretable notions based on inductive biases. Further comparisons with baseline measures, such as GULP and CKA, presented in Fig. \ref{ImageNet dendrograms additional} in Section \ref{Additional ImageNet experiments} of the Appendix demonstrate that \metricstname often provides superior clustering quality. We would like to note here that the choice of the kernel function for the \metricstname pseudometric should be driven by the nature of inductive bias that will be useful for the tasks for which the representations/features of interest will be used. Additional discussion regarding kernel (and kernel parameter) selection is provide in Section \ref{Additional ImageNet experiments} of the Appendix.
\section{Conclusion and future work} \label{conclusion}
This paper introduces the \metricstname pseudometric, a novel method for comparing model representations based on their predictive performance in kernel ridge regression tasks. It is shown to be easily interpretable, efficient, and capable of encoding inductive biases, supported by theoretical proofs and experimental validation. Therefore, the \metricstname pseudometric can serve as an useful and versatile exploratory tool for comparison of model representations, including representations learnt by black-box models such as neural networks, deep learning models and Large Language Models (LLMs). In our forthcoming work, we develop a conditional V-statistic type of estimator based on sample splitting and derive more sophisticated covergence guarantees with possibly better dependence on $\lambda$. Other research directions include using \metricstname for model selection, hyperparameter tuning, and enhancing its computational efficiency for large-scale models, such as deep neural networks, to better suit real-world applications.

\bibliography{ref}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix: Proofs}\label{Proofs}
In this appendix, we present the missing proofs of the paper.

\subsection{Definitions and notations}

For constants $a$ and $b$, $a \lesssim b$ (\emph{resp.} $a \gtrsim b$) denotes that there exists a positive constant $c$ (\emph{resp.} $c'$) such that $a\leq cb$ (\emph{resp.} $a \geq c' b)$. $a \asymp b$ denotes that there exists positive constants $c$ and $c'$ such that $cb \leq a \leq c' b$. $[\ell]$ is used to denote $\{1,\ldots,\ell\}$. Let $\mathbf{1}_{A}$ denote the indicator function for the 

Given a topological space $\mathcal{X}$, let $M^{b}_{+}(\mathcal{X})$ denote the space of all finite non-negative Borel measures on $\mathcal{X}$. We denote the space of bounded continuous functions defined on $\mathcal{X}$ by $C_{b}(\mathcal{X})$. For any $\mu \in M^{b}_{+}(\mathcal{X})$, let $L^r(\mathcal{X},\mu)$ denote the Banach space of $r$-power $(r\geq 1)$ $\mu$-integrable functions. For $f \in L^r(\mathcal{X},\mu)\eqcolon L^r(\mu)$, we denote $L^r$-norm of $f$ as $\norm{f}_{L^r(\mu)}\coloneq (\int_{\mathcal{X}}|f|^r\,d\mu)^{1/r}$. $\mu^n := \mu \times \stackrel{n}{...} \times \mu$ denotes the $n$-fold product measure. The equivalence class of the function $f$ is defined as $[f]_{\sim}$ and consists of functions $g \in L^r(\mathcal{X},\mu)$ such that $\norm{f-g}_{L^r(\mu)}=0$. 

For any Hilbert space $H$, we denote the corresponding inner product and norm using $\langle \cdot,\cdot\rangle_{H}$ and $\norm{\cdot}_{H}$, respectively. For any two abstract Hilbert spaces $H_1$ and $H_2$, let $\mathcal{L}(H_1, H_2)$ denote the space of bounded linear operators mapping from $H_1$ to $H_2$ and $\HS(H_{1},H_{2})$ denote the space of Hilbert-Schmidt operators mapping from $H_1$ to $H_2$. For $M \in \mathcal{L}(H_1,H_2)$, its adjoint is denoted by $M^*$. $M \in \mathcal{L}(H) := \mathcal{L}(H,H)$ is called self-adjoint if $M^*=M$. For $M \in \mathcal{L}(H)$, $\operatorname{Tr}(M)$, $\norm{M}_{\mathcal{L}^2(H)}$, and $\norm{M}_{\mathcal{L}^{\infty}(H)}$ denote the trace, Hilbert-Schmidt and operator norms of $M$, respectively. For $x,y \in H$, $x \otimes_{H} y$ is an element of the tensor product space of $H \otimes H$ which can also be seen as an operator from $H \to H$ as $(x \otimes_{H}y)z=x\langle y,z \rangle_{H} $ for any $z \in H$. For any $M\in \mathcal{L}(H)$, we call it a positive definite (respectively, positive semi-definite) operator if $\inprod{f,Mf}_{H} >0$ (respectively, $\inprod{f,Mf}_{H} \geq 0$) for any $f \in H$.

\subsection{An introduction to spectral regularizers}\label{subsec: An introduction to spectral regularizers}

 Consider a spectral regularizer $\gl: [0,\infty) \to \R$ which is a real-valued function compatible with a symmetric, bounded, positive definite and continuous kernel $K$ with $\sup_x K(x,x) \leq \kappa$, in the sense that it satisfies the following regularity conditions, which are standard in the inverse problem and learning theory literature \citep{bauer2007regularization,hagrass2024spectral}

\begin{itemize}
    \item[] $\boldsymbol{(\SpectralAssumptionone)}$ $\sup _{x \in \Gamma}\left|x \gl(x)\right| \leq C_1$; \label{Assumption Spectral Regularizer A1}
    \item[] $\boldsymbol{(\SpectralAssumptiontwo)}$ $\sup _{x \in \Gamma}\left|\lambda \gl(x)\right| \leq C_2$; \label{Assumption Spectral Regularizer A2}
    \item[] $\boldsymbol{(\SpectralAssumptionthree)}$ $ \sup _{x \in \Gamma}\left|1-x \gl(x)\right| x^{2 \varphi} \leq C_3 \lambda^{2 \varphi}$ for $\varphi \in (0, \xi]$, \label{Assumption Spectral Regularizer A3}
    \item[] $\boldsymbol{(\SpectralAssumptionfour)}$ $ \gl(x) >0$ for $x \in \Gamma \setminus \left\{0\right\}$ and $\gl(0) \geq 0$, \label{Assumption Spectral Regularizer A4}
    \item[] $\boldsymbol{(\SpectralAssumptionfive)}$ $x \mapsto x \gl(x)$ is an injective function for $x \in \Gamma$ \label{Assumption Spectral Regularizer A5}
\end{itemize}
where $\Gamma:=[0, \kappa]$ and $C_1$, $C_2$ and $C_3$ are finite positive constants (all independent of $\lambda$). The constant $\xi$ is usually termed as the \emph{qualification} of $\gl$ and determines rates of convergence in the context of learning and hypothesis testing problems \citep{bauer2007regularization,hagrass2024spectral}. The intuition behind reasonable choices of $\gl$ is driven by the fact that we want $\gl(\mathcal{B})\mathcal{B} = \mathcal{B}\gl(\mathcal{B})$ to approximate the identity operator for small enough $\lambda$, which is ensured by Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$} and \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$}, since it ensures $\lim_{\lambda \to 0} x \gl(x) \asymp 1$ (See for details Lemma A.20 of \citet{hagrass2024spectral} for details). Assumption \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$} ensures that positive definite and positive semidefinite operators retain their respective definiteness properties after their eigenvalues are transformed/regularized by $\gl$. Finally, Assumption \hyperref[Assumption Spectral Regularizer A5]{$(\SpectralAssumptionfive)$} is a technical condition that is satisfied by popular spectral functions and its utility will be explained shortly, 

Using functional calculus, given any self-adjoint operator $\mathcal{B}: \mathcal{H} \to \mathcal{H}$ with the spectral representation, $\mathcal{B}=\sum_i \tau_i \psi_i \otimes_H \psi_i$ with $\left(\tau_i, \psi_i\right)_i$ being the eigenvalues and eigenfunctions of $\mathcal{B}$, we can define the spectral regularization of $\mathcal{B}$ as
\[
\gl(\mathcal{B}):=\sum_{i \geq 1} \gl\left(\tau_i\right)\left(\psi_i \otimes_{\mathcal{H}} \psi_i\right)+\gl(0)\left(\boldsymbol{I}-\sum_{i \geq 1} \psi_i \otimes_{\mathcal{H}} \psi_i\right)
\]

It is an easy exercise to show that if $\gl(0) \neq 0$, then $\gl(\mathcal{B})$ is invertible and self-adjoint. Further, if $\gl(0) > 0$, then $\gl(\mathcal{B})$ is positive definite. Finally, if $\gl(x) > 0$ for all $x \in \Gamma = [0, \kappa]$ and satisfies Assumption \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$} with $C_1 \leq 1$, then $[\gl(\mathcal{B})]^{-1} - \mathcal{B}$ is self-adjoint and positive semi-definite. Additionally, if $C_1 < 1$, then $[\gl(\mathcal{B})]^{-1} - \mathcal{B}$ is invertible, self-adjoint and positive definite. Finally, Assumption \hyperref[Assumption Spectral Regularizer 5]{$(\SpectralAssumptionfive)$} guarantees that two self-adjoint positive semi-definite operators $\mathcal{B}_1$ and $\mathcal{B}_2$ are equal if and only if they share the same eigenfunctions and $\mathcal{B}_1 g_{\lambda}(\mathcal{B}_1) = \mathcal{B}_2 g_{\lambda}(\mathcal{B}_2)$.

A popular example of $\gl$ is $\glTik(x)=\frac{1}{x+\lambda}$, yielding $\glTik(\mathcal{B})=(\mathcal{B}+\lambda \boldsymbol{I})^{-1}$, which is well known as the Tikhonov regularizer. Another example is the Showalter regularizer, $\glSh(x)=\frac{1-e^{-x / \lambda}}{x} \mathbf{1}_{\{x \neq 0\}}+\frac{1}{\lambda} \mathbf{1}_{\{x=0\}}$. For both these regularizers, $\gl(0) = \frac{1}{\lambda} > 0$. Note that the spectral cutoff regularizer is defined as $\glCut(x)=\frac{1}{x} \mathbf{1}_{\{x \geq \lambda\}}$ satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$} and \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} but $\glCut(x)=0$ for all $x < \lambda$ and $x \glCut(x) = 1$ for all $x \geq \lambda$.

\subsection{Preliminary results involving RKHS and RKHS-related operators}

\begin{lemma}\label{Elementary properties of inclusion, covariance and integral operators}
    For any representations $\rep : \R^d \to \R^m$ and $\vartheta: \R^d \to \R^t$, and positive definite, symmetric, bounded and continuous kernel $K$ defined on any Euclidean space, let $K_{\rep}(\cdot,\cdot) \coloneq K(\rep(\cdot),\rep(\cdot))$ and $K_{\vartheta}(\cdot,\cdot) \coloneq K(\vartheta(\cdot),\vartheta(\cdot))$ be the unique reproducing kernels corresponding to the ``pullback'' RKHS's $\Hrep \coloneq \Hil\left(K \circ \left(\rep \times \rep\right)\right)$ and $\mathcal{H}_{\vartheta} \coloneq \Hil\left(K \circ \left(\vartheta \times \vartheta\right)\right)$, respectively. Let $\Irep: \Hrep \to \LPtwo, f \to f$ and $\mathfrak{I}_{\vartheta}: \mathcal{H}_{\vartheta} \to \LPtwo, f \to f$ be the corresponding inclusion operators and $\Irepad:\LPtwo \to \Hrep$ and $\mathfrak{I}_{\vartheta}^{*}:\LPtwo \to \mathcal{\vartheta}$ be their corresponding adjoint operators. Define $\Srep =\Irepad\Irep$ to be the covariance operator and $\Trep = \Irep \Irepad$ to be the integral operator corresponding to the RKHS $\Hrep$. Then, both $\Srep: \Hrep \to \Hrep$ and $\Trep: \LPtwo \to \LPtwo$ are compact, self-adjoint and positive semi-definite operators. Further, define $\Sigma_{\rep\vartheta} = \Irepad\mathfrak{I}_{\vartheta}$ to be the cross-covariance operator mapping from $\mathcal{H}_{\vartheta}$ to $\Hrep$. Finally, consider a spectral regularizer $\gl$ that satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$}, \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} , \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$} and \hyperref[Assumption Spectral Regularizer A5]{$(\SpectralAssumptionfive)$}. Then, both $\gl(\Srep)$ and $\gl(\Trep)$ are also compact and self-adjoint operators.
    
    Then, we have that
    \begin{enumerate}[label=(\roman*)] 
        \item \label{Elementary prop 1}
        $\Irepad:\LPtwo \to \Hrep, f \to \int K_{\rep}(\cdot,x)f(x)dP_{X}(x)$
        \item \label{Elementary prop 2} $\Sreplambdainv \Irepad = \Irepad \Treplambdainv$
        \item \label{Elementary prop 3} $\Irep \Sreplambdainv = \Treplambdainv \Irep$
        \item \label{Elementary prop 4} $\Srep \Sreplambdainv = \Sreplambdainv \Srep$
        \item \label{Elementary prop 5} $\Trep \Treplambdainv = \Treplambdainv \Trep$
        \item \label{Elementary prop 6} $\gl(\Srep) \Irepad = \Irepad \gl(\Trep)$
        \item \label{Elementary prop 7} $\Irep \gl(\Srep) = \gl(\Trep) \Irep$
        \item \label{Elementary prop 8} $\gl(\Srep) \Srep = \Srep \gl(\Srep)$
        \item \label{Elementary prop 9} $\gl(\Trep) \Trep = \Trep \gl(\Trep)$
        \item \label{Elementary prop 10} $\Sigma_{\rep\vartheta} = \int K_{\rep}(\cdot,x) \otimes_{\HS(\mathcal{H}_{\vartheta},\Hrep)}  K_{\vartheta}(\cdot,x) dP_{X}(x) = \int K(\rep(\cdot),\rep(x)) \otimes_{\HS(\mathcal{H}_{\vartheta},\Hrep)}  K(\vartheta(\cdot),\vartheta(x)) dP_{X}(x)$
    \end{enumerate}
\end{lemma}

\begin{proof}
    For any $f \in \LPtwo$ and $g \in \Hrep$, we have, by the definition of the adjoint of the inclusion operator $\Irep$,
    \begin{equation*}
    \begin{aligned}
        \inprod{\Irepad f, g}_{\Hrep} =& \inprod{f, \Irep g}_{\LPtwo}\\
        =& \int f(x)g(x)dP_{X}(x)\\
        =& \int f(x)\inprod{K_{\rep}(\cdot,x),g}_{\Hrep}dP_{X}(x)\\
        =& \inprod{\int K_{\rep}(\cdot,x) f(x)dP_{X}(x),g}_{\Hrep}
    \end{aligned}
    \end{equation*}
    This proves \ref{Elementary prop 1}.
    
    Note that, $\Irepad (\Irep \Irepad + \lambda I) = (\Irepad \Irep + \lambda I)\Irepad$. By rearrangement, we obtain $(\Irepad \Irep + \lambda I)^{-1} \Irepad = \Irepad (\Irep \Irepad + \lambda I)^{-1}$, which proves \ref{Elementary prop 2}. Computing the adjoint of both sides of \ref{Elementary prop 2} yields \ref{Elementary prop 3}.

    Since $K$ is a positive definite, symmetric, continuous and bounded kernel defined on a separable domain (the Euclidean space), the integral operator $\Trep$ is a compact, self-adjoint and trace-class operator. Consequently, $\Trep$ admits a spectral decomposition. Let $\left(\mu_{i}^{\rep},e_{i}^{\rep}\right)_{i=1}^{\infty}$ be the eigenvalue-eigenfunction pairs corresponding to the spectral decomposition of $\Trep$.
    Then, we have that
    \begin{equation*}\label{Spectral decomposition of Trep}
        \Trep = \sum_{i=1}^{\infty} \mu_{i}^{\rep} \left(e_{i}^{\rep} \otimes_{\LPtwo} e_{i}^{\rep}\right)
    \end{equation*}
    
    Further, $(e_{i}^{\rep})_{i=1}^{\infty}$ constitutes an orthonormal basis of $\LPtwo$ and we must have that $\mu_{i}^{\rep} > 0$ and $\lim_{i \to \infty} \mu_{i}^{\rep} = 0$.

    Using the fact that $\Irepad \Trep e_{i}^{\rep} = \Irepad \Irep (\Irepad e_{i}^{\rep}) = \Srep (\Irepad e_{i}^{\rep})$ and $\norm{\Irepad e_{i}^{\rep}}_{\Hrep} = (\mu_{i}^{\rep})^{\frac{1}{2}}$, we have that $\Srep$ is also a compact, self-adjoint and trace-class operator which admits the following spectral decomposition 
    \begin{equation*}\label{Spectral decomposition of Srep}
        \Srep = \sum_{i=1}^{\infty} \mu_{i}^{\rep} \left(\frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}} \otimes_{\Hrep} \frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}}\right)
    \end{equation*}

    Based on these spectral decompositions of $\Trep$ and $\Srep$, we can readily derive that
    \begin{equation*}
        \Srep \Sreplambdainv = \sum_{i=1}^{\infty} \frac{\mu_{i}^{\rep}}{\mu_{i}^{\rep} + \lambda} \left(\frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}} \otimes_{\Hrep} \frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}}\right)=\Sreplambdainv \Srep
    \end{equation*}
    and 
    \begin{equation*}
        \Trep \Treplambdainv = \sum_{i=1}^{\infty} \frac{\mu_{i}^{\rep}}{\mu_{i}^{\rep} + \lambda} \left(e_{i}^{\rep} \otimes_{\LPtwo} e_{i}^{\rep}\right)=\Treplambdainv \Trep
    \end{equation*}
   which completes the proof of \ref{Elementary prop 4} and \ref{Elementary prop 5}. 

   Further, the spectral decompositions of $\gl(\Trep)$ and $\gl(\Srep)$ are given by
   \begin{equation*}\label{Spectral decomposition of spectral reg of Trep}
        \gl(\Trep) = \sum_{i=1}^{\infty} \gl(\mu_{i}^{\rep}) \left(e_{i}^{\rep} \otimes_{\LPtwo} e_{i}^{\rep}\right) + \gl(0) \left[I-  \sum_{i=1}^{\infty}  \left(e_{i}^{\rep} \otimes_{\LPtwo} e_{i}^{\rep}\right) \right]
    \end{equation*}
    and
    \begin{equation*}\label{Spectral decomposition of spectral reg of Srep}
        \gl(\Srep) = \sum_{i=1}^{\infty} \gl(\mu_{i}^{\rep}) \left(\frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}} \otimes_{\Hrep} \frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}}\right) + \gl(0) \left[I-  \sum_{i=1}^{\infty} \left(\frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}} \otimes_{\Hrep} \frac{\Irepad e_{i}^{\rep}}{\sqrt{\mu_{i}^{\rep}}}\right) \right]
    \end{equation*}
    which readily demonstrate the compactness and self-adjointness of these operators.

    Using the above expressions of $\gl(\Trep)$ and $\gl(\Srep)$, using the fact that $\Irep \Irepad e_{i}^{\rep} = \mu_{i}^{\rep} e_{i}^{\rep}$ and some elementary rearrangements, we can obtain \ref{Elementary prop 6}. Computing the adjoints of both sides of \ref{Elementary prop 6}, we obtain \ref{Elementary prop 7}. The commutativity properties \ref{Elementary prop 8} and \ref{Elementary prop 9} are also readily apparent from the spectral decompositions of $\gl(\Trep)$ and $\gl(\Srep)$.

   Finally, note that, for any $f \in \mathcal{H}_{\vartheta}$, we have that \begin{equation*}
   \begin{aligned}
   \Sigma_{\rep\vartheta} =& \Irepad \mathfrak{I}_{\vartheta} f\\
   =&  \int K_{\rep}(\cdot,x) f(x) dP_{X}(x)\\
   =& \int K_{\rep}(\cdot,x) \inprod{ K_{\vartheta}(\cdot,x),f}_{\mathcal{H}_{\vartheta}} dP_{X}(x)\\
   =&  \left[\int K_{\rep}(\cdot,x) \otimes_{\HS(\mathcal{H}_{\vartheta},\Hrep)}  K_{\vartheta}(\cdot,x) dP_{X}(x)\right]f \\
   =& \left[\int K(\rep(\cdot),\rep(x)) \otimes_{\HS(\mathcal{H}_{\vartheta},\Hrep)}  K(\vartheta(\cdot),\vartheta(x)) dP_{X}(x) \right]f
   \end{aligned}
   \end{equation*}
    which completes the proof of \ref{Elementary prop 10}.
\end{proof}

\begin{lemma}\label{Elementary properties of sampling, empirical covariance operator and kernel Gram matrix}
    For any representations $\rep : \R^d \to \R^m$ and $\vartheta: \R^d \to \R^t$, and positive definite, symmetric, bounded and continuous kernel $K$ defined on any Euclidean space, let $K_{\rep}(\cdot,\cdot) \coloneq K(\rep(\cdot),\rep(\cdot))$ and $K_{\vartheta}(\cdot,\cdot) \coloneq K(\vartheta(\cdot),\vartheta(\cdot))$ be the unique reproducing kernels corresponding to the ``pullback'' RKHS's $\Hrep \coloneq \Hil\left(K \circ \left(\rep \times \rep\right)\right)$ and $\mathcal{H}_{\vartheta} \coloneq \Hil\left(K \circ \left(\vartheta \times \vartheta\right)\right)$, respectively. Given $n$ i.i.d samples $\left\{X_i\right\}_{i=1}^n \sim P_X^n$, let $\Samprep: \Hrep \to \R^n, f \to \frac{1}{\sqrt{n}} (f(X_1), \dots, f(X_n))^{\top}$ and $\Sampb: \Hb \to \R^n, f \to \frac{1}{\sqrt{n}} (f(X_1), \dots, f(X_n))^{\top}$ be the corresponding sampling operators, and let $\Samprepad:\R^n \to \Hrep$ and $\Sampbad:\R^n \to \Hb$ be their adjoint operators. Define $\Srephat =\Samprepad\Samprep$ to be the empirical covariance operator corresponding to the RKHS $\Hrep$ and $\Sabhat = \Samprepad\Sampb$ to be the empirical cross-covariance operator corresponding to the RKHS's mapping from $\mathcal{H}_{\vartheta}$ to $\Hrep$. 

    Further, define the empirical Gram matrices $\Knrep$ and $\Knb$ whose respective $(i,j)$-th elements are the kernel evaluations for the $(i,j)$-th input data pair $(X_{i},X_{j})$, i.e., $\left(\Knrep\right)_{ij} = K(\repone(X_{i}),\repone(X_{j}))$ and $\left(\Knb\right)_{ij} = K(\vartheta(X_{i}),\vartheta(X_{j}))$.
    
    Then, both $\Srephat: \Hrep \to \Hrep$ and $\Knrep: \R^n \to \R^n$ are compact, self-adjoint and positive semi-definite operators. Further, we have that
    \begin{enumerate}[label=(\roman*)] 
        \item \label{Sampling Elementary prop 1}
        $\Samprepad : \R^n \to \Hrep, \alpha = (\alpha_1,\dots,\alpha_n)^{\top} \to \frac{1}{\sqrt{n}}\sum_{i=1}^{n}\alpha_i K_{\rep}(\cdot,X_i)$
        \item \label{Sampling Elementary prop 2}
        $\frac{1}{n}\Knrep = \Samprepad\Samprep$
        \item \label{Sampling Elementary prop 3} $\Srephat=\frac{1}{n}\sum_{i=1}^{n}K_{\rep}(\cdot,X_{i}) \otimes_{\Hrep}  K_{\rep}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\rep(\cdot),\rep(X_{i})) \otimes_{\Hrep} K(\rep(\cdot),\rep(X_{i}))$
        \item \label{Sampling Elementary prop 4} $\Sabhat=\frac{1}{n}\sum_{i=1}^{n}K_{\rep}(\cdot,X_{i}) \otimes_{\HS(\Hb,\Hrep)} K_{\vartheta}(\cdot,X_{i})
    =\frac{1}{n}\sum_{i=1}^{n}K(\rep(\cdot),\rep(X_{i})) \otimes_{\HS(\Hb,\Hrep)} K(\vartheta(\cdot),\vartheta(X_{i}))$
        \item \label{Sampling Elementary prop 5} $\Samprepad (\Samprep \Samprepad + \lambda I)^{-1} = \Samprepad\Knblambdainv = \Sreplambdainvhat \Samprepad = (\Samprepad \Samprep+\lambda I)^{-1}\Samprepad$
        \item \label{Sampling Elementary prop 6} $ (\Samprep \Samprepad + \lambda I)^{-1}\Samprep = \Knblambdainv \Samprep = \Samprep \Sreplambdainvhat = \Samprep (\Samprepad \Samprep+\lambda I)^{-1}$
        \item \label{Sampling Elementary prop 7} $\gl(\Srephat) \Irepad = \Irepad \gl(\frac{1}{n}\Knrep)$
        \item \label{Sampling Elementary prop 8} $\Irep \gl(\Srephat)  = \gl(\frac{1}{n}\Knrep) \Irep$
        \item \label{Sampling Elementary prop 9} $\gl(\Srephat) \Srephat = \Srephat \gl(\Srephat)$
        \item \label{Sampling Elementary prop 10} $\gl(\frac{1}{n}\Knrep)\Knrep = \Knrep \gl(\frac{1}{n}\Knrep)$
        \item \label{Sampling Elementary prop 11} For any operator $\mathcal{A} : \R^n \to \R^n$, $\Tr(\Samprepad A \Samprep) = \Tr( A \Samprep \Samprepad)$
    \end{enumerate}
\end{lemma}

\begin{proof} For any $f \in \Hrep$ and $\alpha \in \R^n$, we have, by definition of $\Samprep$ and adjoint operators, $\inprod{\Samprepad \alpha, g}_{\Hrep}=\inprod{\alpha,\Samprep g}_{2}=\frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i g\left(X_i\right)= \frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i \inprod{K_{\rep}(\cdot,X_i),g}_{\Hrep} = \inprod{\frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i K_{\rep}(\cdot,X_i),g}_{\Hrep}$. This completes the proof of \ref{Sampling Elementary prop 1}

The result in \ref{Sampling Elementary prop 2} follows directly from the definitions of $\Knrep$, $\Samprep$ and $\Samprepad$. The rest of the proofs follow the same techniques used to prove Lemma \ref{Elementary properties of inclusion, covariance and integral operators}.
\end{proof}

\subsection{Expressing the \metricstname distance in terms of RKHS operators}\label{Expressing the UKP distance in terms of RKHS operators}



\begin{lemma}\label{Regularized regression solution}
 Let $Y$ be the random real-valued response corresponding to the input $X$ generated from the nonparametric regression model $Y=\eta(X) + \epsilon$, where $\epsilon$ is mean-zero noise and $\eta(x) = \E(Y \mid X = x)$ is the population regression function of $Y$ on $X$. For any $\lambda>0$, consider a spectral regularizer $\gl$ that satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$}, \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} , \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$} and \hyperref[Assumption Spectral Regularizer A5]{$(\SpectralAssumptionfive)$} with $C_1 \leq 1$ and $\gl(0)>0$.
 
 Given a representation $\rep : \R^d \to \R^m$, and positive definite, symmetric, bounded and continuous base kernel $K$ defined on any Euclidean space, let $\alpha_{\gl}^{\rep}$ be the population kernel ridge regression estimator of the regression function $\eta$ using the pullback kernel $K_{\rep}(\cdot,\cdot) = K(\rep(\cdot),\rep(\cdot))$, defined as 
\begin{equation}\label{Definition of alpha general case}
    \alpha_{\gl}^{\rep} = \underset{f \in \Hrep}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \norm{(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f}_{\Hrep}^{2}
\end{equation}
can be expressed as $\alpha_{\gl}^{\rep} = \gl(\Srep) \Irepad \eta \in \Hrep$. Consequently, as an element of $\Hrep$ embedded into $\LPtwo$, $\alpha_{\gl}^{\rep}$ can be expressed as \begin{equation}\label{Expression of alpha general case}
\Irep \alpha_{\gl}^{\rep} = \Irep \gl(\Srep) \Irepad \eta =\Trep \gl(\Trep) \eta =  \gl(\Trep) \Trep \eta \in \LPtwo.
\end{equation}

In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, we can express the solution to Equation \ref{Definition of alpha general case} as
\begin{equation}\label{Definition of alpha Tikhonov}
    \alpha_{\lambda}^{\rep} \coloneq \alpha_{\glTik} = \underset{f \in \Hrep}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \lambda \norm{f}_{\Hrep}^{2} = \Sreplambdainv \Irepad\eta .
\end{equation}
\end{lemma}
 Consequently, as an element of $\Hrep$ embedded into $\LPtwo$, $\alpha_{\lambda}^{\rep}$ can be expressed as \begin{equation}\label{Expression of alpha Tikhonov}\Irep \alpha_{\lambda}^{\rep} = \Irep \Sreplambdainv \Irepad \eta =\Trep \Treplambdainv \eta =   \Treplambdainv \Trep \eta \in \LPtwo.
 \end{equation}

\begin{proof}
    Consider a fixed population regression function $\eta(x) = \E(Y \mid X = x)$ corresponding to a fixed joint distribution $P_{XY}$ of $(X, Y)$ with marginal distribution of $X$ as $P_X$. Note that, under the given conditions on the spectral regularizer $\gl$ and the kernel $K$, $\gl(\Srep)$ is an invertible, self-adjoint and positive-definite operator, while $\gl(\Srep)^{-1} - \Srep$ is a self-adjoint and positive semi-definite operator.
    
    Now, for any $f \in \Hrep$, we have
    \[
    \begin{aligned}
        &\E\left[Y-f(X)\right]^{2} + \norm{(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f}_{\Hrep}^{2}\\
        =& \E\left[Y-\inprod{f,K_{\rep}(\cdot,X)}_{\Hrep}\right]^{2} +\inprod{(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f,(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f}_{\Hrep}\\
        =&\E(Y^{2}) - 2\E\left[Y\inprod{f,K_{\rep}(\cdot,X)}_{\Hrep}\right] + \E\left[\inprod{f,K_{\rep}(\cdot,X)}_{\Hrep}^{2}\right] + \inprod{f,(\gl(\Srep)^{-1} - \Srep)f}_{\Hrep}\\
        =&\E(Y^{2}) - 2\E\left[\eta(X)\inprod{f,K_{\rep}(\cdot,X)}_{\Hrep}\right] + \E\inprod{f,\left[K_{\rep}(\cdot,X) \otimes_{\Hrep}K_{\rep}(\cdot,X)\right] f}_{\Hrep} \\
        &+ \inprod{f,(\gl(\Srep)^{-1} - \Srep)f}_{\Hrep}\\
        =&\E(Y^{2}) - 2\inprod{f,\I_{\rep}^{*}\eta}_{\Hrep} + \inprod{f,\Srep f}_{\Hrep} + \inprod{f,(\gl(\Srep)^{-1} - \Srep)f}_{\Hrep}\\
        =&\E(Y^{2}) - 2\inprod{f,\I_{\rep}^{*}\eta}_{\Hrep} + \inprod{f,(\Srep + \gl(\Srep)^{-1} - \Srep)f}_{\Hrep}\\
        =&\E(Y^{2}) - 2\inprod{\gl(\Srep)^{-\frac{1}{2}}f,\gl(\Srep)^{\frac{1}{2}}\I_{\rep}^{*}\eta}_{\Hrep} + \inprod{\gl(\Srep)^{-\frac{1}{2}}f,\gl(\Srep)^{-\frac{1}{2}}f}_{\Hrep}\\
        =&\E(Y^{2}) + \norm{\gl(\Srep)^{-\frac{1}{2}}f - \gl(\Srep)^{\frac{1}{2}}\I_{\rep}^{*}\eta}_{\Hrep}^{2} - \norm{\gl(\Srep)^{\frac{1}{2}}\I_{\rep}^{*}\eta}_{\Hrep}^{2}.
    \end{aligned}
    \]

Therefore, the kernel ridge regression estimator of $\eta$ using the representation $\rep(X)$ and the pullback kernel $K_{\rep}$ is given by 
\[
\begin{aligned}
    \alpha_{\gl}^{\rep} = & \underset{f \in \Hrep}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \norm{(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f}_{\Hrep}^{2}
    = \gl(\Srep)\I_{\rep}^{*}\eta \in \Hrep.
\end{aligned}
\]
Using the inclusion operator $\I_{\rep}$, we can embed $\alpha_{\gl}^{\rep}$ in $\LPtwo$. Using this fact, together with Parts \ref{Elementary prop 6} and \ref{Elementary prop 9} of Lemma \ref{Elementary properties of inclusion, covariance and integral operators}, we have that
\[\Irep \alpha_{\lambda}^{\rep} = \Irep \Sreplambdainv \Irepad \eta =\Trep \Treplambdainv \eta =   \Treplambdainv \Trep \eta.\]

In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, we can readily observe that $\gl(\Srep) = \Sreplambdainv$ and $\gl(\Srep)^{-1} - \Srep = \lambda I$. Substituting these expressions into the derivation of the closed form expression of $\alpha_{\gl}^{\rep}$ above leads to the closed form solution for $\alpha_{\lambda}^{\rep} = \alpha_{\glTik}^{\rep}$ and yields the desired result, .    
\end{proof}

\begin{remark}\label{rem: low pass spcetral filer of regression function}
    The population kernel ridge regression estimator of the regression function $\eta$ using the pullback kernel $K_{\rep}(\cdot,\cdot)$ given by $\alpha_{\gl}^{\rep} = \gl(\Srep) \Irepad \eta$ and its $\LPtwo$ embedding $\Irepad \alpha_{\gl}^{\rep}=\Trep \gl(\Trep) \eta =  \gl(\Trep) \Trep \eta$ is defined not only when the kernel regression problem in Equation \ref{Expression of alpha general case} is well-posed \Big(i.e. $\gl(\Srep)^{-1} - \Srep$ is positive semi-definite, which is equivalent to convexity of the RKHS norm based penalty $\norm{(\gl(\Srep)^{-1} - \Srep)^{\frac{1}{2}}f}_{\Hrep}^{2}$ \Big), but is well-defined in even more relaxed settings when the regularized kernel regression problem is ill-posed. In general, $\alpha_{\gl}^{\rep}$ and $\alpha_{\lambda}^{\rep}$ can be interpreted as a low-pass spectral filter applied to the true regression function $\eta$ that damp out the contribution of $\eta$ along the ``low energy" or less important eigenfunctions of $\Srep$ (energy being a measure of the strength of alignment of $\eta$ with a particular eigenfunction and is proportional to the magnitude of the corresponding eigenvalue) and retain/emphasize the contribution of $\eta$ along the more the important eigenfunctions of $\Srep$ (See Sections 3 and 4 of \citet{SpecAlgGerfo2008}  for a detailed discussion).
\end{remark}

 For any $\lambda>0$, consider a spectral regularizer $\gl$ that satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$}, \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} , \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$} and \hyperref[Assumption Spectral Regularizer A5]{$(\SpectralAssumptionfive)$} with $C_1 \leq 1$ and $\gl(0)>0$. Given two representations $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$, and positive definite, symmetric, bounded and continuous base kernel $K$ defined on any Euclidean space, let $\alpha_{\gl}^{\repone}$ and $\alpha_{\gl}^{\reptwo}$ be the population kernel ridge regression estimators of the regression function $\eta$ using their respective pullback kernels $K_{\repone}(\cdot,\cdot) = K(\repone(\cdot),\repone(\cdot))$ and $K_{\reptwo}(\cdot,\cdot) = K(\reptwo(\cdot),\reptwo(\cdot))$, defined as
 \begin{equation}\label{Definition of alpha general case repone}
    \alpha_{\gl}^{\repone} = \underset{f \in \Hone}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \norm{(\gl(\Sone)^{-1} - \Sone)^{\frac{1}{2}}f}_{\Hone}^{2}
\end{equation}
and
\begin{equation}\label{Definition of alpha general case reptwo}
    \alpha_{\gl}^{\reptwo} = \underset{f \in \Htwo}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \norm{(\gl(\Stwo)^{-1} - \Stwo)^{\frac{1}{2}}f}_{\Htwo}^{2}
\end{equation}

In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, let $\alpha_{\lambda}$ and $\beta_{\lambda}$ be the population kernel ridge regression estimators of the regression function $\eta$, given by
\begin{equation}\label{Definition of alpha Tikhonov repone}
    \alpha_{\lambda}^{\repone} = \underset{f \in \Hone}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \lambda \norm{f}_{\Hone}^{2}
\end{equation}
and
\begin{equation}\label{Definition of alpha Tikhonov reptwo}
\alpha_{\lambda}^{\reptwo} = \underset{f \in \Htwo}{\argmin} \hspace{2pt} \E\left[Y-f(X)\right]^{2} + \lambda \norm{f}_{\Htwo}^{2},
\end{equation}
respectively. The prediction loss being the squared error loss, $\alpha_{\gl}^{\repone}$ and $\alpha_{\gl}^{\reptwo}$ depend on the distribution of $Y$ only through the population regression function $\eta$. We suppress this dependence on $\eta$ in the notation for convenience and clarity.

We now define the kernel ridge regression-based pseudometric between the two representations of the input $\repone$ and $\reptwo$, based on the difference between predictions for $Y$ uniformly over all regression functions $\eta \in \LPtwo$ such that its $\LPtwo$ norm is bounded above by 1.
\begin{definition}\label{Definition of pseudometric operator norm general case}
    For any $\lambda>0$, choice of kernel $K(\cdot,\cdot)$ and choice of spectral regularizer $\gl$, the \metricstname (\metricfullname) distance between representations $\repone(X)$ and $\reptwo(X)$ is defined as, 
    \[
    \dopgl(\repone,\reptwo) \coloneq \underset{\norm{\eta}_{\LPtwo} \leq 1}{\sup} \left(\E\left[\alpha_{\gl}^{\repone}(X)-\alpha_{\gl}^{\reptwo}(X)\right]^{2}\right)^{\frac{1}{2}},
    \]
    where $\alpha_{\gl}^{\repone}$ and $\alpha_{\gl}^{\reptwo}$ are defined in Equations \ref{Definition of alpha general case repone} and \ref{Definition of alpha general case reptwo}, respectively.
\end{definition}

% The reasoning behind the $\mathcal{L}^{\infty}$ in the notation for the \metricstname distance $\dopgl$ will be clear going forward, as we will demonstrate that it is actually an operator norm in an appropriate formal sense.


\begin{theorem}\label{Characterization of pseudometric in terms of operator norm}
For any $\lambda>0$, consider a spectral regularizer $\gl$ that satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$}, \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} , \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$} and \hyperref[Assumption Spectral Regularizer A5]{$(\SpectralAssumptionfive)$} with $C_1 \leq 1$ and $\gl(0)>0$. Further, assume that the base kernel $K$ is defined on any Euclidean space and is positive definite, symmetric, bounded and continuous. Then, the \metricstname distance $\dopgl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as 
\[
\begin{aligned}
     \dopgl(\repone,\reptwo) =& \norm{\gl(\Tone) \Tone - \gl(\Ttwo) \Ttwo}_{\optwo} \\
     =& \norm{ \Tone \gl(\Tone) -  \Ttwo \gl(\Ttwo)}_{\optwo} \\
     =& \norm{ \Irepone \gl(\Sone) \Ireponead -  \Ireptwo \gl(\Stwo) \Ireptwoad }_{\optwo}.
\end{aligned}
\]
In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, the UKP distance $ \dop (\repone,\reptwo) \coloneqq  \dopglTik(\repone,\reptwo)$ can be expressed as
\[
\begin{aligned}
     \dop(\repone,\reptwo) =& \norm{\Tonelambdainv \Tone - \Ttwolambdainv \Ttwo}_{\optwo}\\
     =&\norm{\Tone \Tonelambdainv  - \Ttwo \Ttwolambdainv }_{\optwo}\\.
     =& \norm{ \Irepone \Sonelambdainv \Ireponead -  \Ireptwo \Stwolambdainv \Ireptwoad }_{\optwo}.
\end{aligned}
\]
% where $X$ and $X^{\prime}$ are i.i.d observations drawn from $P_{X}$.
\end{theorem}

\begin{proof}
        Under the conditions imposed on the spectral regularizer $\gl$ and the base kernel $K$,  the population kernel regression estimators $\alpha_{\gl}^{\repone}$ and $\alpha_{\gl}^{\reptwo}$ as defined in Equations \ref{Definition of alpha general case repone} and \ref{Definition of alpha general case reptwo} and used in Definition \ref{Definition of pseudometric operator norm general case} are explicitly given by $\alpha_{\gl}^{\repone} = \gl(\Sone) \Ireponead \eta$ and $\alpha_{\gl}^{\reptwo} = \gl(\Stwo) \Ireptwoad \eta$, with their corresponding $\LPtwo$ embeddings being $\Ireponead \alpha_{\gl}^{\repone}=\Tone \gl(\Tone) \eta =  \gl(\Tone) \Tone \eta$ and $\Ireptwoad \alpha_{\gl}^{\reptwo}=\Ttwo \gl(\Ttwo) \eta =  \gl(\Ttwo) \Ttwo \eta$, respectively. Consequently, for any fixed $\eta = \E(Y \mid X) \in \LPtwo$, we have that
    \[
    \begin{aligned}
    E\left[\alpha_{\gl}^{\repone}(X) -\alpha_{\gl}^{\reptwo}(X)\right]^{2}=& \int \left[\alpha_{\gl}^{\repone}(X) -\alpha_{\gl}^{\reptwo}(X)\right]^{2}dP_X(x)\\
    =&\norm{\Irepone \alpha_{\gl}^{\repone} - \Ireptwo \alpha_{\gl}^{\reptwo}}_{\LPtwo}^{2}\\
    =& \norm{\gl(\Tone)\Tone\eta - \gl(\Ttwo) \Ttwo \eta}_{\LPtwo}^{2}\\
    =& \norm{\left[\gl(\Tone)\Tone - \gl(\Ttwo) \Ttwo \right]\eta}_{\LPtwo}^{2}.
    \end{aligned}
    \]

    Finally, using the definition of the operator norm of $\left[\gl(\Tone)\Tone - \gl(\Ttwo) \Ttwo \right]$ together with Parts \ref{Elementary prop 6} and \ref{Elementary prop 9} of Lemma \ref{Elementary properties of inclusion, covariance and integral operators}, we obtain that
    \[
    \begin{aligned}
        \dopgl(\repone,\reptwo) =& \underset{\norm{\eta}_{\LPtwo} \leq 1}{\sup} \left(\E\left[\alpha_{\lambda}(X)-\beta_{\lambda}(X)\right]^{2}\right)^{\frac{1}{2}}\\
        =& \underset{\norm{\eta}_{\LPtwo} \leq 1}{\sup} \norm{\left[\gl(\Tone)\Tone - \gl(\Ttwo) \Ttwo \right]\eta}_{\LPtwo}\\
        =& \norm{\gl(\Tone) \Tone - \gl(\Ttwo) \Ttwo}_{\optwo} \\
         =& \norm{ \Tone \gl(\Tone) -  \Ttwo \gl(\Ttwo)}_{\optwo} \\
         =& \norm{ \Irepone \gl(\Sone) \Ireponead -  \Ireptwo \gl(\Stwo) \Ireptwoad }_{\optwo}.
    \end{aligned}
    \]

    In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, we obtain the required result for $\dop = \dopglTik$.
        
\end{proof}

\begin{remark}
    A very specific case of our proposed pseudometric $\dLpgl$, named the GULP distance, was analyzed in \citet{GULP}. However, the authors of \cite{GULP} inaccurately derived GULP to correspond to a Hilbert-Schmidt norm, when it should actually correspond to an operator norm. This stems from the fact that, in the proof of Lemma 1 in Section A.1 of \citet{GULP} (Page 14 of both the NeurIPS version \citet{GULP} and the ArXiv version \citet{}), the squared GULP distance is expressed as the supremum of an expectation (as we do in Definition \ref{Definition of pseudometric}) but they erroneously interchange the supremum and the expectation. By Jensen's inequality, the resulting quantity after this interchange can be shown to be greater than or equal to the definition of GULP distance. We claim that the distance actually analyzed in \citet{GULP} corresponds to making the specific choices of $p=2$, spectral regularizer $\gl = \glTik$ and the linear base kernel $K = K_{\operatorname{lin}}(x,y) = x^{T}y$ in our proposed \metricstname distance $\dLpgl$.
    % The \metricstname distance for the choice $p=2$ (i.e. Hilbert-Schmidt norm) is a generalization of the GULP distance, as proposed in \citet{GULP}, in the sense that, if we choose the kernel for the \metricstname to be the linear kernel $K_{lin}(x,y) = x^{T}y$, we exactly recover the GULP distance.
\end{remark}

% \begin{remark}
% One of the most novel aspect of our contributions is the exact identification of the mathematical relationship between representations that lead them to have the same generalization performance. To be specific, the GULP paper derives in their Theorem 1 and Theorem 2 that two representations  and  are equivalent from the lens of the GULP metric if and only if one can be expressed as an orthogonal linear transformation of the other. However, the authors of the GULP paper do not discuss why orthogonality of representations plays such a crucial role in their results. Our results clearly show that the source of the appearance of the orthogonality condition is the choice of the similarity function in the GULP paper, which is the linear kernel . We are able to delineate the exact relationship between the choice of invariance and the choice of the kernel, thus extending their results to a much broader domain. Further, the proof techniques used to prove the invariance properties of the GULP metric in Theorem 2 are specific to the case of the linear kernel and rely on complicated manipulations based on linear algebra theory (such as analyzing homogeneous Sylvester equations). In contrast, our proofs for the invariance properties of the UKP metric (Lemma 2, Corollary 1 and Corollary 2) use standard and systematic functional analysis techniques in RKHSs. Therefore, the proofs are easier to understand and generalize the results available in the GULP paper. Moreover, we emphasize that no closed-form expression of the population version of the pseudometric (in the sense that we do in our paper in Proposition 1) was given in the GULP paper. The authors of the GULP paper only provided the closed-form expression of the estimator of the pseudometric, and that too only for the linear kernel. Therefore, the preliminary KRR discussion in the GULP paper considers only a specific case of the plug-in estimator we propose in our paper in Equation 3, and express in a more computationally tractable form in terms of Gram matrices in Proposition 2.
% \end{remark}

\subsection{Properties of the \metricstname distance}\label{Properties of the UKP distance}

For any $p \geq 1$, we will use $\norm{\cdot}_{\Lp(\mathcal{S})}$ to denote the $p$-Schatten norm of any operator mapping from its domain $\mathcal{S}$ into itself. In particular, for $p=1,2$ and $\infty$, the $p$-Schatten norm corresponds to the trace norm, Hilbert-Schmidt norm and the operator norm, respectively.

Using the monotonicity properties of $p$-Schatten norms (See Proposition 2.1 of \citet{pfeiffer2021stability}) we can develop a hierarchy of distances (pseudometrics), which we call generalized \metricstname distances, corresponding to the choice of the Schatten norm $\norm{\cdot}_{\Lptwo}$ for any $ p \geq 1$, defined as follows:
\begin{definition}\label{Definition of pseudometric Lp norm general case}
    For any $\lambda>0$, choice of kernel $K(\cdot,\cdot)$,choice of spectral regularizer $\gl$ and $p \geq 1$, the $(\gl,K,p)$-\metricstname (\metricfullname) distance between representations $\repone(X)$ and $\reptwo(X)$ is defined as, 
    \[
    \dLpgl(\repone,\reptwo) \coloneq \norm{\gl(\Tone) \Tone - \gl(\Ttwo) \Ttwo}_{\Lptwo},
    \]
    where $\Tone$ and $\Ttwo$ are the integral operators corresponding to the pullback RKHS's $\Hone$ and $\Htwo$, respectively.
\end{definition}

Further, the following theorem also serves to show that the $(\gl,K,p)$-\metricstname distance satisfies the axioms of a pseudometric for any valid choice of $\norm{\cdot}_{\Lptwo}$ and spectral regularizer $\gl$.


Of particular importance is the choice $p=2$, which corresponds to the Hilbert-Schmidt norm, since it leads to a pseudometric which can be efficiently estimated using i.i.d samples from $P_X$.

\begin{theorem}\label{Theorem: Monotonicity and Pseudometricity}
    Assume that the setting of Theorem \ref{Characterization of pseudometric in terms of operator norm} holds true. Then, for any $1 \leq p \leq \infty$, we have that
    \begin{equation}
    \begin{aligned}\label{monotonicity of family of UKP distances for p choices}
        \dopgl(\repone,\reptwo) \leq \dLpgl(\repone,\reptwo) \leq \donegl(\repone, \reptwo)
    \end{aligned}
    \end{equation}
    
    Further, the $\dLpgl(\repone,\reptwo)$ distance satisfies the following properties:
    \begin{enumerate}
        \item For any function $\repone: \R^d \to \R^k$ for some $k \in \N$, $\dLpgl(\repone,\repone) = 0$,
        \item (Non-negativity) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $\dLpgl(\repone,\reptwo) \geq 0$,
        \item (Symmetric) For any two functions $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ for some $k,l \in \N$, $\dLpgl(\repone,\reptwo) = \dLpgl(\reptwo,\repone)$,
        \item (Triangle inequality) For any three functions $\repone: \R^d \to \R^k$, $\reptwo: \R^d \to \R^l$ and $\repthree: \R^d \to \R^m$ for some $k,l,m \in \N$, $\dLpgl(\repone,\reptwo) \leq \dLpgl(\repone,\repthree) + \dLpgl(\repthree,\reptwo)$.
    \end{enumerate}
    Hence, $\dLpgl(\repone,\reptwo)$ is a pseudometric over the space of all functions that maps $\R^d$ to some Euclidean space $\R^t$ for any $t \in \N$.
\end{theorem}

\begin{proof}
    Equation \ref{monotonicity of family of UKP distances for p choices} and the 4 pseudometric properties readily
    follow from the expression of $\dLpgl(\repone,\reptwo)$ in Definition \ref{Definition of pseudometric Lp norm general case} in terms of the $p$-Schatten norm of $\left[\gl(\Tone) \Tone - \gl(\Ttwo) \Ttwo\right]$ and using the properties of $p$-Schatten norms as given in Proposition 2.1 of \citet{pfeiffer2021stability}.
\end{proof}

We now analyze the invariance properties of the pseudometric $\dLpgl$ and identify the transformations of the representations $\repone$ and $\reptwo$ that leave its value unchanged. Based on the following theorem, we can identify representations that \metricstname treats as equivalent in terms of prediction-based performance for a general collection of kernel ridge regression tasks corresponding to a particular kernel $K$.
\begin{remark}
One of the most novel aspect of our contributions is the exact identification of the mathematical relationship between representations that lead them to have the same generalization performance. To be specific, the GULP paper derives in their Theorem 1 and Theorem 2 that two representations  and  are equivalent from the lens of the GULP metric if and only if one can be expressed as an orthogonal linear transformation of the other. However, the authors of the GULP paper do not discuss why orthogonality of representations plays such a crucial role in their results. Our results clearly show that the source of the appearance of the orthogonality condition is the choice of the similarity function in the GULP paper, which is the linear kernel . We are able to delineate the exact relationship between the choice of invariance and the choice of the kernel, thus extending their results to a much broader domain. Further, the proof techniques used to prove the invariance properties of the GULP metric in Theorem 2 are specific to the case of the linear kernel and rely on complicated manipulations based on linear algebra theory (such as analyzing homogeneous Sylvester equations). In contrast, our proofs for the invariance properties of the UKP metric use standard and systematic functional analysis techniques in RKHSs. Therefore, the proofs are easier to understand and generalize the results available in the GULP paper. Moreover, we emphasize that no closed-form expression of the population version of the pseudometric was given in the GULP paper. The authors of the GULP paper only provided the closed-form expression of the estimator of the pseudometric, and that too only for the linear kernel. Therefore, the preliminary KRR discussion in the GULP paper considers only a specific case of the plug-in estimator we propose in our paper, and express in a more computationally tractable form in terms of Gram matrices.
\end{remark}
\begin{theorem}\label{Invariance of UKP distance general case}
     Assume that the setting of Theorem \ref{Characterization of pseudometric in terms of operator norm} holds true. Then, for any $p \geq 1$ and $\lambda>0$, given any two representations $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ we have that 
     \begin{equation}\label{Necessity and sufficiency of UKP distance being zero if and only if the integral operators coincide}
         \dLpgl(\repone,\reptwo) = 0 \,\ \textrm{ if and only if }\,\ \Tone = \Ttwo .
     \end{equation}

     Further, let $\mathcal{H}$ be the class of transformations under which the kernel $K$ is invariant, i.e., $\mathcal{H} = \left\{h : K(\cdot,\cdot) = K(h(\cdot),h(\cdot))\textrm{ a.e. } P_{X}\right\}$. Then, the \emph{\metricstname} distance $\dLpgl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ is invariant under the same class of transformations that the kernel $K$ is invariant for, i.e., for any $h_{1},h_{2} \in \mathcal{H}$, 
\[
\dLpgl(h_{1} \circ \repone,h_{2} \circ \reptwo) = \dLpgl(\repone,\reptwo)
\]
and if either $h_{1}$ or $h_{2}$ does not belong to $\mathcal{H}$, \[
\dLpgl(h_{1} \circ \repone,h_{2} \circ \reptwo) \neq \dLpgl(\repone,\reptwo).
\]

Consequently, a necessary and sufficient condition for the \emph{\metricstname} distance $\dLpgl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ to be zero is that $K_{\repone}(\cdot,\cdot) = K_{\reptwo}(\cdot,\cdot)$ a.e. $P_{X}$.
\end{theorem}

\begin{proof}
    The sufficiency part of the claim in Equation \ref{Necessity and sufficiency of UKP distance being zero if and only if the integral operators coincide} is trivial. For the necessity part, we observe that, by the definiteness of $p$-Schatten norms (follows readily from the definiteness of $1$-Schatten norm i.e trace norm, see Lemma 8 of Chapter 3 of \citet{Schattennormbook}), we must have that 
    \begin{equation}\label{regularized integral operators coincide}
    \gl(\Tone) \Tone = \gl(\Ttwo) \Ttwo.
    \end{equation}

    Under the given conditions on the kernel $K$, the integral operators $\Tone$ and $\Ttwo$ corresponding to the kernels $K_{\repone}$ and $K_{\reptwo}$ both admit spectral decompositions. Let $\left(\mu_{i}^{\repone},e_{i}^{\repone}\right)_{i=1}^{\infty}$ and $\left(\mu_{j}^{\reptwo},e_{j}^{\reptwo}\right)_{j=1}^{\infty}$ be the eigenvalue-eigenfunction pairs corresponding to the spectral decomposition of $\Tone$ and $\Ttwo$, respectively. Then, we have that
    \begin{equation*}\label{Spectral decomposition of Tf}
        \Tone = \sum_{i=1}^{\infty} \mu_{i}^{\repone} \left(e_{i}^{\repone} \otimes_{\LPtwo} e_{i}^{\repone}\right)
    \end{equation*}
    and
    \begin{equation*}\label{Spectral decomposition of Tg}
        \Ttwo = \sum_{j=1}^{\infty} \mu_{j}^{\reptwo} \left(e_{j}^{\reptwo} \otimes_{\LPtwo} e_{j}^{\reptwo}\right).
    \end{equation*}

    Since $K$ is a positive definite, symmetric, continuous and bounded kernel defined on a separable domain, $\Tone$ and $\Ttwo$ are compact, self-adjoint, trace-class operators. Therefore, we must have that $\mu_{i}^{\repone},\mu_{j}^{\reptwo} > 0$ and $\lim_{i \to \infty} \mu_{i}^{\repone} = \lim_{j \to \infty} \mu_{j}^{\reptwo} = 0$. Further, $(e_{i}^{\repone})_{i=1}^{\infty}$ and $(e_{j}^{\reptwo})_{j=1}^{\infty}$ constitute a pair of orthonormal bases of $\range(\Tone)$ and $\range(\Ttwo)$, respectively.

    Consequently, it can be readily verified that
    \begin{equation*}\label{Spectral decomposition of regualrized Tf}
        \gl(\Tone)\Tone = \sum_{i=1}^{\infty} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone}) \left(e_{i}^{\repone} \otimes_{\LPtwo} e_{i}^{\repone}\right)
    \end{equation*}
    and
    \begin{equation*}\label{Spectral decomposition of regularized Tg}
        \gl(\Ttwo)\Ttwo = \sum_{j=1}^{\infty} \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo}) \left(e_{j}^{\reptwo} \otimes_{\LPtwo} e_{j}^{\reptwo}\right).
    \end{equation*}

    Therefore, we must have that
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 1}
        \begin{aligned}[b]
            &\gl(\Tone)\Tone = \gl(\Ttwo)\Ttwo\\
            \iff & \sum_{i=1}^{\infty} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone}) \left(e_{i}^{\repone} \otimes_{\LPtwo} e_{i}^{\repone}\right) = \sum_{j=1}^{\infty} \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo}) \left(e_{j}^{\reptwo} \otimes_{\LPtwo} e_{j}^{\reptwo}\right).
        \end{aligned}
    \end{equation}

    Define $t_{ij} \coloneq \inprod{e_{i}^{\repone},e_{j}^{\reptwo}}_{\LPtwo}$ for all $i,j$. Further, define $V_{i} = \left\{j \in \mathbb{N} : t_{ij} \neq 0\right\}$ for all $i$ and $W_{j} = \left\{i \in \mathbb{N} : t_{ij} \neq 0\right\}$ for all $j$. 
    
    Now, taking the $\LPtwo$ inner product of both the RHS and LHS of \eqref{Equality of regularized integral operators If and Ig part 1} with $e_{j}^{\reptwo}$, we have that
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 2}
        \sum_{i=1}^{\infty} \mu_{i}^{\repone} \gl(\mu_{i}^{\repone})t_{ij} e_{i}^{\repone}(\cdot) = \mu_{j}^{\reptwo}\gl(\mu_{j}^{\reptwo}) e_{j}^{\reptwo}(\cdot).
    \end{equation}

    Taking the $\LPtwo$ inner product of both the RHS and LHS of \eqref{Equality of regularized integral operators If and Ig part 2} with $e_{k}^{\repone}$, we have that
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 3}
        \begin{aligned}[b]
            &\mu_{k}^{\repone} \gl(\mu_{k}^{\repone})t_{kj} = \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo})t_{kj}\\
            \iff & t_{kj} \left(\mu_{k}^{\repone} \gl(\mu_{k}^{\repone})- \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo})  \right) = 0.
        \end{aligned}
    \end{equation}


    Taking the $\LPtwo$ inner product of both the RHS and LHS of \eqref{Equality of regularized integral operators If and Ig part 2} with $e_{k}^{\reptwo}$, we have that
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 4}
        \begin{aligned}[b]
        & \sum_{i=1}^{\infty} \mu_{i}^{\repone} \gl(\mu_{i}^{\repone})t_{ij}t_{ik} = \begin{cases}
            \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo}) \textrm{ if } j = k \\
            0, \textrm{ if } j \neq k
        \end{cases} \\
        \iff & \sum_{i \in W_{j} \cap W_{k}}\mu_{i}^{\repone} \gl(\mu_{i}^{\repone})t_{ij}t_{ik} = \begin{cases}
            \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo}) \textrm{ if } j = k \\
            0, \textrm{ if } j \neq k
        \end{cases}.
        \end{aligned}
    \end{equation}

    Using \eqref{Equality of regularized integral operators If and Ig part 3} and \eqref{Equality of regularized integral operators If and Ig part 4}, we have that

    \begin{equation}\label{Equality of regularized integral operators If and Ig part 5}
        \begin{aligned}[b]
        \mu_{j}^{\reptwo}\gl(\mu_{j}^{\reptwo}) \left[ \sum_{i \in W_{j}}t_{ij}^{2} - 1\right] = 0
        \end{aligned}
    \end{equation}
    and, if $j \neq k$,
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 6}
        \begin{aligned}[b]
        \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo})\left( \sum_{i \in W_{j}\cap W_{k}}t_{ij}t_{ik}\right) = 0.
        \end{aligned}
    \end{equation}

    Therefore, from \eqref{Equality of regularized integral operators If and Ig part 5} and \eqref{Equality of regularized integral operators If and Ig part 6}, we obtain that
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 7}
        \sum_{i \in W_{j}}t_{ij}^{2} = 1
    \end{equation}
    and, if $j \neq k$,
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 8}
        \sum_{i \in W_{j}\cap W_{k}}t_{ij}t_{ik}=0.
    \end{equation}

    In exactly analogous manner, we can also obtain 
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 9}
        \sum_{j \in V_{i}}t_{ij}^{2} = 1
    \end{equation}
    and, if $i \neq k$,
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 10}
        \sum_{j \in V_{i}\cap V_{k}}t_{ij}t_{kj}=0.
    \end{equation}

    Note that $(e_{j}^{\reptwo})_{j=1}^{\infty}$ can be extended to obtain an orthonormal basis for $\LPtwo$. Let $B=\left\{\cup_{j=1}^{\infty}e_{j}^{\reptwo}\right\} \cup \left\{\cup_{l=1}^{\infty}z_{l}^{\reptwo}\right\}$ be the resulting orthonormal basis of $\LPtwo$ obtained by said extension.

    Now,
    \begin{equation}\label{Expressing one set of eigenfunctions in terms of orthonormal basis}
        \begin{aligned}
            e_{i}^{\repone} =& \sum_{j=1}^{\infty} \inprod{e_{i}^{\repone},e_{j}^{\reptwo}}e_{j}^{\reptwo} + \sum_{l=1}^{\infty} \inprod{e_{i}^{\repone},z_{l}^{\reptwo}}z_{l}^{\reptwo}.
        \end{aligned}
    \end{equation}

    Therefore, using \eqref{Expressing one set of eigenfunctions in terms of orthonormal basis} and \eqref{Equality of regularized integral operators If and Ig part 9} along with the orthonormality of $(e_{i}^{\repone})_{i=1}^{\infty}$, we have,
     \begin{equation*}\label{Expressing one set of eigenfunctions in terms of other set of eigenfunctions}
        \begin{aligned}[b]
            &\norm{e_{i}^{\repone}}_{\LPtwo} = 1\\
            \iff &\sum_{j=1}^{\infty} \inprod{e_{i}^{\repone},e_{j}^{\reptwo}}^{2} + \sum_{l=1}^{\infty} \inprod{e_{i}^{\repone},z_{l}^{\reptwo}}^{2} = 1\\
            \iff & \sum_{j \in V_{i}} t_{ij}^{2} + \sum_{l=1}^{\infty} \inprod{e_{i}^{\repone},z_{l}^{\reptwo}}^{2} = 1\\
            \iff &  \sum_{l=1}^{\infty} \inprod{e_{i}^{\repone},z_{l}^{\reptwo}}^{2} = 0\\
            \iff & \inprod{e_{i}^{\repone},z_{l}^{\reptwo}} = 0 \textrm{ for all l and i}.
        \end{aligned}
    \end{equation*}

    Hence, for all $i$, $e_{i}^{\repone} \in \operatorname{Span}\left\{e_{j}^{\reptwo}, j \in \mathbb{N}\right\}$. Consequently, $\Tone \gl(\Tone) e_{j}^{\reptwo} = \sum_{i=1}^{\infty} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone})t_{ij}e_{i}^{\repone} \in \operatorname{Span}\left\{e_{i}^{\repone}, i \in \mathbb{N}\right\} \subset \operatorname{Span}\left\{e_{j}^{\reptwo}, j \in \mathbb{N}\right\}$. 

    Now, using \eqref{Equality of regularized integral operators If and Ig part 8} and \eqref{Equality of regularized integral operators If and Ig part 3}, for any $j \neq k$, we have
    \begin{equation*}
        \begin{aligned}[b]
            \inprod{\Tone \gl(\Tone)e_{j}^{\reptwo},e_{k}^{\reptwo}}_{\LPtwo} =& \sum_{i=1}^{\infty} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone})t_{ij}t_{ik}\\
            =& \sum_{i \in W_{j}\cap W_{k}} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone})t_{ij}t_{ik}\\
            =& \mu_{j}^{\reptwo} \gl(\mu_{j}^{\reptwo}) \sum_{i \in W_{j}\cap W_{k}} t_{ij}t_{ik}\\
            =&0.
        \end{aligned}
    \end{equation*}

    Finally, using \eqref{Equality of regularized integral operators If and Ig part 5} and \eqref{Equality of regularized integral operators If and Ig part 3}, we have that
    \begin{equation*}
        \begin{aligned}[b]
            \inprod{\Tone \gl(\Tone)e_{j}^{\reptwo},e_{j}^{\reptwo}}_{\LPtwo} =& \sum_{i=1}^{\infty} \mu_{i}^{\reptwo}\gl(\mu_{i}^{\reptwo})t_{ij}^{2}\\
            =& \sum_{i \in W_{j}}\mu_{i}^{\reptwo}\gl(\mu_{i}^{\reptwo})t_{ij}^{2}\\
            = &\mu_{j}^{\reptwo}\gl(\mu_{j}^{\reptwo})\sum_{i \in W_{j}}t_{ij}^{2}\\
            =&\mu_{j}^{\reptwo}\gl(\mu_{j}^{\reptwo}) > 0.
        \end{aligned}
    \end{equation*}

    Therefore, $\Tone\gl(\Tone) e_{j}^{\repone}=\mu_{j}^{\reptwo}\gl(\mu_{j}^{\reptwo})  e_{j}^{\reptwo}$ for all $j$. Therefore, all the eigenfunctions of $\Ttwo \gl(\Ttwo)$ (and hence those of $\Ttwo$) are also eigenfunctions of $\Tone \gl(\Tone)$ (and hence those of $\Tone$). By symmetry, all the eigenfunctions of $\Tone$ are also eigenfunctions of $\Ttwo$. Therefore, $\Tone$ and $\Ttwo$ (equivalently, $\Tone \gl(\Tone)$ and $\Ttwo \gl(\Ttwo)$) have exactly the same eigenfunctions.

    Consequently, \eqref{Equality of regularized integral operators If and Ig part 1} can be now written as
    \begin{equation}\label{Equality of regularized integral operators If and Ig part 11}
        \begin{aligned}[b]
            &\gl(\Tone)\Tone = \gl(\Ttwo)\Ttwo\\
            \iff & \sum_{i=1}^{\infty} \mu_{i}^{\repone}\gl(\mu_{i}^{\repone}) \left(e_{i}^{\repone} \otimes_{\LPtwo} e_{i}^{\repone}\right) = \sum_{i=1}^{\infty} \mu_{i}^{\reptwo} \gl(\mu_{i}^{\reptwo}) \left(e_{i}^{\repone} \otimes_{\LPtwo} e_{i}^{\repone}\right).
        \end{aligned}
    \end{equation}

    Taking the $\LPtwo$ inner product of both the RHS and LHS of \eqref{Equality of regularized integral operators If and Ig part 11} with $e_{i}^{\repone}$ twice and using the injectivity of $x \mapsto x\gl(x)$ over $x \in [0,\kappa]$, we have that, for any $i$,
    \begin{equation*}
        \begin{aligned}[b]
            &\mu_{i}^{\repone} \gl(\mu_{i}^{\repone}) = \mu_{i}^{\reptwo} \gl(\mu_{i}^{\reptwo})\\
            \iff & \mu_{i}^{\repone} = \mu_{i}^{\reptwo}.
        \end{aligned}
    \end{equation*}

    Therefore, we must have that the integral operators $\Tone$ and $\Ttwo$ have the same spectral decomposition. Consequently, we must have that \begin{equation}\label{Spectral decomposition matches so equality of integral operators}
        \Tone = \Ttwo.
    \end{equation}

    This concludes the proof of the necessity part.   

    Equation \ref{Spectral decomposition matches so equality of integral operators} is equivalent to the following condition : For any $f \in \LPtwo$, 
    \begin{equation}\label{Spectral decomposition matches so equality of inclusion adjoint operators}
        \int \left[K_{\repone}(\cdot,x) f(x) dP_{X}(x) - K_{\reptwo}(\cdot,x)\right] f(x) dP_{X}(x) = 0.
    \end{equation}
    Consequently, Equation \ref{Spectral decomposition matches so equality of integral operators} is equivalent to the condition $K_{\repone}(\cdot,\cdot) = K_{\reptwo}(\cdot,\cdot)$ a.e. $P_{X}$. 

    On the other hand, if $K_{\repone}$ and $K_{\reptwo}$ differ on a set of positive measure under $P_X$, then clearly $\Tone$ and $\Ttwo$ define two distinct operators. Using the injectivity of $x \mapsto x\gl(x)$, this ensures that $\Tone \gl(\Tone) \neq \Ttwo \gl(\Ttwo)$, leading to $\dLpgl(\repone,\reptwo)>0$. This completes the proof of the required result.
\end{proof}

\subsection{Properties of $\dtwogl$}\label{Properties of dtwo}

\begin{theorem}\label{Squared dtwo pseudometric using trace operators general case}
    Assume that the setting of Theorem \ref{Characterization of pseudometric in terms of operator norm} holds true. Then, 
    for any $\lambda>0$, the squared \emph{\metricstname} distance $\dtwogl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as
    \[
\begin{aligned}
        \left[\dtwogl(\repone,\reptwo)\right]^{2}
        = \Tr\left(\gl(\Sone)\Sone\gl(\Sone)\Sone\right) + \Tr\left(\gl(\Stwo)\Stwo\gl(\Stwo)\Stwo\right)-2\Tr\left(\gl(\Sone)\Sonetwo\gl(\Stwo)\Stwoone\right).
    \end{aligned}
    \]

    In particular, when the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, the squared UKP distance $ \dtwo(\repone,\reptwo)$ can be expressed as 
    \[
\begin{aligned}
        \left[\dtwo(\repone,\reptwo)\right]^{2}
        = \emph{Tr}\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\right) + \emph{Tr}\left(\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\right)-2\emph{Tr}\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo\repone}\right).
    \end{aligned}
    \]
    
\end{theorem}

\begin{proof} Using Definition \ref{Definition of pseudometric Lp norm general case} and the results of Lemma \ref{Elementary properties of inclusion, covariance and integral operators}, we have that the squared \metricstname distance $\dtwogl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as 
\[
\begin{aligned}
     \left[\dtwogl(\repone,\reptwo)\right]^{2} =& \norm{\gl(\Tone) \Tone - \gl(\Ttwo) \Ttwo}_{\HStwo}^{2} \\
     =& \norm{ \Tone \gl(\Tone) -  \Ttwo \gl(\Ttwo)}_{\HStwo}^{2} \\
     =& \norm{ \Irepone \gl(\Sone) \Ireponead -  \Ireptwo \gl(\Stwo) \Ireptwoad }_{\HStwo}^{2}\\
     =& \inprod{\Irepone \gl(\Sone) \Ireponead  , \Irepone \gl(\Sone) \Ireponead }_{\HStwo} + \inprod{\Ireptwo \gl(\Stwo) \Ireptwoad ,   \Ireptwo \gl(\Stwo) \Ireptwoad }_{\HStwo} \\
     &- 2 \inprod{\Irepone \gl(\Sone) \Ireponead , \Ireptwo \gl(\Stwo) \Ireptwoad }_{\HStwo}\\
     =& \Tr\left(\gl(\Sone)\Sone\gl(\Sone)\Sone\right) + \Tr\left(\gl(\Stwo)\Stwo\gl(\Stwo)\Stwo\right)-2\Tr\left(\gl(\Sone)\Sonetwo\gl(\Stwo)\Stwoone\right).
\end{aligned}
\]
When the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, the UKP distance $ \dtwo (\repone,\reptwo) \coloneqq  \dtwoglTik(\repone,\reptwo)$ can be derived from the above computation.
\end{proof}

\subsection{Estimation of $\dtwogl$ and finite-sample concentration of the estimator}

Using the plugin-estimators $\Sonehat$, $\Stwohat$, $\Sonetwohat$ and $\Stwoonehat$ for estimating their corresponding population counterparts, we can obtain a V-statistic type estimator for $\dtwogl$  based on i.i.d samples $X_1,\dots,X_n$ drawn from $P_X$, and is given by
\begin{equation}\label{Estimator of dtwo general case in terms of covariance operators}
\begin{aligned}
        &\dtwoglhat(\repone,\reptwo)\\
        =& \left[\Tr\left(\gl(\Sone)\Sone\gl(\Sone)\Sone\right) + \Tr\left(\gl(\Stwo)\Stwo\gl(\Stwo)\Stwo\right)-2\Tr\left(\gl(\Sone)\Sonetwo\gl(\Stwo)\Stwoone\right)\right]^{\frac{1}{2}}.
    \end{aligned}
\end{equation}

Using the results in Lemma \ref{Elementary properties of sampling, empirical covariance operator and kernel Gram matrix}, one can prove an equivalent expression of the estimator $\dtwoglhat(\repone,\reptwo)$ in terms of the kernel Gram matrices $\Knone$ and $\Kntwo$, which is more useful in practice and implementation.

\begin{theorem}\label{Estimator in terms of Gram matrices}
    For any $\lambda>0$, the V-statistic type estimator $\dtwoglhat(\repone,\reptwo)$ of  $\dtwogl(\repone,\reptwo)$ between representations $\repone(X)$ and $\reptwo(X)$ can be expressed as
    \[
    \begin{aligned}
        &\dtwoglhat(\repone,\reptwo)\\
        =& \frac{1}{n}\left[\Tr\left(\Knone\gl(\Knonelambda)\Knone\gl(\Knonelambda)\right) + \Tr\left(\Kntwo\gl(\Kntwolambda)\Kntwo\gl(\Kntwolambda)\right)\right.\\
        &-2\left.\Tr\left(\Knone\gl(\frac{1}{n}\Knone\Kntwo\gl(\frac{1}{n}\Kntwo)\right)\right]^{\frac{1}{2}}.
    \end{aligned}
    \]
    When the chosen spectral regularizer is the Tikhonov regularizer $\glTik(x) = \frac{1}{x+\lambda}$, the estimator $\dtwohat(\repone,\reptwo)$ of $\dtwo(\repone,\reptwo)$ is given by
    \[
    \begin{aligned}
        &\dtwohat(\repone,\reptwo)\\
        =& \left[\Tr\left(K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}\right) + \Tr\left(K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}\right)\right.\\
        &-2\left.\Tr\left(K_{n,\repone}(K_{n,\repone}+n\lambda I)^{-1}K_{n,\reptwo}(K_{n,\reptwo}+n\lambda I)^{-1}\right)\right]^{\frac{1}{2}}.
    \end{aligned}
    \]
\end{theorem}

We are also able to prove a finite-sample concentration inequality for the estimator $\dtwohat(\repone,\reptwo)$ for $\lambda>0$ that demonstrates that the statistical estimation error of $\dtwo(\repone,\reptwo)$ converges to 0 at the parametric rate $n^{-\frac{1}{2}}$, where $n$ is the sample size. For simplicity, the concentration inequality is derived for the specific regularizer that corresponds to kernel ridge regression (Tikhonov regularization), but similar rates of convergence (in terms of sample size $n$) will hold for other choices of the regularizer $\gl$ with possible different dependence on $\lambda$.

\paragraph{Proof of Theorem \ref{Theorem: Finite sample convergence}}\label{Proof of Theorem 2}

\begin{proof}
    Note that for any $x,y \in \R^{d}$, $\hat{\Sigma}_{\repone}^{-\lambda}\left[K_{\repone}(\cdot,x)\otimes_{\Hone}K_{\repone}(\cdot,x)\right]\hat{\Sigma}_{\repone}^{-\lambda}\left[K_{\repone}(\cdot,y)\otimes_{\Hone}K_{\repone}(\cdot,y)\right]$ is a rank-one operator with eigenvalue $\inprod{\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,x),\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,y)}_{\Hone}^{2}$ and eigenfunction $\frac{\hat{\Sigma}_{\repone}^{-\lambda}K_{\repone}(\cdot,x)}{\norm{\hat{\Sigma}_{\repone}^{-\lambda}K_{\repone}(\cdot,x)}_{\Hone}}$. Similarly, $\hat{\Sigma}_{\reptwo}^{-\lambda}\left[K_{\reptwo}(\cdot,x)\otimes_{\Htwo}K_{\reptwo}(\cdot,x)\right]\hat{\Sigma}_{\reptwo}^{-\lambda}\left[K_{\reptwo}(\cdot,y)\otimes_{\Htwo}K_{\reptwo}(\cdot,y)\right]$ is a rank-one operator with eigenvalue $\inprod{\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,x),\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,y)}_{\Htwo}^{2}$ and eigenfunction $\frac{\hat{\Sigma}_{\reptwo}^{-\lambda}K_{\reptwo}(\cdot,x)}{\norm{\hat{\Sigma}_{\reptwo}^{-\lambda}K_{\reptwo}(\cdot,x)}_{\Htwo}}$. Further, \\$\hat{\Sigma}_{\repone}^{-\lambda}\left[K_{\repone}(\cdot,x)\otimes_{\HS(\Hone,\Htwo)}K_{\reptwo}(\cdot,x)\right] \times\hat{\Sigma}_{\reptwo}^{-\lambda}\left[K_{\reptwo}(\cdot,y)\otimes_{\HS(\Hone,\Htwo)}K_{\repone}(\cdot,y)\right]$ is a rank-one operator with eigenvalue $\inprod{\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,x),\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,y)}_{\Hone}\times\inprod{\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,x),\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,y)}_{\Htwo}$ and eigenfunction $\frac{\hat{\Sigma}_{\repone}^{-\lambda}K_{\repone}(\cdot,x)}{\norm{\hat{\Sigma}_{\repone}^{-\lambda}K_{\repone}(\cdot,x)}_{\Hone}}$.

    Using these facts, we have that the squared V-statistic type estimator of $d_{\lambda,K}^{\text{\metricstname}}$ can be expressed as
    \[
    \begin{aligned}
&\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\\
        =&\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left[\inprod{\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{i}),\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{j})}_{\Hone}\right.\left.-\inprod{\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{i}),\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{j})}_{\Htwo}\right]^{2}.
    \end{aligned}
    \]
Let us define the following quantity
\[
\begin{aligned}
    &\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\\
    \coloneq&\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left[\inprod{\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{i}),\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{j})}_{\Hone}\right.\left.-\inprod{\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{i}),\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{j})}_{\Htwo}\right]^{2} 
\end{aligned}
\]
which is $\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}$ with $\hat{\Sigma}_{\repone}$ and $\hat{\Sigma}_{\repone}$ replaced by $\Sigma_{\repone}$ and $\Sigma_{\repone}$, respectively. We utilize the triangle inequality to bound the difference between the squared V-statistic type estimator $\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}$ and the squared population distance $\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}$ as follows:
\begin{equation}\label{Triangle inequality for concentration result}
    \begin{aligned}
        &\left|\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|\\
        &\leq \underbrace{\left|\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|}_{\mathbf{A}} +\underbrace{\left|\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|}_{\mathbf{B}}.
    \end{aligned}
\end{equation}

We now proceed to bound $\mathbf{A}$. Let us define
\[
\hat{A}_{ij,\repone} = \inprod{\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{i}),\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{j})}_{\Hone},
\]
\[
A_{ij,\repone} = \inprod{\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{i}),\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{j})}_{\Hone},
\]
\[
\hat{A}_{ij,\reptwo} = \inprod{\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{i}),\hat{\Sigma}_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{j})}_{\Htwo},
\]
\[
A_{ij,\reptwo} = \inprod{\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{i}),\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{j})}_{\Htwo}.
\]
Then, we have that 
\[
\left|\hat{A}_{ij,\repone}\right|\leq \norm{K_{\repone}(\cdot,X_{i})}_{\Hone}^{2} \times \norm{\hat{\Sigma}_{\repone}^{-\frac{\lambda}{2}}}_{\Op(\Hone)}^{2}\leq \frac{\kappa}{\lambda}.
\]
Similarly, we can show that $\left|\hat{A}_{ij,\reptwo}\right|\leq \frac{\kappa}{\lambda}$,$\left|A_{ij,\repone}\right|\leq \frac{\kappa}{\lambda}$ and $\left|A_{ij,\reptwo}\right|\leq \frac{\kappa}{\lambda}$. Now, we have that
\[
\begin{aligned}
    \left|\hat{A}_{ij,\repone} - A_{ij,\repone}\right|
    =&\left|\inprod{K_{\repone}(\cdot,X_{i}),\left(\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}\right)K_{\repone}(\cdot,X_{j})}_{\Hone}\right|\\
    &\leq \kappa \norm{\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)}
    \leq \kappa \norm{\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}}_{\HS(\Hone)}.
\end{aligned}
\]
Similarly, we have that
\[
\begin{aligned}
    \left|\hat{A}_{ij,\reptwo} - A_{ij,\reptwo}\right|=&\left|\inprod{K_{\reptwo}(\cdot,X_{i}),\left(\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}\right)K_{\reptwo}(\cdot,X_{j})}_{\Htwo}\right|\\
    &\leq \kappa \norm{\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}}_{\Op(\Htwo)}
    \leq \kappa \norm{\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}}_{\HS(\Htwo)}.
\end{aligned}
\]

Note that,
\[
\begin{aligned}
    &\norm{\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)}\\
    =&\left\|\left(\hat{\Sigma}_{\repone}+\lambda I\right)^{-1}\left(\Sigma_{\repone}+\lambda I\right)\left(\Sigma_{\repone}+\lambda I\right)^{-1} \right.\left.- \left(\hat{\Sigma}_{\repone}+\lambda I\right)^{-1}\left(\hat{\Sigma}_{\repone}+\lambda I\right)\left(\Sigma_{\repone}+\lambda I\right)^{-1}\right\|_{\Op(\Hone)}\\
    =&\norm{\hat{\Sigma}_{\repone}^{-\lambda}\left[\left(\Sigma_{\repone}+\lambda I\right)-\left(\hat{\Sigma}_{\repone}+\lambda I\right)\right]\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)}\\
    \leq& \norm{\hat{\Sigma}_{\repone}^{-\lambda}}_{\Op(\Hone)} \norm{\Sigma_{\repone} - \hat{\Sigma}_{\repone}}_{\Op(\Hone)} \norm{\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)}\\
    \leq& \frac{1}{\lambda^{2}}\norm{\Sigma_{\repone} - \hat{\Sigma}_{\repone}}_{\Op(\Hone)}
    \leq \frac{1}{\lambda^{2}}\norm{\Sigma_{\repone} - \hat{\Sigma}_{\repone}}_{\HS(\Hone)}.
\end{aligned}
\]
Similarly, $\norm{\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}}_{\Op(\Htwo)}\leq \frac{1}{\lambda^{2}}\norm{\Sigma_{\reptwo} - \hat{\Sigma}_{\reptwo}}_{\Op(\Htwo)}\leq \frac{1}{\lambda^{2}}\norm{\Sigma_{\reptwo} - \hat{\Sigma}_{\reptwo}}_{\HS(\Htwo)}$.

Therefore, we have that 
\begin{equation}\label{Bound on A}
\begin{aligned}[b]
    \mathbf{A}
    =&\left|\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left[\left(\hat{A}_{ij,\repone}-\hat{A}_{ij,\reptwo}\right)^{2} - \left(A_{ij,\repone}-A_{ij,\reptwo}\right)^{2}\right]\right|\\
    =& \left|\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left[\left(\hat{A}_{ij,\repone}-\hat{A}_{ij,\reptwo}\right) - \left(A_{ij,\repone}-A_{ij,\reptwo}\right)\right]\right.\left.\left[\left(\hat{A}_{ij,\repone}-\hat{A}_{ij,\reptwo}\right) + \left(A_{ij,\repone}-A_{ij,\reptwo}\right)\right]\right|\\
    \leq& \kappa \left(\norm{\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)} + \norm{\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}}_{\Op(\Htwo)}\right)\times \left(\frac{2\kappa}{\lambda}+\frac{2\kappa}{\lambda}\right)\\
    =&\frac{4\kappa^{2}}{\lambda}\left[\norm{\hat{\Sigma}_{\repone}^{-\lambda}-\Sigma_{\repone}^{-\lambda}}_{\Op(\Hone)} + \norm{\hat{\Sigma}_{\reptwo}^{-\lambda}-\Sigma_{\reptwo}^{-\lambda}}_{\Op(\Htwo)}\right]\\
    \leq&\frac{4\kappa^{2}}{\lambda^{3}}\left[\norm{\hat{\Sigma}_{\repone}-\Sigma_{\repone}}_{\HS(\Hone)} + \norm{\hat{\Sigma}_{\reptwo}-\Sigma_{\reptwo}}_{\HS(\Htwo)}\right].
\end{aligned}
\end{equation}

Let us define $Z_{i}^{\repone} = K_{\repone}(\cdot,X_{i})\otimes_{\Hone}K_{\repone}(\cdot,X_{i})$. Then, $Z_{i}^{\repone}$'s are i.i.d random variables, $\E(Z_{i}^{\repone}) = \Sigma_{\repone}$ and $\hat{\Sigma}_{\repone} - \Sigma_{\repone} = \frac{1}{n}\sum_{i=1}^{n}\left[Z_{i}^{\repone}-\E(Z_{i}^{\repone})\right]$. Similarly, let us define $Z_{i}^{\reptwo} = K_{\reptwo}(\cdot,X_{i})\otimes_{\Htwo}K_{\reptwo}(\cdot,X_{i})$. Then $Z_{i}^{\reptwo}$'s are i.i.d random variables, $\E(Z_{i}^{\reptwo}) = \Sigma_{\reptwo}$ and $\hat{\Sigma}_{\reptwo} - \Sigma_{\reptwo} = \frac{1}{n}\sum_{i=1}^{n}\left[Z_{i}^{\reptwo}-\E(Z_{i}^{\reptwo})\right]$.

Note that,
\[
\begin{aligned}
    \norm{Z_{i}^{\repone}}_{\HS(\Hone)}    =&\sqrt{\inprod{Z_{i}^{\repone},Z_{i}^{\repone}}_{\HS(\Hone)}}
    =\inprod{K_{\repone}(\cdot,X_{i}),K_{\repone}(\cdot,X_{i})}_{\Hone}
    =K_{\repone}(X_{i},X_{i})
    \leq \kappa \coloneq B.
\end{aligned}
\]

Further, 
\[
\begin{aligned}
   &\E\norm{Z_{i}^{\repone}-\E(Z_{i}^{\repone})}_{\HS(\Hone)}^{2}
   =\E\left[\inprod{Z_{i}^{\repone},Z_{i}^{\repone}}_{\HS(\Hone)}\right] - \inprod{\Sigma_{\repone},\Sigma_{\repone}}_{\HS(\Hone)}
   \leq& \E\left[\inprod{Z_{i}^{\repone},Z_{i}^{\repone}}_{\HS(\Hone)}\right]\\
   =&\E\left[\inprod{K_{\repone}(\cdot,X_{i}),K_{\repone}(\cdot,X_{i})}_{\Hone}^{2}\right]
   =\E\left[K_{\repone}(X_{i},X_{i})^{2}\right]
   \leq \kappa^{2} \coloneq \theta^{2}.
\end{aligned}
\]
Similarly, we can show that $\norm{Z_{i}^{\reptwo}}_{\HS(\Htwo)}\leq \kappa = B$ and $\E\norm{Z_{i}^{\repone}-\E(Z_{i}^{\repone})}_{\HS(\Hone)}^{2}\leq \kappa^{2} = \theta^{2}$.

Note that since $K(\cdot,\cdot)$ is bounded and continuous,$\Hone$ and $\Htwo$ are separable Hilbert spaces. Now, using Bernstein's inequality for separable Hilbert spaces (Theorem D.1 in \citet{sriperumbudur2022approximate}), we have that, for any $0<\delta<1$,
\[
P\left(\norm{\hat{\Sigma}_{\repone}-\Sigma_{\repone}}_{\HS(\Hone)} \geq \frac{2\kappa\log(\frac{6}{\delta})}{n} + \sqrt{\frac{2\kappa^{2}\log(\frac{6}{\delta})}{n}}\right) \leq \frac{\delta}{3}
\]
and 
\[
P\left(\norm{\hat{\Sigma}_{\reptwo}-\Sigma_{\reptwo}}_{\HS(\Hone)} \geq \frac{2\kappa\log(\frac{6}{\delta})}{n} + \sqrt{\frac{2\kappa^{2}\log(\frac{6}{\delta})}{n}}\right) \leq \frac{\delta}{3}.
\]

Therefore, we have that, for any $0<\delta<1$,
\[
\begin{aligned}
    &P\left(\mathbf{A}=\left|\left[\hat{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|\right.\geq \left.\frac{8\kappa^{2}}{\lambda^{3}}\left[\frac{2\kappa\log(\frac{6}{\delta})}{n} + \sqrt{\frac{2\kappa^{2}\log(\frac{6}{\delta})}{n}}\right]\right) \leq \frac{2\delta}{3}.
\end{aligned}
\]

We now proceed to bound $\mathbf{B}$.

Let us define 
\[
\begin{aligned}
    b_{ij} \coloneq& \frac{1}{n^{2}}\left[\inprod{\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{i}),\Sigma_{\repone}^{-\frac{\lambda}{2}}K_{\repone}(\cdot,X_{j})}_{\Hone}-\right.\left.\inprod{\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{i}),\Sigma_{\reptwo}^{-\frac{\lambda}{2}}K_{\reptwo}(\cdot,X_{j})}_{\Htwo}\right]^{2}\\
    =&\frac{1}{n^{2}}\left[A_{ij,\repone}-A_{ij,\reptwo}\right]^{2}.
\end{aligned}
\]
Then, clearly, we have that $\left(b_{ij}\right)_{i,j=1,i \neq j}^{n}$'s are i.i.d random variables. Similarly, $\left(b_{ii}\right)_{i=1}^{n}$ are i.i.d random variables. Further, $\E(b_{ij})=\frac{\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}}{n^{2}}$ if $i\neq j$ and $|b_{ij}|\leq \frac{1}{n^{2}}\left[|A_{ij,\repone}|+|A_{ij,\reptwo}|\right]^{2}\leq \frac{4\kappa^{2}}{\lambda^{2} n^{2}}$ for any $i,j$. Therefore, $\left|\E(b_{ij})\right|\leq \E\left|b_{ij}\right|\leq  \frac{4\kappa^{2}}{\lambda^{2} n^{2}}$ for any $i,j$ and $\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} \leq \frac{4\kappa^{2}}{\lambda^{2}}$.

Now, we have that, $$\E\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} = \frac{n(n-1)}{n^{2}}\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} + n\E(b_{11}).$$ Consequently, $\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \E\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} = \frac{1}{n}\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - n\E(b_{11})$. Therefore, $$\left|\left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \E\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|\leq \frac{8\kappa^{2}}{\lambda^{2} n}.$$ 

Now, using McDiarmid's inequality, we have that,
\[
\begin{aligned}
    P\left(\left|\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2} - \E\left[\tilde{d}_{\lambda}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}\right|\geq \right. \left.\frac{4\kappa^{2}}{\lambda^{2}}\sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right)\leq \frac{\delta}{3}.
\end{aligned}
\]

Therefore, we have that,
\[
\begin{aligned}
    P\left(\mathbf{B} \geq \frac{\kappa^{2}}{\lambda^{2}}\left[\frac{8}{n} + 4\sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right]\right) \leq \frac{\delta}{3}.
\end{aligned}
\]

Finally, we have that,
\[
\begin{aligned}
P\left(\mathbf{A}+\mathbf{B} \leq \frac{8\kappa^{3}}{\lambda^{3}}\left[\frac{2\log(\frac{6}{\delta})}{n} + \sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right] + \frac{4\kappa^{2}}{\lambda^{2}}\left[\frac{2}{n} + \sqrt{\frac{2\log(\frac{6}{\delta})}{n}}\right]\right)
\geq 1-\delta,
\end{aligned}
\]
which completes the proof.
\end{proof}
\subsection{Bound on the sensitivity of risk functional for a pair of representations}

\begin{theorem}\label{Risk bound}
    Assume $\lambda>0$ and consider a spectral regularizer $g_{\lambda}$ that satisfies Assumptions \hyperref[Assumption Spectral Regularizer A1]{$(\SpectralAssumptionone)$}, \hyperref[Assumption Spectral Regularizer A2]{$(\SpectralAssumptiontwo)$}, \hyperref[Assumption Spectral Regularizer A3]{$(\SpectralAssumptionthree)$} and \hyperref[Assumption Spectral Regularizer A4]{$(\SpectralAssumptionfour)$}. Let $\Riskrep^{\gl} (\eta)= \E\left\{[\alpha_{\gl}^{\rep}(X) - \eta(X)]^2\right\}$ be the squared-loss-regression based risk functional corresponding to the kernel regularized population estimator of the regression function $\eta = \E(Y | X)$ using the pullback kernel $K_{\rep}$ and the spectral regularizer $\gl$ corresponding to any representation $\rep: \R^d \to \R^\textrm{out}$. Then, for any two given representations $\repone: \R^d \to \R^k$ and $\reptwo: \R^d \to \R^l$ with corresponding base kernel $K$, the sensitivity of the risk functional can be bounded in terms of the \metricstname distance $\dopgl$ as follows - 

    \begin{equation}
        \begin{aligned}
             |\Riskrepone^{\gl} (\eta)-\Riskreptwo^{\gl}(\eta)| &\leq 4\times \dopgl (\repone,\reptwo) \times\|\eta\|_{\LPtwo}^2
        \end{aligned}
    \end{equation}

    In particular, when we choose $\gl$ to be the Tikhonov regularizer, the risk sensitivity bound can be expressed as- 
    \begin{equation}\label{risk sensitivity bound for Tikhonov regularizer}
        \begin{aligned}
             |\Riskrepone^{\lambda} (\eta)-\Riskreptwo^{\lambda}(\eta)| &\leq 4\times d_{\lambda,K, \Op}^{\text{\metricstname}}(\repone,\reptwo) (\repone,\reptwo) \times\|\eta\|_{\LPtwo}^2
        \end{aligned}
    \end{equation}
\end{theorem}
\begin{proof}
Observe that,
\[\begin{aligned}
   \Riskrepone (\eta)=& \E\left\{[\alpha_{\gl}^{\repone}(X) - \eta(X)]^2\right\} \\
   =& \left\|(\I_{\repone}\gl(\Sone)\I_{\repone}^*- I)\eta\right\|_{\LPtwo}^{2}\\
   =& \left\|(\I_{\repone}\I_{\repone}^*\gl(\Tone)- I)\eta\right\|_{\LPtwo}^{2}\\
   =& \left\|(\Tone\gl(\Tone)- \I)\eta\right\|_{\LPtwo}^{2}\\
   =&\left \langle \left(\Tone\gl(\Tone)- I\right)^{2}\eta, \eta \right\rangle_{\LPtwo}\\
   =&\|\left(\Tone\gl(\Tone)- I\right)\eta\|_{\LPtwo}^2\\
   = & \|u\|^2
\end{aligned} \]
where $u \coloneqq \left(\Tone\gl(\Tone)- I\right)\eta$.

Similarly,
\[\begin{aligned}
   \Riskreptwo (\eta)=& \E\left\{[\alpha_{\gl}^{\reptwo}(X) - \eta(X)]^2\right\} \\
    =&\left \langle \left(\Ttwo\gl(\Ttwo)- I\right)^{2}\eta, \eta \right\rangle_{\LPtwo}\\
    =&\|\left(\Ttwo\gl(\Ttwo)- I\right)\eta\|_{\LPtwo}^2\\
   = & \|v\|^2
\end{aligned} \]
where $v \coloneqq \left(\Ttwo\gl(\Ttwo)- I\right)\eta$. 

Therefore, we have that
\[\begin{aligned}
   &|\Riskrepone (\eta)-\Riskreptwo(\eta)|\\
   &= |\|u\|^2- \|v\|^2|\\
   &= |\langle u,u\rangle-\langle v,v \rangle|\\
   &=| \langle u,u\rangle +\langle u,v\rangle-\langle u,v \rangle-\langle v,v \rangle |\\
   &= |\langle u+v,u\rangle-\langle u+v,v \rangle |\\
   &= |\langle u+v,u-v\rangle|\\
   &= |\langle \left(\Tone\gl(\Tone)+ \Ttwo\gl(\Ttwo) -2I\right)\eta , \left(\Tone\gl(\Tone)- \Ttwo\gl(\Ttwo)\right)\eta\rangle_{\LPtwo}|\\
   &\overset{(i)}{\leq} \|\left(\Tone\gl(\Tone)+ \Ttwo\gl(\Ttwo) -2I\right)\eta\|_{\LPtwo}\|\left(\Tone\gl(\Tone)- \Ttwo\gl(\Ttwo)\right)\eta\|_{\LPtwo}\\
   &\overset{(ii)}{\leq} \|(\Tone\gl(\Tone)+ \Ttwo\gl(\Ttwo) - 2I)\|_{L^{\infty}(\LPtwo)}\|\eta\|_{\LPtwo}\|\\
   &\times (\Tone\gl(\Tone) - \Ttwo\gl(\Ttwo))\|_{L^{\infty}(\LPtwo)}\|\eta\|_{\LPtwo}\\
     &\leq \left\{\|\Tone\gl(\Tone) \|_{L^{\infty}(\LPtwo)}+ \|\Ttwo\gl(\Ttwo) \|_{L^{\infty}(\LPtwo)} + 2\right\} \times \dopgl (\repone,\reptwo) \times\|\eta\|_{\LPtwo}^2\\
     &\leq (1+1+2) \times \dopgl (\repone,\reptwo) \times\|\eta\|_{\LPtwo}^2\\
     &= 4\times \dopgl (\repone,\reptwo) \times\|\eta\|_{\LPtwo}^2,
\end{aligned} \]
where $(i)$ follows from Cauchy-Schwarz inequality and $(ii)$ follows from the definition of operator norms. 
\end{proof}
% \subsection{\trt{Small results}}

%  \subsubsection{Implication of UKP distance}

%  For any two representation maps $\repone,\reptwo$ and any $\eta\in \LPtwo$,
%  \[\E\left[(\alpha_{\gl}^{\repone}(X) -\alpha_{\gl}^{\reptwo}(X))^2\right] \leq [\dopgl (\repone,\reptwo)]^2 \|\eta\|^2_{\LPtwo} \leq [\dLpgl (\repone,\reptwo)]^2 \|\eta\|^2_{\LPtwo}.
%  \]

%  The above result is derived from the operator norm based formulation of UKP and will immediately follow from definition.
%  \subsubsection{Mathematical relationship between UKP, Kernelized Ridge CCA and CKA as population measures}

%  \trt{ENTER THE ARGUMENT FOR HILBERT SCHMIDT NORM $\HS$}
 
% The population version of kernelized ridge CCA is given by $$RCCA^{\textrm{pop}} = \Tr (\Sone^{-\lambda}\Sonetwo\Stwo^{-\lambda}\Stwoone)\eqqcolon \|C_{\repone\reptwo}\|_{\HS}^2,$$

% where $C_{\repone\reptwo}= \Sone^{-\frac{\lambda}{2}}\Sonetwo\Stwo^{-\frac{\lambda}{2}}$.

% The normalized version of kernelized ridge CCA is then given by,
% $$\rho^{\textrm{RCCA}}_{\lambda} (\repone, \reptwo) = \frac{RCCA^{\textrm{pop}}(\repone,\reptwo)}{ab} = \frac{\|C_{\repone\reptwo}\|_{\HS}^2}{ab}, $$

% where $a= \|\Sone^{-\frac{\lambda}{2}}\Sone\Sone^{-\frac{\lambda}{2}}\|_{\HS}$ and $b= \|\Stwo^{-\frac{\lambda}{2}}\Stwo\Stwo^{-\frac{\lambda}{2}}\|_{\HS}$ .

%  \begin{theorem}
%      For any $\lambda>0$, \begin{equation}
%      \begin{aligned}
% \left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}
% =& a^2+b^2- RCCA^{\textrm{pop}}(\repone,\reptwo)\\
% =& a^2+b^2-2ab\rho^{\textrm{RCCA}}_{\lambda} (\repone, \reptwo)\\
% \end{aligned}
% \end{equation}
%  \end{theorem}

%  \begin{proof} From Proposition \ref{Proposition: Squared pseudometric using trace operators}, we have that -
%      \begin{equation}
%      \begin{aligned}
% \left[d_{\lambda,K}^{\text{\metricstname}}(\repone,\reptwo)\right]^{2}=& \Tr\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\Sigma_{\repone}^{-\lambda}\Sigma_{\repone}\right) + \Tr\left(\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo}\right)-2\Tr\left(\Sigma_{\repone}^{-\lambda}\Sigma_{\repone\reptwo}\Sigma_{\reptwo}^{-\lambda}\Sigma_{\reptwo\repone}\right)\\
% =& a^2+b^2- 2 RCCA^{\textrm{pop}}(\repone,\reptwo)\\
% =& (a-b)^2+2ab(1-\rho^{\textrm{RCCA}}_{\lambda} (\repone, \reptwo) )
% \end{aligned}
% \end{equation}
%  \end{proof}
%  \begin{corollary}
%      Let $CKA^{\textrm{pop}}(\repone,\reptwo)$ denote the population version of CKA. Consequently, as $\lambda \to \infty$, we have $\underset{\lambda\to\infty}{\lim} \rho^{\textrm{RCCA}}_{\lambda} (\repone, \reptwo) = CKA^{\textrm{pop}}(\repone,\reptwo)$.
%  \end{corollary}

 \subsection{The integrated UKP pseudometric}

Let $(\Lambda, \mu)$ be a $\sigma$-finite measure space (typically $\Lambda \subset(0, \infty)$ ). Let $w: \Lambda \rightarrow[0, \infty)$ be a measurable weight function. Define the integrated UKP (squared) pseudometric-

$$
\mathcal{D}_{w, K}^2(\repone, \reptwo):=\int_{\Lambda} w(\lambda)\left[\dLpgl(\repone, \reptwo)\right]^2 \mu(d \lambda)
$$


Its corresponding estimator is the $\lambda$-integral of the closed-form function of the data-

$$
\widehat{\mathcal{D}}_{w, K}^2(\repone, \reptwo):=\int_{\Lambda} w(\lambda)\left[\widehat{\dLpgl}(\repone, \reptwo)\right]^2 \mu(d \lambda).
$$


Assuming, 
$
J_2:=\int_{\Lambda} \frac{w(\lambda)}{\lambda^2} \mu(d \lambda)<\infty, \quad J_3:=\int_{\Lambda} \frac{w(\lambda)}{\lambda^3} \mu(d \lambda)<\infty,
$
$\mathcal{D}_{w, K}^2(\repone, \reptwo)$ forms a valid pseudometric and we can obtain similar results as in \metricstname.

% These are minimal and sharp in view of the pointwise bounds $\left|A_{i j, \star}(\lambda)\right| \leq \kappa / \lambda$ and the resolvent perturbation scaling $\lambda^{-2}$ used in the proof of Theorem 2. 

% \begin{theorem} (Finite‑sample concentration for the integrated UKP)
%     Assume $K$ is bounded by $\kappa$ as above and that $J_2, J_3<\infty$. Then for any $\delta \in(0,1)$, with probability at least $1-\delta$,

% $$
% \begin{aligned}
% \left|\widehat{\mathcal{D}}_{w, K}^2(\phi, \psi)-\mathcal{D}_{w, K}^2(\phi, \psi)\right| \leq & 8 \kappa^3 J_3\left[\frac{2 \log (6 / \delta)}{n}+\sqrt{\frac{2 \log (6 / \delta)}{n}}\right] \\
% & +4 \kappa^2 J_2\left[\frac{2}{n}+\sqrt{\frac{2 \log (6 / \delta)}{n}}\right] .
% \end{aligned}
% $$


% Consequently, $\left|\widehat{\mathcal{D}}_{w, K}^2-\mathcal{D}_{w, K}^2\right|=O_{\mathbb{P}}\left(n^{-1 / 2}\right)$ with an explicit constant that depends on $w$ only through the integrals $J_2, J_3$.
% \end{theorem}
% \begin{proof}
%     The proof follows the same decomposition as in your appendix proof of Theorem 2, but after forming the exact $\lambda$-integral. For clarity we write out all steps.

% Step 0: Two standing pointwise bounds
% From the lines preceding and including (19) in your appendix one has, for each $\lambda>0$,

% $$
% \left|\widehat{A}_{i j, \star}(\lambda)\right|,\left|A_{i j, \star}(\lambda)\right| \leq \frac{\kappa}{\lambda}, \quad\left\|\widehat{\Sigma}_{\star}^{-\lambda}-\Sigma_{\star}^{-\lambda}\right\|_{L^{\infty}} \leq \frac{1}{\lambda^2}\left\|\widehat{\Sigma}_{\star}-\Sigma_{\star}\right\|_{L^2}, \quad \star \in\{\phi, \psi\},
% $$

% and hence

% $$
% \left|\left[\widehat{A}_{i j, \phi}(\lambda)-\widehat{A}_{i j, \psi}(\lambda)\right] \pm\left[A_{i j, \phi}(\lambda)-A_{i j, \psi}(\lambda)\right]\right| \leq \frac{4 \kappa}{\lambda}
% $$


% All these are explicitly derived in the block surrounding (19) in your appendix .
% Step 1: Decomposition (triangle inequality)
% Exactly as in your equation (18), write for each $\lambda$

% $$
% \left[\hat{d}_\lambda^{\mathrm{UKP}}\right]^2-\left[d_{\lambda, K}^{\mathrm{UKP}}\right]^2 \leq A(\lambda)+B(\lambda),
% $$

% where

% $$
% A(\lambda):=\left[\hat{d}_\lambda^{\mathrm{UKP}}\right]^2-\left[\tilde{d}_\lambda^{\mathrm{UKP}}\right]^2, \quad B(\lambda):=\left[\hat{d}_\lambda^{\mathrm{UKP}}\right]^2-\left[d_{\lambda, K}^{\mathrm{UKP}}\right]^2
% $$

% (the definitions agree with your (18)) .
% Integrate both sides against $w(\lambda) \mu(d \lambda)$ and use $\left|\int f\right| \leq \int|f|$ to get

% $$
% \left|\widehat{\mathcal{D}}_{w, K}^2-\mathcal{D}_{w, K}^2\right| \leq \underbrace{\int_{\Lambda} w(\lambda)|A(\lambda)| \mu(d \lambda)}_{=: A_w}+\underbrace{\int_{\Lambda} w(\lambda)|B(\lambda)| \mu(d \lambda)}_{=: B_w} .
% $$


% We now bound $A_w$ and $B_w$ on a single high-probability event.
% Step 2: The $A_w$ term (plug-in resolvent error; $\lambda^{-3}$ )
% By the algebra in your line-by-line derivation leading to (19), for each $\lambda$,

% $$
% |A(\lambda)| \leq \frac{4 \kappa^2}{\lambda^3}\left(\left\|\widehat{\Sigma}_\phi-\Sigma_\phi\right\|_{L^2\left(\mathcal{H}_\phi\right)}+\left\|\widehat{\Sigma}_\psi-\Sigma_\psi\right\|_{L^2\left(\mathcal{H}_\psi\right)}\right) . \quad \text { (this is exactly (19)) }
% $$


% Integrating and using the definition of $J_3$,

% $$
% A_w \leq 4 \kappa^2 J_3\left(\left\|\widehat{\Sigma}_\phi-\Sigma_\phi\right\|_{L^2}+\left\|\widehat{\Sigma}_\psi-\Sigma_\psi\right\|_{L^2}\right)
% $$


% Now apply Bernstein's inequality in separable Hilbert spaces to the centered average $\widehat{\Sigma}_\phi-\Sigma_\phi=\frac{1}{n} \sum_{i=1}^n\left(Z_i^\phi-\mathbb{E} Z_i^\phi\right)$ where $Z_i^\phi:=K_\phi\left(\cdot, X_i\right) \otimes K_\phi\left(\cdot, X_i\right)$ (and similarly for $\psi$ ). As in your proof, $\left\|Z_i^\phi\right\|_{L^2} \leq \kappa$ and $\mathbb{E}\left\|Z_i^\phi-\mathbb{E} Z_i^\phi\right\|_{L^2}^2 \leq \kappa^2$, whence by Theorem D. 1 of Sriperumbudur-Sterge (quoted in your appendix) we get, for any $\delta \in(0,1)$,

% $$
% \mathbb{P}\left(\left\|\widehat{\Sigma}_\phi-\Sigma_\phi\right\|_{L^2} \geq \frac{2 \kappa \log (6 / \delta)}{n}+\sqrt{\frac{2 \kappa^2 \log (6 / \delta)}{n}}\right) \leq \frac{\delta}{3}
% $$

% and the same inequality with $\phi$ replaced by $\psi$ (these two displays are exactly as written in your appendix)



% On the intersection of these two events (probability at least $1-2 \delta / 3$ by a union bound), we therefore have

% $$
% \begin{aligned}
% A_w & \leq 4 \kappa^2 J_3\left(2 \cdot \frac{2 \kappa \log (6 / \delta)}{n}+2 \cdot \sqrt{\frac{2 \kappa^2 \log (6 / \delta)}{n}}\right) \\
% & =8 \kappa^3 J_3\left(\frac{2 \log (6 / \delta)}{n}+\sqrt{\frac{2 \log (6 / \delta)}{n}}\right)
% \end{aligned}
% $$
% This reproduces, after integration, the same structure (and constants) as the $\lambda^{-3}$ term in Theorem 2 (compare to your equation (19) and the line immediately following)


% Step 3: The $B_w$ term (V-statistic deviation; $\lambda^{-2}$ )
% Define for each $i, j$ the $\lambda$-integrated summands

% $$
% b_{i j}^{(w)}:=\frac{1}{n^2} \int_{\Lambda} w(\lambda)\left(A_{i j, \phi}(\lambda)-A_{i j, \psi}(\lambda)\right)^2 \mu(d \lambda)
% $$


% Then

% $$
% \left[\hat{d}_\lambda^{\mathrm{UKP}}\right]^2 \text { integrated over } w \quad \Longrightarrow \quad \widetilde{\mathcal{D}}_{w, K}^2:=\sum_{i, j=1}^n b_{i j}^{(w)}
% $$

% and, by Fubini's theorem (justified by $\left|A_{i j, \star}(\lambda)\right| \leq \kappa / \lambda$ and $J_2<\infty$ ),

% $$
% \mathbb{E} b_{i j}^{(w)}= \begin{cases}\frac{1}{n^2} \mathcal{D}_{w, K}^2(\phi, \psi), & i \neq j \\ \mathbb{E} b_{11}^{(w)} \leq \frac{4 \kappa^2}{n^2} J_2, & i=j\end{cases}
% $$

% using $(a-b)^2 \leq(|a|+|b|)^2 \leq(2 \kappa / \lambda)^2$ to bound the diagonal (the same "off/diag" treatment is in your appendix)

%  Furthermore, for every $i, j$,

% $$
% 0 \leq b_{i j}^{(w)} \leq \frac{4 \kappa^2}{n^2} J_2
% $$


% A short computation (identical to your computation right after the definition of $b_{i j}$ but integrated) gives

% $$
% \mathbb{E} \widetilde{\mathcal{D}}_{w, K}^2=\frac{n(n-1)}{n^2} \mathcal{D}_{w, K}^2(\phi, \psi)+n \mathbb{E} b_{11}^{(w)}
% $$


% Therefore

% $$
% \left|\mathcal{D}_{w, K}^2-\mathbb{E} \widetilde{\mathcal{D}}_{w, K}^2\right|=\left|\frac{1}{n} \mathcal{D}_{w, K}^2-n \downarrow_1^{(w)}\right| \leq \frac{4 \kappa^2}{n} J_2+\frac{4 \kappa^2}{n} J_2=\frac{8 \kappa^2}{n} J_2
% $$
% exactly mirroring the $\lambda$-wise bound in your appendix (see the lines deriving $\leq 8 \kappa^2 /\left(\lambda^2 n\right)$ )

% For the random fluctuation $\widetilde{\mathcal{D}}_{w, K}^2-\mathbb{E} \widetilde{\mathcal{D}}_{w, K}^2$, observe that replacing one coordinate $X_i$ by an independent copy can change $\widetilde{\mathcal{D}}_{w, K}^2$ only through the $i$-th row and column in the double sum, i.e. $2 n$ terms, each by at most $\left(4 \kappa^2 / n^2\right) J_2$. Hence the bounded differences constant is

% $$
% c_i \leq \frac{8 \kappa^2}{n} J_2, \quad i=1, \ldots, n .
% $$


% By McDiarmid's inequality (used exactly in your proof for the non-integrated version), for any $\delta \in(0,1)$,

% $$
% \mathbb{P}\left(\left|\widetilde{\mathcal{D}}_{w, K}^2-\mathbb{E} \widetilde{\mathcal{D}}_{w, K}^2\right| \geq 4 \kappa^2 J_2 \sqrt{\frac{2 \log (6 / \delta)}{n}}\right) \leq \frac{\delta}{3},
% $$

% which is the integrated analogue of your display right after applying McDiarmid (compare the coefficient $4 \kappa^2 / \lambda^2$ to our $4 \kappa^2 J_2$ )


% Putting the bias and fluctuation together, on the McDiarmid event (probability $\geq 1-\delta / 3$ ) we obtain

% $$
% B_w=\int w(\lambda)|B(\lambda)| \mu(d \lambda) \leq \frac{8 \kappa^2}{n} J_2+4 \kappa^2 J_2 \sqrt{\frac{2 \log (6 / \delta)}{n}}=4 \kappa^2 J_2\left[\frac{2}{n}+\sqrt{\frac{2 \log (6 / \delta)}{n}}\right] .
% $$


% Step 4: Union bound and conclusion
% Combining the $A_w$ event (probability $\geq 1-2 \delta / 3$ ) and the $B_w$ event (probability $\geq 1-\delta / 3$ ) yields the stated bound simultaneously with probability at least $1-\delta$. This proves the theorem.
% \end{proof}
\subsection{Computational complexity of $\dtwohat$}\label{Computational Complexity}

From the expression of the estimator $\dtwohat$ in Proposition \ref{Proposition: Estimator in terms of Gram matrices}, it can be shown that its computational complexity is $O(n^3)$, where $n$ is the sample size. Notably, the GULP distance proposed in \citet{GULP} shares the same complexity. The primary computational cost arises from inverting the Gram matrix, which can be reduced using kernel approximation techniques like Random Fourier Features (RFF) or Nystr\"{o}m approximation. For example, by using $D$ RFF samples from the spectral distribution of the kernel $K$ or $D$ subsamples from the $n$ data samples in the Nystr\"{o}m method, the complexity of the UKP distance estimator $\dtwohat(\phi,\psi)$ can be reduced from $O(n^3)$ to $O(nD^{2} + D^{3})$, which is significantly lower than $O(n^3)$ when $D \ll n$. Exploring the tradeoff between the statistical accuracy of \metricstname distance estimation and the computational efficiency of kernel approximation methods is a promising direction for future research.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END OF PROOFS%%%%%%%%%%%%%%%%%%

\newpage

\section{Appendix: Additional Experiments}
In this appendix, we discuss the ability of \metricstname to identify differences in architectures and inductive biases and provide additional experimental results.

\subsection{Ability of \metricstname to identify differences in architectures and inductive biases} \label{ImageNet experiments experiments}
 A key source of inductive biases in neural network models is their architecture, with features such as residual connections and variations in convolutional filter complexity shaping the representations learned during training. As a pseudometric over feature space, the \metricstname distance is expected to capture intrinsic differences in these inductive biases, which are known to impact generalization performance across tasks. To explore this, we analyze representations from 35 pre-trained neural network architectures used for image classification, described in detail in Section \ref{Additional ImageNet experiments} of the Appendix.

 We estimate pairwise \metricstname distances between model representations using 3,000 images from the validation set of the ImageNet dataset \citep{krizhevsky2012imagenet}, a regularization parameter $\lambda=1$ and a Gaussian kernel with bandwidth $\sigma=10$. The tSNE embedding method is then used to embed these representations into 2-D space utilizing the distance measures given by the \metricstname pseudometric. Concurrently, we perform an agglomerative (bottom-up) hierarchical clustering of the representations based on the pairwise \metricstname distances and obtain the corresponding dendrogram. We observe in Fig. \ref{DendrogramandtSNE} that similar architectures which share important properties, such as the Regnets and Resnets are clustered together, while they are well separated from smaller efficient architectures such as MobileNets and ConvNexts. This demonstrates that the \metricstname distance effectively captures notions of similarity and dissimilarity aligned with interpretable notions based on inductive biases. Further comparisons with baseline measures, such as GULP and CKA, presented in Fig. \ref{ImageNet dendrograms additional} in Section \ref{Additional ImageNet experiments} of the Appendix demonstrate that \metricstname often provides superior clustering quality. We would like to note here that the choice of the kernel function for the \metricstname pseudometric should be driven by the nature of inductive bias that will be useful for the tasks for which the representations/features of interest will be used. Additional discussion regarding kernel (and kernel parameter) selection is provide in Section \ref{Additional ImageNet experiments} of the Appendix.
 \begin{figure}[t]
\begin{center}
\includegraphics[width=10cm]{Figures/DendogramtSNE_imagenet/DendogramandTSNE_for_UKP_dist_RBF_1.000000e+00_1.000000e+01.png}
\caption{Clustering based on \metricstname distance is sensitive to differences in architectures of neural network models.}\label{DendrogramandtSNE}
\end{center}
\vspace{-4mm}
\end{figure}
\subsection{MNIST experiments}
\label{MNIST Experiments additional}

\paragraph{Training details} We have already described the architectures of the 50 ReLU networks we trained for experiments using the MNIST dataset in Section \ref{MNIST experiments}. We used the uniform Kaiming initialization \cite{he2015delving} for initializing the network weights for every network with a specific width and depth, while the biases are set to zero at initialization. We used a single A100 GPU on the Google Colab platform. We chose to use the Adam optimizer with a learning rate of $10^{-4}$ and a batch size of 100 to train the 50 ReLU networks. We follow a training scheme similar to that used in \citet{GULP}.

\paragraph{Clustering of representations based on UKP aligns with architectural characteristics of networks}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e-02.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Heatmaps/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e-02.png}
    \end{subfigure}
    
    \caption{Heatmaps representing \metricstname distance between pairs of fully-connected ReLU networks of different depths and widths. We choose the kernel for the \metricstname distance to be the Gaussian RBF kernel with bandwidth $\sigma \in \left\{1,10^{-1},10^{-2}\right\}$ along with the regularization parameter $\lambda \in \left\{1,10\right\}$. Along the rows and columns of each of the heatmaps, the ReLU networks are first arranged in order of increasing depth, and then in order of increasing width inside each specific depth level. Darker colors indicate smaller value of \metricstname distance according to the scale attached to each heatmap.}
    \label{MNIST Heatmaps}
\end{figure}

We observe in Fig. \ref{MNIST Heatmaps} that a repeating block structure emerges in each heatmap, with each block corresponding to networks with the same depth. Within each block, i.e., same depth, the pairwise similarities between networks with different widths are higher if the difference of widths of the pair of networks is small, and the similarities are lower otherwise. Further, it seems that the relative difference between networks with different depths is amplified (in terms of the \metricstname distance) if the depths of the networks are larger. For e.g. the contrast between a width 500 and width 600 network is higher when the depth is 9 for both networks, compared to the scenario where both networks have depth 2. We also perform an agglomerative (bottom-up) hierarchical clustering of the representations based on the pairwise \metricstname distances and obtain the corresponding dendrograms as shown in Fig. \ref{MNIST dendrograms}. The dendrograms also exhibit separation between deeper networks (depths 7,8 and 9) and shallow networks (depths 2,4 and 6) over a range of $(\lambda,\sigma)$ choices for the \metricstname distance with Gaussian RBF kernel. This indicates that the \metricstname distance is able to capture the relevant differences in predictive performance that are induced by architectural differences in these networks, over a wide range of values of its tuning parameters.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Dendogram/Dendogram_for_UKP_dist_RBF_1.000000e+00_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Dendogram/Dendogram_for_UKP_dist_RBF_1.000000e+00_1.000000e-01.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Dendogram/Dendogram_for_UKP_dist_RBF_1.000000e+01_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Dendogram/Dendogram_for_UKP_dist_RBF_1.000000e+01_1.000000e-01.png}
    \end{subfigure}
    
    \caption{Dendrograms corresponding to agglomerative hierarchical clustering of representations of 50 ReLU networks based on \metricstname distance}
    \label{MNIST dendrograms}
\end{figure}

\raggedbottom


\paragraph{Generalization ability on kernel ridge regression tasks}

We consider the same setup as discussed in Section \ref{MNIST experiments}. Supplementing our choices of $\lambda=10^{-2}$ and $\sigma=10^{-1}$ corresponding to synthetic kernel ridge regression tasks with Gaussian RBF kernel, we now consider $\lambda \in \left\{10^{-2},1\right\}$  and $\sigma \in \left\{10^{-1},1\right\}$. In Fig. \ref{MNIST generalization plots}, we plot the Spearman's $\rho$ rank correlation coefficient between the $err_{\repone,\reptwo}$'s as defined in Section \ref{MNIST experiments} and the pairwise distances between the representations using the following distances - CCA, linear CKA, nonlinear CKA with Gaussian RBF kernel, GULP and UKP with Gaussian RBF kernel. 

When $(\lambda = 10^{-2},\sigma = 10^{-1})$ and $(\lambda=1,\sigma=10^{-1})$, we observe from Fig. \ref{MNIST generalization plots} that the pairwise \metricstname distance is positively correlated to a moderate extent with the collection of $err_{\repone,\reptwo}$'s, as evident from the large positive values of the blue bars. In contrast, GULP distances show inconsistent behavior across different levels of regularization, while CCA and linear CKA distances show a much lower positive correlation with generalization performance (with CCA even showing negative correlation when $(\lambda=1,\sigma=10^{-1})$). For the remaining choices, none of the distance measures show any consistent behavior, which indicates that an increase in the number of samples used to approximate the model representations may improve the performance of these distance measures.

\raggedbottom

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{Appendix_figures/mnist_experiments/krrgen/png/generalization_includingCKARBF_lambda0.01_sigma0.1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{Appendix_figures/mnist_experiments/krrgen/png/generalization_includingCKARBF_lambda0.01_sigma1.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{Appendix_figures/mnist_experiments/krrgen/png/generalization_includingCKARBF_lambda1_sigma0.1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{Appendix_figures/mnist_experiments/krrgen/png/generalization_includingCKARBF_lambda1_sigma1.png}
    \end{subfigure}
    
    \caption{Spearman's $\rho$ rank correlation coefficient between generalization of kernel ridge regression-based predictors with various distance measures between representations. We report the average correlation across 10 random synthetic kernel ridge regression tasks. Results are similar for 30 trials. Error bars are negligibly small and hence not visible.}
    \label{MNIST generalization plots}
\end{figure}

Unsurprisingly, as a consequence of the relationship between CKA and \metricstname, as discussed in Section \ref{Relation to other comparison measures}, the performance of the CKA distance, when using the Gaussian RBF kernel (with the corresponding bars shown in red), is comparable to that of \metricstname with the same choice of kernel. This similarity in the information conveyed by these two measures can be empirically observed through their scatterplots and the Pearson product-moment correlation coefficient under various choices of tuning parameters. As shown in Fig. \ref{MNIST correlation plots bw UKP CKA}, the nearly linear positive relationship between \metricstname and CKA distances, when both are used with a Gaussian RBF kernel, along with the high positive correlation coefficient, suggests that either measure could be effectively used in practice for comparing representations. However, the \metricstname distance may be preferred over the CKA distance due to its pseudometric properties, particularly the triangle inequality, which proves to be especially useful. In contrast, CKA, being a measure akin to a normalized inner product bounded between 0 and 1, does not satisfy the properties of a pseudometric and may lead to misleading intuitions when comparing different representations.

\raggedbottom

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Correlation/Correlation_plot_for_CKA_dist_RBF_1.000000e+00_and_UKP_dist_RBF_1.000000e+00_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Correlation/Correlation_plot_for_CKA_dist_RBF_1.000000e+00_and_UKP_dist_RBF_1.000000e+00_1.000000e+01.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Correlation/Correlation_plot_for_CKA_dist_RBF_1.000000e+01_and_UKP_dist_RBF_1.000000e+01_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/mnist_experiments/Correlation/Correlation_plot_for_CKA_dist_RBF_1.000000e+01_and_UKP_dist_RBF_1.000000e+01_1.000000e+01.png}
    \end{subfigure}
    
    \caption{Correlation plots between UKP and CKA measures with Gaussian RBF kernel between $\binom{50}{2}$ pairs of ReLU networks trained on MNIST data. Plot titles display the Pearson product-moment correlation coefficient between the distance measures on the two axes.}

    \label{MNIST correlation plots bw UKP CKA}
\end{figure}

\FloatBarrier

\subsection{ImageNet experiments}
\label{Additional ImageNet experiments}

\paragraph{Architectures used and data description} In our experiments, we utilized 35 pretrained models known for achieving state-of-the-art (SOTA) performance in the ImageNet Object Localization Challenge on Kaggle \cite{imagenet-object-localization-challenge}, available from \citet{pytorch}. These models are categorized based on their architectural types as follows:

\begin{itemize}
    \item \textbf{ResNets (17 models):} regnet\_x\_16gf, regnet\_x\_1\_6gf, regnet\_x\_32gf, regnet\_x\_3\_2gf, regnet\_x\_400mf, regnet\_x\_800mf, regnet\_x\_8gf, regnet\_y\_16gf, regnet\_y\_1\_6gf, regnet\_y\_32gf, regnet\_y\_3\_2gf, regnet\_y\_400mf, regnet\_y\_800mf, regnet\_y\_8gf, resnet18, resnext50\_32x4d, wide\_resnet50\_2
    \item \textbf{EfficientNets (8 models):} efficientnet\_b0, efficientnet\_b1, efficientnet\_b2, efficientnet\_b3, efficientnet\_b4, efficientnet\_b5, efficientnet\_b6, efficientnet\_b7
    \item \textbf{MobileNets (3 models):} mobilenet\_v2, mobilenet\_v3\_large, mobilenet\_v3\_small
    \item \textbf{ConvNeXts (2 models):} convnext\_small, convnext\_tiny
    \item \textbf{Other Architectures (5 models):} alexnet, googlenet, inception, mnasnet, vgg16 .
\end{itemize}

The penultimate layer dimensions for these networks, corresponding to the representation sizes, vary from 400 to 4096 depending on the architecture. Each model processes input data as 3-channel RGB images, with each channel having dimensions of 224 × 224 pixels. To approximate the model representations learned by these models using finite-dimensional representations, we used 3000 images from the validation set of the ImageNet dataset. These images were normalized with a mean of (0.485, 0.456, 0.406) and a standard deviation of (0.229, 0.224, 0.225) for each RGB channel. Our choice of models and input preprocessing parameters is similar to those used in \citet{GULP}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e-02.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+00_1.000000e-03.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e-02.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Heatmaps_final/Heatmap_for_UKP_dist_RBF_1.000000e+01_1.000000e-03.png}
    \end{subfigure}
    
     \caption{Heatmaps representing \metricstname distance between pairs of networks of different architecture, pretrained on ImageNet data. We choose the kernel for the \metricstname distance to be the Gaussian RBF kernel with bandwidth $\sigma \in \left\{10^{-1},10^{-2},10^{-3}\right\}$ along with the regularization parameter $\lambda \in \left\{1,10\right\}$. Along the rows and columns of each of the heatmaps, the networks are arranged in the following order from left to right and top to bottom - ResNets, EfficientNets, Other Architectures, MobileNets and ConvNexts. Darker colors indicate smaller value of \metricstname distance according to the scale attached to each heatmap.}
    \label{ImageNet heatmaps}
\end{figure}

\paragraph{Clustering of representations based on UKP aligns with architectural characteristics of networks}

We are interested in observing whether the \metricstname pseudometric is capable of capturing intrinsic differences in predictive performances of different representations. Such intrinsic differences are often the result of the different inductive biases we encode into networks through the choice of architectures, among other factors. 

We first discuss the main architectural similarities and differences between ResNet, RegNet, EfficientNet, MobileNet, alexnet, googlenet, inception, mnasnet, and vgg16, which are controlled by how they address depth, efficiency, and feature extraction. Alexnet and vgg16 are older architectures that use standard convolutional layers arranged in sequential blocks, with vgg16 deepening the network significantly compared to alexnet. Googlenet introduced Inception modules, which combine multiple convolution filters of different sizes to capture multi-scale features, making it more efficient than alexNet and vgg16. Different Inception architectures have been built using the Inception module of Googlenet. ResNet brought the innovation of residual connections (skip connections) to address the vanishing gradient problem, enabling very deep networks, while RegNet refined this concept by creating more regular, scalable structures without explicit skip connections. EfficientNet and mnasnet focus on balanced scaling (depth, width, resolution) and use of MBConv blocks for efficiency, with EfficientNet employing a compound scaling formula. MobileNet, like mnasnet, emphasizes depthwise separable convolutions for lightweight, efficient models suitable for mobile devices. In terms of architectural similarities, resNet and regNet share a focus on structured deep architectures, while EfficientNet and MobileNet share efficiency-driven designs for varied hardware constraints. Alexnet, vgg16, and googlenet represent early convolutional architectures, with googlenet’s Inception modules providing a bridge to more modern designs. In contrast, vgg16 and ResNet are quite different, with vgg16 being sequential and deep, and ResNet leveraging residual connections.

We observe in Fig. \ref{ImageNet heatmaps} that a block structure emerges in the heatmaps across different choices of the tuning parameters for the \metricstname distance, especially corresponding to the 4 major groups of architectures ResNets, EfficientNets, MobileNets and ConvNeXts. We also perform an agglomerative (bottom-up) hierarchical clustering of the representations based on the pairwise \metricstname distances and obtain the corresponding dendrograms as shown in Fig. \ref{ImageNet dendrograms}. The dendrograms exhibit a clear separation between the ResNets/RegNets and the remaining architectures over a range of $(\lambda,\sigma)$ choices for the \metricstname distance with Gaussian RBF kernel. This indicates that, for the class of pretrained ImageNet models we consider, the \metricstname distance captures the relevant differences in predictive performance that are induced by architectural differences in these networks, over a wide range of values of its tuning parameters.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Dendogram_final/Dendogram_for_UKP_dist_RBF_1.000000e+00_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Dendogram_final/Dendogram_for_UKP_dist_RBF_1.000000e+00_1.000000e-02.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Dendogram_final/Dendogram_for_UKP_dist_RBF_1.000000e+01_1.000000e-01.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Dendogram_final/Dendogram_for_UKP_dist_RBF_1.000000e+01_1.000000e-02.png}
    \end{subfigure}
    
    \caption{Dendrograms corresponding to agglomerative hierarchical clustering of representations of 35 pretrained ImageNet networks based on \metricstname distance}
    \label{ImageNet dendrograms}
\end{figure}

To illustrate that the performance of the \metricstname pseudometric is reasonably robust to the choice of the regularization parameter $\lambda$ and kernel parameters (such as bandwidth parameter $\sigma$ for the Gaussian RBF kernel), we have compared the performance of \metricstname's performance with other popular baseline measures such as GULP and CKA. As observed from Fig. \ref{ImageNet dendrograms additional}, the separation between the different classes of networks is more pronounced in the case of \metricstname than GULP. Additionally, the clustering behaviour within the primary classes of networks is much weaker for the CKA compared to the \metricstname and GULP measures, and the separation between the different classes is not clear in the case of CKA.

\begin{figure}[!h]
    \centering

    % First Row
    \hspace*{\fill}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Additional_comparisons_for_UKP_with_GULP_and_CKA/DendogramandTSNE_for_UKP_dist_RBF_1.000000e+00_1.000000e+01_mainpaper.png}
    \subcaption{UKP (with RBF kernel, regularization parameter $\lambda=1$ and kernel bandwidth $\sigma=10$)}
    \end{subfigure}
    \hspace*{\fill}

    \vspace{0.5cm}  % Space between rows
    
    % Second Row
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Additional_comparisons_for_UKP_with_GULP_and_CKA/DendogramandTSNE_for_GULP_dist_1.000000e+00.png}
    \subcaption{GULP (with regularization parameter $\lambda=1$)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Additional_comparisons_for_UKP_with_GULP_and_CKA/DendogramandTSNE_for_CKA_dist_RBF_1.000000e+01.png}
        \subcaption{CKA (with RBF kernel and kernel bandwidth $\sigma=10$)}
    \end{subfigure}

     \caption{tSNE embeddings and dendrograms corresponding to agglomerative hierarchical clustering of representations of 35 pretrained ImageNet networks based on \metricstname (with Gaussian RBF kernel, regularization parameter $\lambda=1$ and kernel bandwidth $\sigma=10$), GULP (with regularization parameter $\lambda=1$) and CKA (with Gaussian RBF kernel and kernel bandwidth $\sigma=10$) distance}
    \label{ImageNet dendrograms additional}
\end{figure}


\paragraph{Relationship between UKP and CKA measures}

The MNIST experiments, along with the theoretical analysis in section \ref{Relation to other comparison measures}, reveal a similarity between the information conveyed by the \metricstname and CKA measures when both use the same kernel. This similarity is also empirically confirmed in the ImageNet experiments, as demonstrated by their scatterplots and the Pearson correlation coefficient across different tuning parameters. As illustrated in Fig. \ref{ImageNet correlation plots UKP CKA}, there is an almost linear positive relationship between \metricstname and CKA distances when both utilize a Gaussian RBF kernel. The strong positive correlation suggests that either measure could be effectively used for comparing representations. However, as previously discussed in Section \ref{Relation to other comparison measures}, \metricstname may be preferred over CKA due to its pseudometric properties, particularly the triangle inequality, which is especially advantageous. In contrast, CKA, being a measure similar to a normalized inner product bounded between 0 and 1, does not satisfy pseudometric properties and may lead to misleading interpretations when comparing different representations.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Correlation_final/Correlation_plot_for_CKA_dist_RBF_1.000000e+00_and_UKP_dist_RBF_1.000000e+00_1.000000e+00.png}
    % \subcaption{$\lambda=1$,$\sigma=1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Correlation_final/Correlation_plot_for_CKA_dist_RBF_1.000000e+00_and_UKP_dist_RBF_1.000000e+00_1.000000e+01.png}
    \end{subfigure}
    
    \vspace{0.5cm}  % Adjusts vertical space between rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Correlation_final/Correlation_plot_for_CKA_dist_RBF_1.000000e+01_and_UKP_dist_RBF_1.000000e+01_1.000000e+00.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Appendix_figures/imagenet_experiments/Correlation_final/Correlation_plot_for_CKA_dist_RBF_1.000000e+01_and_UKP_dist_RBF_1.000000e+01_1.000000e+01.png}
    \end{subfigure}
    
    \caption{Correlation plots between UKP and CKA measures with Gaussian RBF kernel between $\binom{35}{2}$ pairs of networks with different architectures trained on  ImageNet data. Plot titles display the Pearson product-moment correlation coefficient between the distance measures on the two axes.}
    \label{ImageNet correlation plots UKP CKA}
\end{figure}

\paragraph{Choice of kernel function} The choice of kernel function for the \metricstname pseudometric should be guided by the inductive bias most relevant to the tasks for which the representations or features of interest will be used. For instance, consider an image classification task where the model's predictions should remain unaffected by image rotations and translations. In this case, we can incorporate this inductive bias into the \metricstname pseudometric by selecting a rotationally and translationally invariant kernel, such as the Gaussian RBF kernel, as the kernel function for UKP. This approach is particularly useful for comparing the generalization performance of two representations: one obtained through a training or optimization procedure that explicitly enforces rotational and translational invariance, and another trained without such constraints.

Furthermore, even when the true inductive bias is unknown, probing the nature of representations encoded by different models can still provide valuable insights. In this context, the terms ``well-specified" and ``misspecified" kernels refer, respectively, to choices of kernels for the UKP pseudometric that either capture or fail to capture the required inductive bias for a specific class of downstream tasks utilizing the representations or features of interest. Each kernel choice can be viewed as a selection of particular characteristics of the representations that we aim to investigate.

If we have a set of characteristics in mind that we wish to probe, we should select a corresponding set of kernels whose feature maps encode some or all of those characteristics and then analyze the conclusions drawn from using each kernel as the kernel function for the \metricstname pseudometric. When the kernels are ``well-specified", clustering representations based on \metricstname values can help identify useful pairs of representations for specific downstream tasks. In contrast, when the kernels are ``misspecified", the \metricstname values may still cluster representations with characteristics aligned with the feature maps of the ``misspecified" kernels. However, in such cases, the clustering will not be informative for studying generalization performance on downstream tasks. Nonetheless, even with ``misspecified kernels", the \metricstname pseudometric can still provide insights into the characteristics of the representations, though its values will not reliably indicate generalization performance.

Cross-validation or selecting an ``optimal" value for the kernel parameters is not necessary in the context of this paper, as our focus is on an exploratory comparison of the inductive biases encoded by different representations. For example, consider a scenario where we hypothesize that rotational and/or translational invariance are the key inductive biases required for good generalization performance, as in image classification tasks. In this case, the Gaussian RBF kernel is a natural choice. Since the Gaussian RBF kernel remains rotationally and translationally invariant for any value of its bandwidth parameter—which controls the ``scale" at which the kernel perceives the representations—the \metricstname pseudometric should, in principle, capture the extent to which different representations encode rotational and translational invariance, regardless of the specific choice of bandwidth.

Of course, no experimental setup is ever exhaustive. In our study, we focus on datasets from the image domain (MNIST and ImageNet) to illustrate two of the simplest and most fundamental invariances -rotational and translational invariance - which are relevant to most image-related tasks. This consideration motivated our choice of the Gaussian RBF kernel as the kernel function for the \metricstname pseudometric in our experiments.

\paragraph{Code implementation} The Python code for running all the experiments in this paper is available in the following Anonymous GitHub repository: \url{https://anonymous.4open.science/r/Uniform-Kernel-Prober-ICLR-2026-20792/}. The code for comparing our proposed \metricstname pseudometric to other distance measures has been adapted from \url{https://github.com/sgstepaniants/GULP}. 


\end{document}
